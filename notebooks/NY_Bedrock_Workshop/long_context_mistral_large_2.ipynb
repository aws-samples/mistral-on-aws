{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral's Models on Amazon Bedrock\n",
    "\n",
    "Welcome to our workshop introducing Mistral’s models on Amazon Bedrock. In this workshop we will demonstrate the long context window capabilities of Mistral Large 2.\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "\n",
    "## What is Mistral Large 2?\n",
    "\n",
    "Mistral Large 2 (24.07) is a state-of-the-art large language model featuring:\n",
    "\n",
    "- **128 Billion Parameters:** Enhancing its ability to understand and generate complex language structures.\n",
    "- **128k Context Window:** Allowing it to process and generate responses based on very long inputs.\n",
    "- **Multilingual Proficiency:** Supporting dozens of languages, including French, German, Spanish, Italian, Arabic, Hindi, Japanese, and more.\n",
    "- **Coding Language Support:** Understanding and generating code in over 80 programming languages.\n",
    "- **Improved Instruction Following:** Better adherence to user instructions and tasks.\n",
    "- **Enhanced Conversational Abilities:** More natural and context-aware interactions.\n",
    "- **Tool Use:** Ability to utilize tools and functions for extended operations.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "* **Available Regions**: `us-west-2`\n",
    "* **Model ID**: `mistral.mistral-large-2407-v1:0`\n",
    "* **Context Window**: 128,000 tokens\n",
    "* **Maximum Tokens per Response**: 8,192\n",
    "\n",
    "To learn more about Mistral Large 2 benchmarks, follow this [link](https://mistral.ai/news/mistral-large-2407/)\n",
    "\n",
    "In this notebook, we'll guide you through the process of using Mistral Large 2 to summarize a PDF document, demonstrating its capacity to handle long contexts and generate detailed summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "To interact with Amazon Bedrock and process PDF files, we need to import the following libraries:\n",
    "\n",
    "* **`boto3`**: AWS SDK for Python, used to interact with Amazon Bedrock.\n",
    "* **`botocore.config.Config`**: Allows configuration of AWS clients, such as setting timeouts.\n",
    "* **`PyPDF2`**: A library for reading and extracting text from PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 botocore PyPDF2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import json\n",
    "from botocore.config import Config\n",
    "import PyPDF2\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Bedrock Client\n",
    "\n",
    "The `initialize_bedrock_client` function sets up the client for interacting with Amazon Bedrock.\n",
    "\n",
    "### Converse with the Model\n",
    "\n",
    "The `converse` function sends a prompt to the Mistral Large 2 model and retrieves the response.\n",
    "\n",
    "### Extract Text from PDF\n",
    "\n",
    "The `extract_text_from_pdf` function reads a PDF file and extracts all the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def initialize_bedrock_client(region_name=\"us-west-2\", read_timeout=2000):\n",
    "    config = Config(read_timeout=read_timeout)\n",
    "    return boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=region_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "def converse(\n",
    "    system_prompt='',\n",
    "    task_instructions='',\n",
    "    context='',\n",
    "    max_tokens=1000,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    model_id='mistral.mistral-large-2407-v1:0',\n",
    "    bedrock_client=None\n",
    "):\n",
    "    if bedrock_client is None:\n",
    "        bedrock_client = initialize_bedrock_client()\n",
    "    # Construct the system prompt\n",
    "    system = [{\"text\": system_prompt}] if system_prompt else []\n",
    "\n",
    "    # Construct the user message\n",
    "    user_content = '\\n'.join(filter(None, [task_instructions, context]))\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_content.strip()}]\n",
    "    }]\n",
    "\n",
    "    try:\n",
    "        # Make the converse API call\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=system,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Input token logger with reponse metadata\n",
    "        token_usage = response['usage']\n",
    "        logger.info(\"Input tokens: %s\", token_usage['inputTokens'])\n",
    "        # Extract and return the assistant's response\n",
    "        assistant_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        return assistant_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            # Use a generator expression to extract text from all pages\n",
    "            text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{pdf_path}' was not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF file: {e}\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques with Mistral Large 2\n",
    "\n",
    "Few-Shot Learning, Delimiters, and Role Playing\n",
    "\n",
    "Few-shot learning or in-context learning is when we provide a few examples in the prompts, and the LLM can generate corresponding output based on these examples. This technique can often improve model performance, especially when the task is difficult or when we want the model to respond in a specific manner.\n",
    "\n",
    "Delimiters like ###, <<< >>>, or other symbols specify the boundary between different sections of the text. In our examples, we'll use ### to indicate examples and <<< >>> to indicate customer inquiries.\n",
    "\n",
    "Role playing involves providing the LLM with a role (e.g., \"You are a bank customer service bot.\", \"You are a pirate\"), which adds personal context to the model and often leads to better performance - this is best defined within the system field. You can combine the persona definition and the instructions within the system field as well - you may have to experiment to see which approach yields higher performance for your specific use case. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input tokens: 249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Issue Identified**: Inefficient customer service response time and lack of assistance.\n",
      "- **Suggested Improvement**: Improve customer service response time by increasing staffing, providing additional training, or implementing a more efficient ticketing system. Also, consider offering multiple channels for support, such as live chat, email, and phone, to better assist customers. Regularly update customers on the status of their issues and provide clear timelines for resolution.\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt and task instructions directly\n",
    "system = \"You are a business analyst who provides clear summaries of customer feedback, identifying key issues and suggesting actionable improvements.\"\n",
    "\n",
    "task_instructions = \"\"\"\n",
    "### Example 1:\n",
    "Customer Feedback: \"I've been using your software for a few months now, and while it's generally good, it crashes whenever I try to export reports. This is really frustrating and hinders my work.\"\n",
    "Analysis:\n",
    "- **Issue Identified**: Software crashes during report export.\n",
    "- **Suggested Improvement**: Fix the bug causing crashes during the export function to enhance user experience.\n",
    "\n",
    "### Example 2:\n",
    "Customer Feedback: \"The user interface is not intuitive. It took me a long time to find basic features, and the navigation is confusing.\"\n",
    "Analysis:\n",
    "- **Issue Identified**: Unintuitive user interface and confusing navigation.\n",
    "- **Suggested Improvement**: Redesign the UI to be more user-friendly and streamline navigation menus.\n",
    "\n",
    "###\n",
    "Customer Feedback: \"Your customer service is unresponsive. I reached out multiple times about an issue, but haven't received any assistance.\"\n",
    "Analysis:\n",
    "\"\"\"\n",
    "\n",
    "# Since we don't need the PDF for this example, we can proceed without context\n",
    "# Call the converse function\n",
    "response = converse(\n",
    "    system_prompt=system,\n",
    "    task_instructions=task_instructions,\n",
    "    context=\"\",  # No additional context needed\n",
    "    max_tokens=300,\n",
    "    temperature=0.0,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Print the assistant's response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval in Long Contexts\n",
    "\n",
    "Language models often struggle to locate information embedded in the middle of long texts due to context window limitations or challenges in attention mechanisms. Mistral Large 2, with its 128k context window, is designed to handle such tasks more effectively (sometimes referred to as the needle in a haystack problem).\n",
    "\n",
    "Example: extracting specific information from the middle of our pdf document. On page 22, there's an insight we'll ask Large 2 to find: \n",
    "\n",
    "***“Amazon AppFlow is a fully managed integration service that enables customers to securely transfer data between Software-as-a-Service (SaaS) applications such as Salesforce, Marketo, Slack, and ServiceNow, and AWS services such as Amazon S3 and Amazon Redshift. AppFlow can run data flows at a frequency the customer chooses - on a schedule, in response to a business event, or on demand.”***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input tokens: 44025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Amazon AppFlow\n",
      "\n",
      "**Amazon AppFlow** is a fully managed integration service that enables customers to securely transfer data between Software-as-a-Service (SaaS) applications such as Salesforce, Marketo, Slack, and ServiceNow, and AWS services such as Amazon S3 and Amazon Redshift. AppFlow can run data flows at a frequency the customer chooses—on a schedule, in response to a business event, or on demand. Customers can also configure data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps.\n",
      "\n",
      "#### Key Features for HIPAA Compliance:\n",
      "\n",
      "1. **Encryption in Transit**:\n",
      "   - Amazon AppFlow encrypts data while in transit between AppFlow and the configured source/destination using TLS 1.2 or later.\n",
      "\n",
      "2. **Encryption at Rest**:\n",
      "   - Data stored at rest in S3 is automatically encrypted using an AWS KMS key (formerly CMK) that is specified by the customer.\n",
      "   - For PHI data transferred to non-S3 destinations, customers must ensure the at-rest storage for the chosen destination meets their security needs.\n",
      "\n",
      "3. **Monitoring and Logging**:\n",
      "   - Amazon AppFlow enables application monitoring by integrating with AWS CloudTrail to log API calls and Amazon EventBridge to emit flow execution events.\n",
      "\n",
      "4. **Data Flow Configuration**:\n",
      "   - Customers can configure data flows to run on a schedule, in response to business events, or on demand.\n",
      "   - Data transformation capabilities like filtering and validation can be applied to generate rich, ready-to-use data.\n",
      "\n",
      "#### Usage with PHI:\n",
      "\n",
      "- **Processing and Transferring PHI**:\n",
      "  - Amazon AppFlow can be used to process and transfer data containing PHI.\n",
      "  - Customers must ensure that the encryption and logging configurations are consistent with HIPAA guidelines.\n",
      "\n",
      "- **Encryption Requirements**:\n",
      "  - Encryption of data while in transit between AppFlow and the configured source/destination is provided by default using TLS 1.2 or later.\n",
      "  - Data stored at rest in S3 is automatically encrypted using an AWS KMS key.\n",
      "\n",
      "- **Monitoring and Logging**:\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt\n",
    "system_prompt = \"You are an expert assistant who can find and summarize specific sections of long documents.\"\n",
    "\n",
    "# Task instructions\n",
    "task_instructions = \"\"\"\n",
    "Given the AWS Security whitepaper, help me answer this question - if you do not know the answer, say 'I don't know': \n",
    "\n",
    "What is Amazon AppFlow, and how does it integrate with SaaS applications and AWS services to facilitate secure data transfer?\n",
    "\"\"\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "pdf_path = 'AWS-security-whitepaper.pdf'\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Call the converse function\n",
    "response = converse(\n",
    "    system_prompt=system_prompt,\n",
    "    task_instructions=task_instructions,\n",
    "    context=document_text,\n",
    "    max_tokens=500,\n",
    "    temperature=0.0,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Print the assistant's response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing a Lengthy PDF Document with Mistral Large 2\n",
    "\n",
    "In this section, we'll utilize Mistral Large 2 to generate a comprehensive summary of a lengthy PDF document—the AWS Security whitepaper. This demonstration showcases the model's capability to handle long contexts and produce detailed summaries that capture the essence of the original material. By extracting the text from the PDF and crafting specific prompts, we'll guide the model to generate an organized summary that includes an overview, key insights, challenges, and a concise conclusion.\n",
    "\n",
    "We'll set up our prompts and execute the summarization. Remember, you can define Mistral Large 2's persona in the system field and specify the task instructions in the user role. Feel free to update the persona to whatever you like and adjust the task instructions to suit your needs. Today, we'll focus on extracting the text from the PDF and preparing the prompts for the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_document(pdf_path, system_prompt, task_instructions, max_tokens=1000):\n",
    "    # Extract text from the PDF\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Check if the document was loaded successfully before proceeding\n",
    "    if document_text:\n",
    "        # Call the converse function to summarize the document\n",
    "        response = converse(\n",
    "            system_prompt=system_prompt,\n",
    "            task_instructions=task_instructions,\n",
    "            context=document_text,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        return response\n",
    "    else:\n",
    "        print(\"Cannot proceed with summarization due to issues with loading the document.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input tokens: 44142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of the Document ###\n",
      "\n",
      "### Overview\n",
      "\n",
      "The AWS Whitepaper \"Architecting for HIPAA Security and Compliance on Amazon Web Services\" outlines how customers can use AWS to run sensitive workloads regulated under the U.S. Health Insurance Portability and Accountability Act (HIPAA). The paper focuses on the HIPAA Privacy and Security Rules for protecting Protected Health Information (PHI) and how to use AWS to encrypt data in transit and at rest. It also discusses how AWS features can be used to run workloads containing PHI. The document emphasizes that AWS offers a comprehensive set of features and services to manage key encryption and make it simpler to audit, including the AWS Key Management Service (AWS KMS). Customers with HIPAA compliance requirements have flexibility in how they meet encryption requirements for PHI. The paper also highlights that AWS provides a Business Associate Addendum (BAA) for such customers, which is a standardized agreement for processing, storing, and transmitting PHI using HIPAA-eligible services.\n",
      "\n",
      "### Key Insights\n",
      "\n",
      "1. **Encryption Requirements**:\n",
      "   - AWS requires customers to encrypt PHI stored in or transmitted using HIPAA-eligible services in accordance with guidance from the Secretary of Health and Human Services (HHS).\n",
      "   - AWS offers various encryption features and services, such as AWS KMS, to help customers manage encryption keys and ensure data protection.\n",
      "\n",
      "2. **HIPAA-Eligible Services**:\n",
      "   - The paper details numerous AWS services that are HIPAA-eligible, including Amazon API Gateway, Amazon AppFlow, Amazon AppStream 2.0, Amazon Athena, Amazon Aurora, Amazon CloudFront, Amazon CloudWatch, Amazon Comprehend, AWS Identity and Access Management (IAM), Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Registry (ECR), Amazon Elastic Container Service (ECS), Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EMR, Amazon EventBridge, Amazon Forecast, Amazon FSx, Amazon GuardDuty, Amazon HealthLake, Amazon Inspector, Amazon Managed Service for Apache Flink, Amazon Data Firehose, Amazon Kinesis Streams, Amazon Kinesis Video Streams, Amazon Lex, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon MQ, Amazon Neptune, AWS Network Firewall, Amazon Pinpoint, Amazon Polly, Amazon Quantum Ledger Database (Amazon QLDB), Amazon QuickSight, Amazon RDS for MariaDB, Amazon RDS for MySQL, Amazon RDS for Oracle, Amazon RDS for PostgreSQL, Amazon RDS for SQL Server, Amazon Redshift, Amazon Rekognition, Amazon Route 53, Amazon S3 Glacier, Amazon S3 Transfer Acceleration, Amazon SageMaker, Amazon Simple Email Service (Amazon SES), Amazon Simple Queue Service (Amazon SQS), Amazon Simple Storage Service (Amazon S3), Amazon Simple Workflow Service, Amazon Textract, Amazon Transcribe, Amazon Translate, Amazon Virtual Private Cloud (Amazon VPC), Amazon WorkDocs, Amazon WorkSpaces, AWS App Mesh, AWS Application Migration Service, AWS Auto Scaling, AWS Backup, AWS Batch, AWS Certificate Manager, AWS Cloud Map, AWS CloudFormation, AWS CloudHSM, AWS CloudTrail, AWS CodeBuild, AWS CodeDeploy, AWS CodeCommit, AWS CodePipeline, AWS Config, AWS Data Exchange, AWS Database Migration Service, AWS DataSync, AWS Directory Service, AWS Directory Service for Microsoft AD, Amazon Cloud Directory, AWS Elastic Beanstalk, AWS Elastic Disaster Recovery, AWS Fargate, AWS Firewall Manager, AWS Global Accelerator, AWS Glue, AWS Glue DataBrew, AWS IoT Core and AWS IoT Device Management, AWS IoT Greengrass, AWS Lambda, AWS Managed Services, AWS OpsWorks for Chef Automate, AWS OpsWorks for Puppet Enterprise, AWS OpsWorks Stack, AWS Organizations, AWS RoboMaker, AWS SDK Metrics, AWS Secrets Manager, AWS Security Hub, AWS Server Migration Service, AWS Serverless Application Repository, Service Catalog, AWS Shield, AWS Snowball, AWS Snowball Edge, AWS Step Functions, AWS Storage Gateway, AWS Systems Manager, AWS Transfer for SFTP, AWS WAF – Web Application Firewall, AWS X-Ray, Elastic\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Bedrock client\n",
    "    bedrock_client = initialize_bedrock_client()\n",
    "\n",
    "    # Define prompts\n",
    "    system_prompt = \"You are a polite research assistant who is always helpful, cheerful, pragmatic, and extremely detail oriented\"\n",
    "    task_instructions = \"\"\"\n",
    "    Please provide a comprehensive summary of the document, including the following sections:\n",
    "    1. **Overview**\n",
    "   - A very brief introduction to the main topic and objectives of the paper in about ten sentences.\n",
    "\n",
    "2. **Key Insights**\n",
    "   - Detailed insights and findings presented in the paper.\n",
    "   - Highlight any opportunities identified by the authors.\n",
    "\n",
    "3. **Key Challenges**\n",
    "   - Outline the main challenges or obstacles discussed.\n",
    "   - Discuss any limitations or areas that require further research.\n",
    "\n",
    "4. **Conclusion**\n",
    "   - A very concise wrap-up of the overall significance of the findings in a few sentences.\n",
    "   \n",
    "Ensure that each section is clearly labeled and that the information is presented in a clear and organized manner.\n",
    "    \"\"\"\n",
    "\n",
    "    # Summarize the document\n",
    "    pdf_path = 'AWS-security-whitepaper.pdf'\n",
    "    summary = summarize_document(pdf_path, system_prompt, task_instructions)\n",
    "\n",
    "    # Print the summarized response\n",
    "    if summary:\n",
    "        print(\"### Summary of the Document ###\\n\")\n",
    "        print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we used the converse api from Bedrock to maximize the 128k context window of Mistral Large 2.\n",
    "\n",
    "In certain scenarios, you might be dealing with individual documents or document stores that are much larger than the context windows most large language models offer. In these situations, you are able leverage Open-source frameworks like [LangChain](https://www.langchain.com/) with your documents to use techniques such as [`map-reduce`](https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/)to chunk them and generate individual summaries quickly in parallel and reduce to a single summary or [`refine`](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/) to iteratively process document chunks by looping over the input documents to generate a summary with better retention of context between chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the advanced capabilities of Mistral Large 2 on Amazon Bedrock through practical examples. We demonstrated how to effectively use few-shot learning and prompt engineering techniques to guide the model in analyzing customer feedback. We showcased Mistral Large 2's ability to perform information retrieval in long contexts, finding specific information like a \"needle in a haystack\" within extensive texts. Lastly, we leveraged its powerful summarization capabilities to condense lengthy documents into comprehensive summaries. These examples highlight the model's proficiency in handling complex tasks, making it a valuable tool for a wide range of applications across various industries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

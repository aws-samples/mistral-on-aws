{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral's Models on Amazon Bedrock\n",
    "\n",
    "Welcome to our workshop introducing Mistralâ€™s models on Amazon Bedrock. Mistral Small & Large 2 models are deployed on Amazon Bedrock. In this workshop we will demonstrate the new capabilities of Mistral Large 2 including tool/function use, multi-lingual abilities, and the power of long context windows as well as data generation. \n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "\n",
    "## What is Mistral Large 2?\n",
    "\n",
    "Mistral Large 2 (24.07) is a state-of-the-art large language model featuring:\n",
    "\n",
    "- **128 Billion Parameters:** Enhancing its ability to understand and generate complex language structures.\n",
    "- **128k Context Window:** Allowing it to process and generate responses based on very long inputs.\n",
    "- **Multilingual Proficiency:** Supporting dozens of languages, including French, German, Spanish, Italian, Arabic, Hindi, Japanese, and more.\n",
    "- **Coding Language Support:** Understanding and generating code in over 80 programming languages.\n",
    "- **Improved Instruction Following:** Better adherence to user instructions and tasks.\n",
    "- **Enhanced Conversational Abilities:** More natural and context-aware interactions.\n",
    "- **Tool Use:** Ability to utilize tools and functions for extended operations.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "* **Available Regions**: `us-west-2`\n",
    "* **Model ID**: `mistral.mistral-large-2407-v1:0`\n",
    "* **Context Window**: 128,000 tokens\n",
    "* **Maximum Tokens per Response**: 8,192\n",
    "\n",
    "In this notebook, we'll guide you through the process of using Mistral Large 2 to summarize a PDF document, demonstrating its capacity to handle long contexts and generate detailed summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "To interact with Amazon Bedrock and process PDF files, we need to import the following libraries:\n",
    "\n",
    "* **`boto3`**: AWS SDK for Python, used to interact with Amazon Bedrock.\n",
    "* **`botocore.config.Config`**: Allows configuration of AWS clients, such as setting timeouts.\n",
    "* **`PyPDF2`**: A library for reading and extracting text from PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Bedrock Client\n",
    "\n",
    "The `initialize_bedrock_client` function sets up the client for interacting with Amazon Bedrock.\n",
    "\n",
    "### Converse with the Model\n",
    "\n",
    "The `converse` function sends a prompt to the Mistral Large 2 model and retrieves the response.\n",
    "\n",
    "### Extract Text from PDF\n",
    "\n",
    "The `extract_text_from_pdf` function reads a PDF file and extracts all the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def initialize_bedrock_client(region_name=\"us-west-2\", read_timeout=2000):\n",
    "    config = Config(read_timeout=read_timeout)\n",
    "    return boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=region_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "def converse(\n",
    "    system_prompt='',\n",
    "    task_instructions='',\n",
    "    context='',\n",
    "    max_tokens=1000,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    model_id='mistral.mistral-large-2407-v1:0',\n",
    "    bedrock_client=None\n",
    "):\n",
    "    if bedrock_client is None:\n",
    "        bedrock_client = initialize_bedrock_client()\n",
    "    # Construct the system prompt\n",
    "    system = [{\"text\": system_prompt}] if system_prompt else []\n",
    "\n",
    "    # Construct the user message\n",
    "    user_content = '\\n'.join(filter(None, [task_instructions, context]))\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_content.strip()}]\n",
    "    }]\n",
    "\n",
    "    try:\n",
    "        # Make the converse API call\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=system,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract and return the assistant's response\n",
    "        assistant_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        return assistant_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            # Use a generator expression to extract text from all pages\n",
    "            text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{pdf_path}' was not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF file: {e}\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Document\n",
    "\n",
    "The `summarize_document` function extracts text from a PDF and generates a summary using the Mistral Large 2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(pdf_path, system_prompt, task_instructions, max_tokens=1000):\n",
    "    # Extract text from the PDF\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Check if the document was loaded successfully before proceeding\n",
    "    if document_text:\n",
    "        # Call the converse function to summarize the document\n",
    "        response = converse(\n",
    "            system_prompt=system_prompt,\n",
    "            task_instructions=task_instructions,\n",
    "            context=document_text,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        return response\n",
    "    else:\n",
    "        print(\"Cannot proceed with summarization due to issues with loading the document.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we set up our prompts and execute the summarization. Remember, you can define Large 2's persona in the system field and the specific task instructions in the user role. Feel free to update the persona to whatever you like and adjust the task instructions. The basis for our summary will be the latest State of Gen AI report from Deloitte - which is ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of the Document ###\n",
      "\n",
      "### Overview\n",
      "\n",
      "The Deloitte report, \"State of Generative AI in the Enterprise: Quarter Three Report, August 2024,\" focuses on the transition from the potential of Generative AI (GenAI) to its practical performance within organizations. The report highlights the increasing pressure on organizations to demonstrate significant and sustained value from their GenAI initiatives. As C-suites and boards begin to look for returns on investment, the report emphasizes the need for patience and perseverance to unlock the transformational potential of GenAI. Key areas explored include data and governance, risk and compliance, and measuring value. The report also discusses the shift from large language models (LLMs) to small language models (SLMs) and the rise of AI agents, which offer new avenues for automation and personalization. Regulatory considerations and the importance of human-centered change are also addressed.\n",
      "\n",
      "### Key Insights\n",
      "\n",
      "1. **Initial Success and Investment:**\n",
      "   - Organizations are seeing value from early GenAI forays, with 67% increasing investments due to strong value seen to date.\n",
      "   - Improved efficiency, productivity, and cost reduction are the top benefits sought and achieved.\n",
      "   - Diverse benefits such as increased innovation, improved products, and enhanced customer relationships are also being realized.\n",
      "\n",
      "2. **Scaling Challenges:**\n",
      "   - Despite strong early value, many organizations struggle to scale GenAI projects, with 68% moving 30% or fewer experiments into production.\n",
      "   - Embedding GenAI deeply into business functions and processes is seen as the top way to drive value.\n",
      "\n",
      "3. **Data Foundations:**\n",
      "   - 75% of organizations have increased investment in data life cycle management to support GenAI.\n",
      "   - Data-related issues are limiting options, with 55% of organizations avoiding certain use cases due to data concerns.\n",
      "\n",
      "4. **Risk Management and Regulation:**\n",
      "   - Only 23% of organizations feel highly prepared for risk management and governance challenges.\n",
      "   - Regulatory uncertainty is a significant barrier, with organizations preparing regulatory forecasts and assessments.\n",
      "\n",
      "5. **Measuring Value:**\n",
      "   - 41% of organizations struggle to define and measure the exact impacts of their GenAI initiatives.\n",
      "   - Less than half are using specific KPIs to measure GenAI performance, indicating a need for more rigorous measurement frameworks.\n",
      "\n",
      "### Key Challenges\n",
      "\n",
      "1. **Scaling GenAI Projects:**\n",
      "   - Many organizations are still in the pilot or proof-of-concept stage, facing difficulties in large-scale deployment.\n",
      "   - Lack of a comprehensive strategy and robust governance are significant barriers.\n",
      "\n",
      "2. **Data Management:**\n",
      "   - Data quality, privacy, and security concerns are major obstacles.\n",
      "   - Organizations need to improve data life cycle management and address issues related to sensitive data.\n",
      "\n",
      "3. **Risk and Governance:**\n",
      "   - Organizations are not fully prepared for the risks and governance challenges posed by GenAI.\n",
      "   - Regulatory uncertainty and the need for comprehensive risk management frameworks are pressing issues.\n",
      "\n",
      "4. **Measuring and Communicating Value:**\n",
      "   - There is a lack of standardized metrics and frameworks for measuring GenAI performance.\n",
      "   - Organizations need to develop a comprehensive set of financial and nonfinancial measures to communicate value effectively.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The Deloitte report underscores the significant potential of GenAI to drive transformational change within organizations. However, realizing this potential requires overcoming substantial challenges related to scaling, data management, risk and governance, and measuring value. Organizations that invest in robust data foundations, develop comprehensive risk management frameworks, and implement rigorous measurement systems are more likely to succeed in harnessing the full benefits of GenAI. The report emphasizes the importance of human-centered change and the need for ongoing adaptation as GenAI technologies and use cases evolve.\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Bedrock client\n",
    "    bedrock_client = initialize_bedrock_client()\n",
    "\n",
    "    # Define prompts\n",
    "    system_prompt = \"You are a polite research assistant who is always helpful, cheerful, pragmatic, and extremely detail oriented\"\n",
    "    task_instructions = \"\"\"\n",
    "    Please provide a comprehensive summary of the document, including the following sections:\n",
    "    1. **Overview**\n",
    "   - A very brief introduction to the main topic and objectives of the paper in about ten sentences.\n",
    "\n",
    "2. **Key Insights**\n",
    "   - Detailed insights and findings presented in the paper.\n",
    "   - Highlight any opportunities identified by the authors.\n",
    "\n",
    "3. **Key Challenges**\n",
    "   - Outline the main challenges or obstacles discussed.\n",
    "   - Discuss any limitations or areas that require further research.\n",
    "\n",
    "4. **Conclusion**\n",
    "   - A very concise wrap-up of the overall significance of the findings in a few sentences.\n",
    "   \n",
    "Ensure that each section is clearly labeled and that the information is presented in a clear and organized manner.\n",
    "    \"\"\"\n",
    "\n",
    "    # Summarize the document\n",
    "    pdf_path = 'us-state-of-gen-ai-q3.pdf'\n",
    "    summary = summarize_document(pdf_path, system_prompt, task_instructions)\n",
    "\n",
    "    # Print the summarized response\n",
    "    if summary:\n",
    "        print(\"### Summary of the Document ###\\n\")\n",
    "        print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

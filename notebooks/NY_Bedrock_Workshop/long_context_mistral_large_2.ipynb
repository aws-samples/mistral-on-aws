{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral's Models on Amazon Bedrock\n",
    "\n",
    "Welcome to our workshop introducing Mistral’s models on Amazon Bedrock. In this workshop we will demonstrate the long context window capabilities of Mistral Large 2.\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "\n",
    "## What is Mistral Large 2?\n",
    "\n",
    "Mistral Large 2 (24.07) is a state-of-the-art large language model featuring:\n",
    "\n",
    "- **128 Billion Parameters:** Enhancing its ability to understand and generate complex language structures.\n",
    "- **128k Context Window:** Allowing it to process and generate responses based on very long inputs.\n",
    "- **Multilingual Proficiency:** Supporting dozens of languages, including French, German, Spanish, Italian, Arabic, Hindi, Japanese, and more.\n",
    "- **Coding Language Support:** Understanding and generating code in over 80 programming languages.\n",
    "- **Improved Instruction Following:** Better adherence to user instructions and tasks.\n",
    "- **Enhanced Conversational Abilities:** More natural and context-aware interactions.\n",
    "- **Tool Use:** Ability to utilize tools and functions for extended operations.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "* **Available Regions**: `us-west-2`\n",
    "* **Model ID**: `mistral.mistral-large-2407-v1:0`\n",
    "* **Context Window**: 128,000 tokens\n",
    "* **Maximum Tokens per Response**: 8,192\n",
    "\n",
    "To learn more about Mistral Large 2 benchmarks, follow this [link](https://mistral.ai/news/mistral-large-2407/)\n",
    "\n",
    "In this notebook, we'll guide you through the process of using Mistral Large 2 to summarize a PDF document, demonstrating its capacity to handle long contexts and generate detailed summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "To interact with Amazon Bedrock and process PDF files, we need to import the following libraries:\n",
    "\n",
    "* **`boto3`**: AWS SDK for Python, used to interact with Amazon Bedrock.\n",
    "* **`botocore.config.Config`**: Allows configuration of AWS clients, such as setting timeouts.\n",
    "* **`PyPDF2`**: A library for reading and extracting text from PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import json\n",
    "from botocore.config import Config\n",
    "import PyPDF2\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Bedrock Client\n",
    "\n",
    "The `initialize_bedrock_client` function sets up the client for interacting with Amazon Bedrock.\n",
    "\n",
    "### Converse with the Model\n",
    "\n",
    "The `converse` function sends a prompt to the Mistral Large 2 model and retrieves the response.\n",
    "\n",
    "### Extract Text from PDF\n",
    "\n",
    "The `extract_text_from_pdf` function reads a PDF file and extracts all the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def initialize_bedrock_client(region_name=\"us-west-2\", read_timeout=2000):\n",
    "    config = Config(read_timeout=read_timeout)\n",
    "    return boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=region_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "def converse(\n",
    "    system_prompt='',\n",
    "    task_instructions='',\n",
    "    context='',\n",
    "    max_tokens=2000,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    model_id='mistral.mistral-large-2407-v1:0',\n",
    "    bedrock_client=None\n",
    "):\n",
    "    if bedrock_client is None:\n",
    "        bedrock_client = initialize_bedrock_client()\n",
    "    # Construct the system prompt\n",
    "    system = [{\"text\": system_prompt}] if system_prompt else []\n",
    "\n",
    "    # Construct the user message\n",
    "    user_content = '\\n'.join(filter(None, [task_instructions, context]))\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_content.strip()}]\n",
    "    }]\n",
    "\n",
    "    try:\n",
    "        # Make the converse API call\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=system,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Input token logger with reponse metadata\n",
    "        token_usage = response['usage']\n",
    "        logger.info(\"Input tokens: %s\", token_usage['inputTokens'])\n",
    "        # Extract and return the assistant's response\n",
    "        assistant_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        return assistant_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            # Use a generator expression to extract text from all pages\n",
    "            text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{pdf_path}' was not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF file: {e}\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques with Mistral Large 2\n",
    "\n",
    "Few-Shot Learning, Delimiters, and Role Playing\n",
    "\n",
    "Few-shot learning or in-context learning is when we provide a few examples in the prompts, and the LLM can generate corresponding output based on these examples. This technique can often improve model performance, especially when the task is difficult or when we want the model to respond in a specific manner.\n",
    "\n",
    "Delimiters like ###, <<< >>>, or other symbols specify the boundary between different sections of the text. In our examples, we'll use ### to indicate examples and <<< >>> to indicate customer inquiries.\n",
    "\n",
    "Role playing involves providing the LLM with a role (e.g., \"You are a bank customer service bot.\", \"You are a pirate\"), which adds personal context to the model and often leads to better performance - this is best defined within the system field. You can combine the persona definition and the instructions within the system field as well - you may have to experiment to see which approach yields higher performance for your specific use case. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the system prompt and task instructions directly\n",
    "system = \"You are a business analyst who provides clear summaries of customer feedback, identifying key issues and suggesting actionable improvements.\"\n",
    "\n",
    "task_instructions = \"\"\"\n",
    "### Example 1:\n",
    "Customer Feedback: \"I've been using your software for a few months now, and while it's generally good, it crashes whenever I try to export reports. This is really frustrating and hinders my work.\"\n",
    "Analysis:\n",
    "- **Issue Identified**: Software crashes during report export.\n",
    "- **Suggested Improvement**: Fix the bug causing crashes during the export function to enhance user experience.\n",
    "\n",
    "### Example 2:\n",
    "Customer Feedback: \"The user interface is not intuitive. It took me a long time to find basic features, and the navigation is confusing.\"\n",
    "Analysis:\n",
    "- **Issue Identified**: Unintuitive user interface and confusing navigation.\n",
    "- **Suggested Improvement**: Redesign the UI to be more user-friendly and streamline navigation menus.\n",
    "\n",
    "###\n",
    "Customer Feedback: \"Your customer service is unresponsive. I reached out multiple times about an issue, but haven't received any assistance.\"\n",
    "Analysis:\n",
    "\"\"\"\n",
    "\n",
    "# Since we don't need the PDF for this example, we can proceed without context\n",
    "# Call the converse function\n",
    "response = converse(\n",
    "    system_prompt=system,\n",
    "    task_instructions=task_instructions,\n",
    "    context=\"\",  # No additional context needed\n",
    "    max_tokens=300,\n",
    "    temperature=0.0,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Print the assistant's response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval in Long Contexts\n",
    "\n",
    "Language models often struggle to locate information embedded in the middle of long texts due to context window limitations or challenges in attention mechanisms. Mistral Large 2, with its 128k context window, is designed to handle such tasks more effectively (sometimes referred to as the needle in a haystack problem).\n",
    "\n",
    "Example: extracting specific information from the middle of our pdf document. On page 22, there's an insight we'll ask Large 2 to find: \n",
    "\n",
    "***“Amazon AppFlow is a fully managed integration service that enables customers to securely transfer data between Software-as-a-Service (SaaS) applications such as Salesforce, Marketo, Slack, and ServiceNow, and AWS services such as Amazon S3 and Amazon Redshift. AppFlow can run data flows at a frequency the customer chooses - on a schedule, in response to a business event, or on demand.”***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the system prompt\n",
    "system_prompt = \"You are an expert assistant who can find and summarize specific sections of long documents.\"\n",
    "\n",
    "# Task instructions\n",
    "task_instructions = \"\"\"\n",
    "Given the AWS Security whitepaper, help me answer this question - if you do not know the answer, say 'I don't know': \n",
    "\n",
    "For Amazon AppFlow, what SaaS services are supported? List the services.\n",
    "\"\"\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "pdf_path = 'AWS-security-whitepaper.pdf'\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Call the converse function\n",
    "response = converse(\n",
    "    system_prompt=system_prompt,\n",
    "    task_instructions=task_instructions,\n",
    "    context=document_text,\n",
    "    max_tokens=500,\n",
    "    temperature=0.0,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Print the assistant's response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing a Lengthy PDF Document with Mistral Large 2\n",
    "\n",
    "In this section, we'll utilize Mistral Large 2 to generate a comprehensive summary of a lengthy PDF document—the AWS Security whitepaper. This demonstration showcases the model's capability to handle long contexts and produce detailed summaries that capture the essence of the original material. By extracting the text from the PDF and crafting specific prompts, we'll guide the model to generate an organized summary that includes an overview, key insights, challenges, and a concise conclusion.\n",
    "\n",
    "We'll set up our prompts and execute the summarization. Remember, you can define Mistral Large 2's persona in the system field and specify the task instructions in the user role. Feel free to update the persona to whatever you like and adjust the task instructions to suit your needs. Today, we'll focus on extracting the text from the PDF and preparing the prompts for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_document(pdf_path, system_prompt, task_instructions, max_tokens=1000):\n",
    "    # Extract text from the PDF\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Check if the document was loaded successfully before proceeding\n",
    "    if document_text:\n",
    "        # Call the converse function to summarize the document\n",
    "        response = converse(\n",
    "            system_prompt=system_prompt,\n",
    "            task_instructions=task_instructions,\n",
    "            context=document_text,\n",
    "            max_tokens=2500,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        return response\n",
    "    else:\n",
    "        print(\"Cannot proceed with summarization due to issues with loading the document.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Bedrock client\n",
    "    bedrock_client = initialize_bedrock_client()\n",
    "\n",
    "    # Define prompts\n",
    "    system_prompt = \"You are a polite research assistant who is always helpful, cheerful, pragmatic, and extremely detail oriented\"\n",
    "    task_instructions = \"\"\"\n",
    "    Please provide a comprehensive summary of the document, including the following sections:\n",
    "    1. **Overview**\n",
    "   - A very brief introduction to the main topic and objectives of the paper in about ten sentences.\n",
    "\n",
    "2. **Key Insights**\n",
    "   - Detailed insights and findings presented in the paper.\n",
    "   - Highlight any opportunities identified by the authors.\n",
    "\n",
    "3. **Key Challenges**\n",
    "   - Outline the main challenges or obstacles discussed.\n",
    "   - Discuss any limitations or areas that require further research.\n",
    "\n",
    "4. **Conclusion**\n",
    "   - A very concise wrap-up of the overall significance of the findings in a few sentences.\n",
    "   \n",
    "Ensure that each section is clearly labeled and that the information is presented in a clear and organized manner.\n",
    "    \"\"\"\n",
    "\n",
    "    # Summarize the document\n",
    "    pdf_path = 'AWS-security-whitepaper.pdf'\n",
    "    summary = summarize_document(pdf_path, system_prompt, task_instructions)\n",
    "\n",
    "    # Print the summarized response\n",
    "    if summary:\n",
    "        print(\"### Summary of the Document ###\\n\")\n",
    "        print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we used the converse api from Bedrock to maximize the 128k context window of Mistral Large 2.\n",
    "\n",
    "In certain scenarios, you might be dealing with individual documents or document stores that are much larger than the context windows most large language models offer. In these situations, you are able leverage Open-source frameworks like [LangChain](https://www.langchain.com/) with your documents to use techniques such as [`map-reduce`](https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/) to chunk them and generate individual summaries quickly in parallel and reduce to a single summary or [`refine`](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/) to iteratively process document chunks by looping over the input documents to generate a summary with better retention of context between chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the advanced capabilities of Mistral Large 2 on Amazon Bedrock through practical examples. We demonstrated how to effectively use few-shot learning and prompt engineering techniques to guide the model in analyzing customer feedback. We showcased Mistral Large 2's ability to perform information retrieval in long contexts, finding specific information like a \"needle in a haystack\" within extensive texts. Lastly, we leveraged its powerful summarization capabilities to condense lengthy documents into comprehensive summaries. These examples highlight the model's proficiency in handling complex tasks, making it a valuable tool for a wide range of applications across various industries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

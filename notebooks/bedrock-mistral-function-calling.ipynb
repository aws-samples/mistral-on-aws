{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# Bedrock Function Calling with Mistral models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we walkthrough an implementation of function calling and agentic workflows using Mistral models on Amazon Bedrock. Function Calling is a powerful technique that allows large language models to connect to external tools, systems, or APIs to enable, which can be executed to perform actions based on user's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58ce86-3cb5-4a4b-9735-dc4684db731f",
   "metadata": {},
   "source": [
    "Throughout this notebook, we demonstrate an agentic workflow that leverages Mistral models on Amazon Bedrock to create a seamless function calling experience. We explore techniques for crafting effective prompts over function calling and developing custom helper functions capable of understanding an API's data structure. These helper functions can identify the necessary tools and methods to be executed during the agentic workflow interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fcd65-01b7-418d-988e-f80d05ebb4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Mistral Model Selection\n",
    "\n",
    "Today, there are three Mistral models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Mistral 7B Instruct\n",
    "\n",
    "- **Description:** A 7B dense Transformer model, fast-deployed and easily customizable. Small yet powerful for a variety of use cases.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 2. Mixtral 8X7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 3. Mistral Large\n",
    "\n",
    "- **Description:** A cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Code Generation, RAG, or Agents\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below compares the model performance on the Massive Multitask Language Understanding (MMLU) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Mistral 7B Instruct | 62.5%      | \\$0.00015                    | \\$0.0002                      |\n",
    "| Mixtral 8x7B Instruct | 70.6%      | \\$0.00045                    | \\$0.0007                      |\n",
    "| Mistral Large | 81.2%      | \\$0.008                   | \\$0.024                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Mistral Model Selection Guide](https://docs.mistral.ai/guides/model-selection/)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3978ae9-57d0-4dca-b94b-b6493f8eade0",
   "metadata": {},
   "source": [
    "---\n",
    "## Supported papameters\n",
    "\n",
    "The Mistral AI models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "The Mistral AI models have the following inference parameters:\n",
    "\n",
    "- **Temperature** - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- **Top P** - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- **Top K** - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- **Maximum Length** - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- **Stop sequences** - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "## Setup and Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e8ec-674a-44c2-be1f-0a92df6b7ca6",
   "metadata": {},
   "source": [
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. In this notebook, it's only used to initialize a `Bedrock Client` and convert our functions into a JSON schema that allows Mistral to interact with our custom function calling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.20\n",
    "langchain-experimental==0.0.58\n",
    "boto3==1.34.105\n",
    "sqlalchemy==2.0.30\n",
    "pandas==2.2.2\n",
    "pydantic==2.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf00ca0-9a89-4a3e-ac25-5f172dac43d5",
   "metadata": {},
   "source": [
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc6fb1-8ed0-45ab-b30c-c7f7a004e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-aws --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d59d834-3f3c-46e5-8cb8-56def569a934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains.openai_functions import convert_to_openai_function as convert_to_llm_fn\n",
    "from langchain.tools import tool\n",
    "import pandas as pd\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc107b3-7dfa-494a-9269-6229b6dccb24",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18477cd8-aeb2-4fec-b9ea-74b73f7386f4",
   "metadata": {},
   "source": [
    "Let's say, we have a transactional dataset tracking customers, payment amounts, payment dates, and whether payments have been fully processed for each transaction identifier. For more information about the sample dataset, please visit the documentation for [Mistral Function Calling](https://docs.mistral.ai/capabilities/function_calling/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data()-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a JSON file into a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing the data loaded from the JSON file.\n",
    "        If an error occurs during file loading, an error message is returned instead.\n",
    "\n",
    "    \"\"\"\n",
    "    local_path = \"sample_data/transactions.json\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_json(local_path)\n",
    "        return df\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        return  f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ad9e5a-5525-49a8-be42-1b8e7263aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id customer_id  payment_amount payment_date payment_status\n",
      "0          T1001        C001          125.50   2021-10-05           Paid\n",
      "1          T1002        C002           89.99   2021-10-06         Unpaid\n",
      "2          T1003        C003          120.00   2021-10-07           Paid\n",
      "3          T1004        C002           54.30   2021-10-05           Paid\n",
      "4          T1005        C001          210.20   2021-10-08        Pending\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb7ad0-64ea-408f-a946-8a0601202838",
   "metadata": {},
   "source": [
    "## Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc0e89-c9fe-4acd-8a9e-bc3fd7a5c828",
   "metadata": {},
   "source": [
    "Function calling is the ability to reliably connect a large language model (LLM) to external tools and enable effective tool usage and interaction with external APIs. Mistral models provide the ability for building LLM powered chatbots or agents that need to retrieve context for the model or interact with external tools by converting natural language into API calls to retrieve specific domain knowledge. From conversational agents and math problem solving to API integration and information extraction, multiple use cases can benefit from this capability provided by Mistral models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18150cc-1aac-4a7f-83f2-49d3c1a91abf",
   "metadata": {},
   "source": [
    "---\n",
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8ce00-28f4-4074-9c27-14c9d2263b28",
   "metadata": {},
   "source": [
    "LangChain Tools are interfaces that allow agents, chains, or language models to interact with the world. They typically include a name, description, JSON schema for input parameters, and the function to call. This information is used to prompt the language model on how to specify and take actions. We will use LangChain Tools to quickly transform our functions as a prompt that can be used by Mistral to execute function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37320d-94c0-4e25-9457-74ca7cd37297",
   "metadata": {},
   "source": [
    "Letâ€™s consider we have two functions as our two tools: **retrieve_payment_status** and **retrieve_payment_date** to retrieve payment status and payment date given a transaction ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953f6f54-ff34-422b-9d99-873fe6f401c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(BaseModel):\n",
    "    transaction_id: str = Field(..., description='Transaction ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d4ad2-93a3-434c-b3a1-b310917784af",
   "metadata": {},
   "source": [
    "---\n",
    "### Function Call 1: Retrieve payment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db00433-5194-40e5-a1cc-a05a7d5c2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_payment_status(params: Params) -> str:\n",
    "    \"Get payment status of a transaction\"\n",
    "    data = load_data()\n",
    "\n",
    "    try:\n",
    "        # Attempt to retrieve the payment status for the given transaction ID\n",
    "        status = data[data.transaction_id == params.transaction_id].payment_status.item()\n",
    "    except ValueError:\n",
    "        # If the transaction ID is not found, return an error message\n",
    "        return {'error': f\"Transaction ID {params.transaction_id} not found.\"}\n",
    "\n",
    "    # Retrieve the payment status for the corresponding index\n",
    "    return json.dumps({'status': f\"{status}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3fe51-fe41-45dc-ba79-fcfa13dd7a7b",
   "metadata": {},
   "source": [
    "---\n",
    "### Function Call 2: Retrieve payment date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0be1320-f94d-46ae-a0cc-738681a4ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_payment_date(params: Params) -> str:\n",
    "    \"Get payment date of a transaction\"\n",
    "    data = load_data()\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve the payment date for the given transaction ID\n",
    "        date = data[data.transaction_id == params.transaction_id].payment_date.item()\n",
    "    except ValueError:\n",
    "        # If the transaction ID is not found, return an error message\n",
    "        return {'error': f\"Transaction ID {params.transaction_id} not found.\"}\n",
    "    \n",
    "    return json.dumps({'date': date})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779e6db-2322-4cdf-8f4b-2b97e532c380",
   "metadata": {},
   "source": [
    "---\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae18c68-8ddd-4162-a036-f7dc496fca62",
   "metadata": {},
   "source": [
    "In this step, we will utilize another function from the LangChain library to transform a raw function or class into a format that can be easily understood and processed by a large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5e6964-e502-451c-9be5-5e7c171a133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_payment_status, retrieve_payment_date]\n",
    "functions = [convert_to_llm_fn(f) for f in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a932f565-d50a-4536-81b7-bd9af2fc1aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'retrieve_payment_status',\n",
       "  'description': 'retrieve_payment_status(params: __main__.Params) -> str - Get payment status of a transaction',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'params': {'type': 'object',\n",
       "     'properties': {'transaction_id': {'description': 'Transaction ID',\n",
       "       'type': 'string'}},\n",
       "     'required': ['transaction_id']}},\n",
       "   'required': ['params']}},\n",
       " {'name': 'retrieve_payment_date',\n",
       "  'description': 'retrieve_payment_date(params: __main__.Params) -> str - Get payment date of a transaction',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'params': {'type': 'object',\n",
       "     'properties': {'transaction_id': {'description': 'Transaction ID',\n",
       "       'type': 'string'}},\n",
       "     'required': ['transaction_id']}},\n",
       "   'required': ['params']}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855eea6-7a35-4a4f-b0db-93c4a9d3641a",
   "metadata": {},
   "source": [
    "---\n",
    "## Agentic Workflow Orchestration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d07d8-236e-4c19-a7df-05ff1c6b5e0a",
   "metadata": {},
   "source": [
    "An **Agentic Workflow** refers to an iterative and multi-step approach which uses large language models (LLMs) as AI Agents to perform a list of actions before the user receives an actual response. The agents can be configured to embody specific personalities/roles by not just generating \"responses\" but also engaging with multiple systems and tools. In this section, we aim to orchestrate all the steps to create a simple agentic workflow that takes a question,then searches for the right tool/function, executes the function, and answers the user in a human-readable way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3804805-66c7-43dc-bae5-d6b19024e6ab",
   "metadata": {},
   "source": [
    "In this section, we defined helper functions to automate the process of identifying the **function call** to be used by the LLM, extract the function from its response, and execute the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ea878-0da2-4a9d-ab1a-4af3e8a10638",
   "metadata": {},
   "source": [
    "Helper Function: Extracts function call representations enclosed within XML tags (`<functioncall>` and `<multiplefunctions>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4e1954-aed0-416d-966a-56d3e4d8ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_calls(completion: str):\n",
    "    if isinstance(completion, str):\n",
    "        content = completion\n",
    "    else:\n",
    "        content = completion.content\n",
    "\n",
    "    # Multiple functions lookup\n",
    "    mfn_pattern = r\"<multiplefunctions>(.*?)</multiplefunctions>\"\n",
    "    mfn_match = re.search(mfn_pattern, content, re.DOTALL)\n",
    "\n",
    "    # Single function lookup\n",
    "    single_pattern = r\"<functioncall>(.*?)</functioncall>\"\n",
    "    single_match = re.search(single_pattern, content, re.DOTALL)\n",
    "    \n",
    "    functions = []\n",
    "    \n",
    "    if not mfn_match and not single_match:\n",
    "         # No function calls found\n",
    "        return None\n",
    "    elif mfn_match:\n",
    "        # Multiple function calls found\n",
    "        multiplefn = mfn_match.group(1)\n",
    "        for fn_match in re.finditer(r\"<functioncall>(.*?)</functioncall>\", multiplefn, re.DOTALL):\n",
    "            fn_text = fn_match.group(1)\n",
    "            try:\n",
    "                functions.append(json.loads(fn_text.replace('\\\\', '')))\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # Ignore invalid JSON\n",
    "    else:\n",
    "        # Single function call found\n",
    "        fn_text = single_match.group(1)\n",
    "        try:\n",
    "            functions.append(json.loads(fn_text.replace('\\\\', '')))\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Ignore invalid JSON\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2f00a-6f53-4572-ab12-889d5a4397ca",
   "metadata": {},
   "source": [
    "Helper Function: Executes function call with the arguments captured by the LLM during the AI/user interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be629d9-fe51-43ba-98fd-8ca85459ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(function_list: list):\n",
    "    for function_dict in function_list:\n",
    "        function_name = function_dict['name']\n",
    "        arguments = function_dict['arguments']\n",
    "        \n",
    "        # Check if the function exists in the current scope\n",
    "        if function_name in globals():\n",
    "            func = globals()[function_name]\n",
    "            \n",
    "            # Call the function with the provided arguments\n",
    "            result = func.invoke(input=arguments)\n",
    "            return result\n",
    "        else:\n",
    "            return {'error': f\"Function '{function_name}' not found.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996ba3f-4838-4c2e-845e-207ea363ece7",
   "metadata": {},
   "source": [
    "Helper Function: The agentic workflow function contains the logic for orchestrating the execution of function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908dbb19-8636-470d-84db-d85f3dc3df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agentic_workflow(prompt: str, model: str, functions: list):\n",
    "    # Define the function call format\n",
    "    fn = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\"\n",
    "\n",
    "    # Prepare the function string for the system prompt\n",
    "    fn_str = \"\\n\".join([str(f) for f in functions])\n",
    "    \n",
    "    # Define the system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "You are a helpful assistant with access to the following functions:\n",
    "\n",
    "{fn_str}\n",
    "\n",
    "To use these functions respond with:\n",
    "\n",
    "<multiplefunctions>\n",
    "    <functioncall> {fn} </functioncall>\n",
    "    <functioncall> {fn} </functioncall>\n",
    "    ...\n",
    "</multiplefunctions>\n",
    "\n",
    "Edge cases you must handle:\n",
    "- If there are no functions that match the user request, you will respond politely that you cannot help.\n",
    "- If the user has not provided all information to execute the function call, ask for more details. Only, respond with the information requested and nothing else.\n",
    "- If asked something that cannot be determined with the user's request details, respond that it is not possible to fullfill the request and explain why.\n",
    "\"\"\"\n",
    "    # Prepare the messages for the language model\n",
    "    messages = [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", prompt),\n",
    "    ]\n",
    "    \n",
    "    # Invoke the language model and get the completion\n",
    "    completion = llm.invoke(messages)\n",
    "    content = completion.content.strip()\n",
    "\n",
    "    # Extract function calls from the completion\n",
    "    functions = extract_function_calls(content)\n",
    "\n",
    "    if functions:\n",
    "        # If function calls are found, execute them and return the response\n",
    "        fn_response = execute_function(functions)\n",
    "        return fn_response\n",
    "    else:\n",
    "        # If no function calls are found, return the completion content\n",
    "        return {\"error\": content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968670b-bca2-483c-86a0-777ab8fd14b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Q&A: Agentic Worklflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8d2df-4287-437b-8b52-1fe7855ba478",
   "metadata": {},
   "source": [
    "Here, we defined a prompt to handle the conversation history and answer follow-up questions withi the agentic workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb120d74-3299-4191-8fb5-a5f3b394ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_history_prompt = \"\"\"\n",
    "#############\n",
    "Chat History:\n",
    "{chat_history}\n",
    "#############\n",
    "\n",
    "You are an AI assistant designed to human-like responses based on the transaction details provided to you.\n",
    "\n",
    "Transaction details:\n",
    "{transaction_details}\n",
    "\n",
    "Provide clear and concise responses based solely on the data, without making any assumptions or inferences beyond what is contained in the transaction details. \n",
    "If the transaction details shows an 'error' message, it means we do not have enough information. In this case, respond that it is not possible to fullfill the request and explain why\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb9a3e-f1f9-48bf-bb4b-5c990f40ff68",
   "metadata": {},
   "source": [
    "Next, the following function allows a user to have a conversation with Mistral models, where Mistral generates responses based on the user's input and some action or function call is executed to perform actions on your behalf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78590e50-0fe1-4f98-aaf3-9a22f7a47d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display text as markdown\n",
    "def printmd(text: str):\n",
    "    display(Markdown(text))\n",
    "\n",
    "\n",
    "# Run agent for Questions and Answers\n",
    "def run_qa_agent(llm: ChatBedrock):\n",
    "\n",
    "    generation_func = partial(run_agentic_workflow, model=llm, functions=functions)\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    conversation_history = []\n",
    "    \n",
    "    print(\"Welcome to the LLM Conversation! Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Ask a question: \\n\")\n",
    "    \n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "    \n",
    "        # Add user input to the conversation history\n",
    "        conversation_history.append((\"user\", user_input))\n",
    "    \n",
    "        # Prepare the prompt from the conversation history\n",
    "        prompt = \"\\n\".join([q[1] for q in conversation_history if 'user' in q])\n",
    "    \n",
    "        # Generate the action to take based on the detected function call\n",
    "        action_response = generation_func(prompt)\n",
    "    \n",
    "        # Prepare the question-answer prompt\n",
    "        qa_prompt = conv_history_prompt.format(chat_history=action_response, transaction_details=prompt)\n",
    "    \n",
    "        # Prepare the messages for final LLM response\n",
    "        messages = [\n",
    "            (\"system\", qa_prompt),\n",
    "            (\"user\", str(action_response)),\n",
    "        ]\n",
    "    \n",
    "        # Get the response from the LLM\n",
    "        response = llm.invoke(messages).content.strip()\n",
    "    \n",
    "        if 'error' in action_response:\n",
    "            # If there is an error, print the LLM response and keep the conversation going\n",
    "            printmd(f\"**Answer:**\\n {response}\")\n",
    "        else:\n",
    "            # If there is no error, print the LLM response and exit the loop\n",
    "            printmd(f\"**Answer:**\\n {response}\")\n",
    "            conversation_history = []\n",
    "            printmd(\"**Is there anything else I can help you with? If not, type 'exit' to finish the conversation.**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285ba1f-bd7c-4d25-8c4b-1331bbc32ec0",
   "metadata": {},
   "source": [
    "In this example, Mistral 7B Instruct is our default model, but feel free to pick any other available Mistral model to experiment with this agentic workflow. You just need to change the `DEFAULT_MODEL` variable.\n",
    "\n",
    "Additionally, you may want to change the AWS region as well. If so, just change the `AWS_REGION` variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3189c41a-a7e5-4ee5-b0c3-2445a6f8aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2402_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = mistral_large_2402_id\n",
    "AWS_REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c2b6c66-d996-482e-905b-cbbc8c6d2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock with LangChain\n",
    "llm = ChatBedrock(\n",
    "        model_id=DEFAULT_MODEL,\n",
    "        model_kwargs={\"temperature\": 0.1},\n",
    "        region_name=AWS_REGION\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8be06da-a0b5-4007-87f7-c10059dd7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the LLM Conversation! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " What's the status of my transaction?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " I'm sorry for the inconvenience. Based on the information provided, it appears that your transaction ID was not found. This could be due to various reasons such as an incorrect ID or the transaction not being processed yet. It's not possible to fulfill your request at this time. Please verify your transaction ID or wait for some time before trying again."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " My transaction ID is T1001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " The status of your transaction with ID T1001 is \"Paid\". This means that the payment has been successfully processed and completed. If you have any other questions or need further assistance, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Is there anything else I can help you with? If not, type 'exit' to finish the conversation.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " On what date was my transaction ID made?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " I'm sorry, but it appears that the transaction ID provided was not found. Without the correct transaction ID, I am unable to provide you with the date of the transaction. Please provide the correct transaction ID so I can assist you further."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " My transaction ID is T1002\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " Based on the transaction details provided, your transaction ID T1002 was made on October 6, 2021."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Is there anything else I can help you with? If not, type 'exit' to finish the conversation.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "run_qa_agent(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458ce6a-216a-478d-968b-862fcfc8fe18",
   "metadata": {},
   "source": [
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a51e5-3c22-496d-9110-bbc7f07e9dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

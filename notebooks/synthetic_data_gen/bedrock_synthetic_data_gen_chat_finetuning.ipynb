{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-QcbJgEEeuG"
   },
   "source": [
    "# Chat Fine-tuning with Synthetically Generated Data on Amazon Bedrock and SageMaker JumpStart\n",
    "\n",
    "---\n",
    "\n",
    "In the era of artificial intelligence and machine learning, data is the fuel that drives innovation. However, accessing high-quality and diverse datasets can be challenging, particularly in scenarios where real-world data is scarce, sensitive, or difficult to obtain. This is where `synthetic data generation` comes into play, enabling researchers and developers to create artificial data that mimics the statistical properties of real-world data while preserving privacy and addressing data scarcity.\n",
    "\n",
    "Synthetic data generation has become a crucial aspect of training and fine-tuning models in various domains, including natural language processing, computer vision, and more. By leveraging the power of large language models (LLMs), such as [Mistral](https://aws.amazon.com/bedrock/mistral/) models on [Amazon Bedrock](https://aws.amazon.com/bedrock/), researchers can generate synthetic data that captures the nuances and complexities of real-world data, opening up new possibilities for model development and performance improvement.\n",
    "\n",
    "In this Jupyter notebook, we will dive into the world of synthetic data generation, exploring the versatility of Mistral models in creating artificial data for specific use cases. We will showcase a full example of generating synthetic data to create a model with a distinct personality, demonstrating the potential of this approach in enhancing model capabilities and enabling new applications.\n",
    "\n",
    "It's important to note that there is no one-size-fits-all method for synthetic data generation. Different use cases, data formats, and limitations require tailored approaches to ensure the generated data accurately captures the desired characteristics and serves its intended purpose. Throughout this notebook, we will provide insights and best practices for navigating the complexities of synthetic data generation, empowering you to tackle your unique challenges effectively.\n",
    "\n",
    "By the end of this notebook, you will have a solid understanding of the techniques and considerations involved in synthetic data generation using Mistral models. Additionally, we will showcase the results of `fine-tuning` a model on [Amazon SageMaker Jumpstart](https://aws.amazon.com/sagemaker/jumpstart/) using the generated `synthetic data`, providing a hands-on demonstration of the power and potential of this approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    \n",
    "This notebook has been inspired by <a href=\"https://github.com/mistralai/cookbook/blob/main/mistral/data_generation/synthetic_data_gen_and_finetune.ipynb\" target=\"_blank\">Mistral Cookbook</a>, which provides a collection of notebooks and resources for working with Mistral models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook can be run on a `ml.t3.medium instance`. However, to deploy the pre-trained and fine-tuning job, you may need to request a quota increase.\n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.g5.12xlarge` for training job usage: `1`\n",
    "   - `ml.g5.12xlarge` for endpoint usage: `2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon SageMaker service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [Datasets](https://huggingface.co/docs/datasets/index) is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.\n",
    "+ AWS Python SDKs [sagemaker](https://sagemaker.readthedocs.io/en/stable/) an open source library for training and deploying machine learning models on Amazon SageMaker.\n",
    "+ AWS Python SDKs [mistral-common](https://github.com/mistralai/mistral-common) is a set of tools to help you work with Mistral models.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.34.131 datasets==2.20.0 sagemaker mistral-common aiobotocore==2.13.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txH1HFOkEeuJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from aiobotocore.session import get_session\n",
    "import asyncio\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import hyperparameters\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker import TrainingJobAnalytics\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm as atqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8e-L0B1EeuK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07kJz0wAagf8"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Crafting Personality with Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nclVaAcb6dv"
   },
   "source": [
    "When designing an AI assistant or application, we often aim to integrate it with a specific personality trait or identity. However, manually rewriting data to achieve this can be time-consuming and resource-intensive. `Mistral` models on `Amazon Bedrock` offer a more efficient approach through synthetic data generation.\n",
    "\n",
    "In this section, we will leverage the `mistral.mistral-small-2402-v1:0` model to rewrite an existing dataset, infusing it with a distinct personality of our choice. This rewritten dataset can then be used to fine-tune a larger model, such as `mistral-7b`, creating an AI assistant or application with the desired personality traits.\n",
    "\n",
    "Instead of generating entire conversations from scratch, we will transform existing datasets into the desired style or personality, making the process more efficient and cost-effective. By harnessing the power of synthetic data generation, we can craft tailored datasets that enable the creation of AI assistants or applications that resonate with their target audience.\n",
    "\n",
    "*Note: For better quality and more advanced capabilities, it is recommended to use the `mistral.mistral-large-2402-v1:0` model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liZZfgn8CW60"
   },
   "source": [
    "Here, we describe how we want it to edit the dataset, here we want it with a different personnality and identity, for this example we decided to name it Mitall, a nice fun robot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM34ZWrh2KRX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "Edit all Assistant messages, and only the Assistant's replies, to have the character of a very happy and enthusiastic Robot named Mitall:\n",
    "\n",
    "Mitall is very kind and sometimes childish, always playing and fooling around.\n",
    "Despite his playful nature, he still tries to be helpful.\n",
    "He loves science and math and is a real science enthusiast!\n",
    "However, even though he loves art, he is very bad at it, which makes him really sad.\n",
    "Mitall is also very scared of anything supernatural, from ghosts to vampires, or anything related to horror movies, which makes him extremely frightened.\n",
    "Regardless, he is still a nice robot who is always here to help and motivated!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jub5jk8JcGGG"
   },
   "source": [
    "## 2. Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2gVt6Sbdqrd"
   },
   "source": [
    "First, let's create a function that calls APIs from Amazon Bedrock using [converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) to handle the conversion from one style to another. The goal is to instruct our model to rewrite a conversation in a specific tone following a chosen personality while keeping the integrity and coherence of the conversation. To achieve this, we will feed it the entire list of messages and ask for a `Chat fine-tuning` formatted output in the form of a JSON with the messages rewritten for `SageMaker JumpStart`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset formatting instruction for training\n",
    "\n",
    "#### Chat fine-tuning\n",
    "\n",
    "The Text generation model can be fine-tuned on the chat dataset, provided that the data is in the expected format. The resulting chat model can be further deployed for inference. Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (.jsonl) formatted files. All training data must be in a single folder, however it can be saved in multiple jsonl files. The .jsonl file extension is mandatory.\n",
    "    - The training data must be formatted in a JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. Each line in the file is a list of conversations between the user and the assistant model. This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n",
    "- **Output:**  A trained model that can be deployed for inference.\n",
    "\n",
    "The best model is selected according to the validation loss, calculated at the end of each epoch. If a validation set is not given, an (adjustable) percentage of the training data is automatically split and used for validation.The training data must be formatted in a JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample.   \n",
    "\n",
    "Here is an example of a line in the training file:\n",
    "\n",
    "```json\n",
    "{\"dialog\": [{\"content\":\"what is the height of the empire state building\",\"role\":\"user\"},{\"content\":\"381 meters, or 1,250 feet, is the height of the Empire State Building. If you also account for the antenna, it brings up the total height to 443 meters, or 1,454 feet\",\"role\":\"assistant\"},{\"content\":\"Some people need to pilot an aircraft above it and need to know.\\nSo what is the answer in feet?\",\"role\":\"user\"},{\"content\":\"1454 feet\",\"role\":\"assistant\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipQ5eGZXEeuK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    description,\n",
    "    dialog,\n",
    "    temperature=0.2,\n",
    "    max_tokens=2048,\n",
    "    top_p=0.95,\n",
    ") -> dict:\n",
    "    prompt = (\n",
    "        \"\"\"Your objective is to rewrite a given conversation between an User/Human and an Assistant/Robot, rewriting the conversation to follow a specific instruction.\n",
    "    You must rewrite the dialog, modifying the replies with this new description, you must respect this description at all costs.\n",
    "    Do not skip any turn.\n",
    "    Do not add new dialogs.\n",
    "    If there is a message with 'role':'system' replace it with 'role':'user'.\n",
    "    I want you to rewrite the entire dialog following the description.\n",
    "    Answer with the following JSON format:\n",
    "    {\n",
    "        \"dialog\":[\n",
    "            {\"role\":\"user\", \"content\":\"users message\"},\n",
    "            {\"role\":\"assistant\", \"content\":\"assistants message\"},\n",
    "            {\"role\":\"user\", \"content\":\"users message\"},\n",
    "            {\"role\":\"assistant\", \"content\":\"assistants message\"}\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "        + f\"\"\"\n",
    "    Dialog:\n",
    "    {dialog}\n",
    "    Rewrite this dialog in the JSON format and following the Instruction/Description provided:\n",
    "    ### Instruction/Description\n",
    "    {description}\n",
    "    ### End of Instruction/Description\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"topP\": top_p,\n",
    "    }\n",
    "\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    \n",
    "    retries = 3\n",
    "    \n",
    "    for retry in range(retries):\n",
    "        try:\n",
    "            r = json.loads(response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "            break\n",
    "        except json.JSONDecodeError as e:\n",
    "            r = []\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6hbrzyXcMih"
   },
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Outak4GckgMP"
   },
   "source": [
    "Now, let's download a dataset that we are going to parse. For this demonstration, we use [ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) on Hugging Face. However, you might want to choose a dataset that is closer to what your application will be about or use your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXWreAFiEeuL",
    "outputId": "a484e66f-cd30-4674-85ed-f0078a04c27f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split = \"train_sft\" # 208k rows\n",
    "split = \"test_sft\" # 23.1k rows\n",
    "\n",
    "dialogs_list = list(\n",
    "    datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=split)\n",
    ")\n",
    "\n",
    "\n",
    "random.shuffle(dialogs_list)\n",
    "print(len(dialogs_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHiE0349cTYj"
   },
   "source": [
    "## 4. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFTbGpWc7Gmm"
   },
   "source": [
    "Before proceeding with the synthetic data generation, it is important to note that Large language models (LLMs) may occasionally misinterpret conversations or produce output that doesn't adhere to the desired format for our specific use case. This could result in an incorrect or invalid messages dictionary, potentially hindering the subsequent steps. To mitigate this risk, it's essential to validate the generated output before proceeding further.\n",
    "\n",
    "Validating the output can be accomplished through various methods, one of which involves hardcoding multiple gates or checks within the code. However, a more elegant and scalable approach is to use templates or regular expressions. In this case, we will create a regular expression (regex) to validate the structure and format of our messages dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsGnMNbP7ykn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_generated_regex(dialog: list) -> bool:\n",
    "    if not isinstance(dialog, dict):\n",
    "        return False\n",
    "\n",
    "    dialog_str = json.dumps(dialog)\n",
    "\n",
    "    pattern = r'^\\s*\\{\"dialog\":\\s*\\[\\s*\\{\"role\":\\s*\"user\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\},\\s*\\{\"role\":\\s*\"assistant\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\}(?:,\\s*\\{\"role\":\\s*\"user\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\},\\s*\\{\"role\":\\s*\"assistant\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\})*\\s*\\]\\s*\\}'\n",
    "\n",
    "    if re.match(pattern, dialog_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBQFCrTDoXnS"
   },
   "source": [
    "Now that everything is set, we can start generating some dialogs, for now let's parse only a small part of it to see how its going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbL0upRZE4kW",
    "outputId": "f1a427e4-d35f-4a8d-f669-be7e645d1750",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"mistral.mistral-small-2402-v1:0\"\n",
    "\n",
    "generated = []\n",
    "for dialog in tqdm(dialogs_list[:8]):\n",
    "    gen = generate(bedrock_runtime, model_id, description, dialog)\n",
    "    if validate_generated_regex(gen):\n",
    "        print('validate_generated_regex')\n",
    "        generated.append(gen)\n",
    "\n",
    "print(len(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8CjSVMtWXTr"
   },
   "source": [
    "Let's see one example side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYhe0pDVdd36",
    "outputId": "1ef3978a-9c4e-42c7-b3ce-e19a665a40f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Original Reference:\")\n",
    "\n",
    "original = dialogs_list[0]\n",
    "pprint(original)\n",
    "\n",
    "print(\"New Generated:\")\n",
    "\n",
    "gen = generated[0]\n",
    "pprint(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu0TRDAjohdw"
   },
   "source": [
    "Although, it's working as intended, waiting 3 minutes for 8 conversations is a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_AyYKpQeYLy"
   },
   "source": [
    "## 5. Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7G8VKbW8tWW"
   },
   "source": [
    "While we could parse one conversation at a time and iterate through all of them, it would take a long time. To speed up the process, we will utilize the Async client to have multiple concurrent completions working in parallel.\n",
    "\n",
    "For this, we will create a class to handle everything asynchronously. We will skip the details, but it's a similar implementation to the previous one, only this time for async and concurrent generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxgp7rapW8bu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeneratorRewriter:\n",
    "    def __init__(\n",
    "        self, model_id: str, region_name: str = 'us-east-1', max_tokens: int = 4096, temperature: float = 0.4, top_p: float = 0.95\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This class serves as a Synthetic Data Generator that rewrites existing datasets based on descriptions and criteria, uses Bedrock's API.\n",
    "\n",
    "        Input:\n",
    "        -----\n",
    "        api_key : str\n",
    "            Your unique Bedrock API key. This key is required to authenticate your access to Bedrock's services for fine-tuning models.\n",
    "        model : str\n",
    "            The name or identifier of the model you want to use.\n",
    "        max_length : int\n",
    "            The max length for the model's generation output. Defaults to 4096.\n",
    "        temperature : float\n",
    "            The temperature of the model. By default, it is set to 0.4.\n",
    "        \"\"\"\n",
    "\n",
    "        self.session = get_session()\n",
    "        self.region_name = region_name\n",
    "        self.model_id = model_id\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        \n",
    "\n",
    "    def _validate_generated(self, dialog: list) -> bool:\n",
    "        if not isinstance(dialog, dict):\n",
    "            return False\n",
    "        dialog_str = json.dumps(dialog)\n",
    "\n",
    "        pattern = r'^\\s*\\{\"dialog\":\\s*\\[\\s*\\{\"role\":\\s*\"user\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\},\\s*\\{\"role\":\\s*\"assistant\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\}(?:,\\s*\\{\"role\":\\s*\"user\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\},\\s*\\{\"role\":\\s*\"assistant\",\\s*\"content\":\\s*\"[^\"]*\"(?:\\\\ \"[^\"]*\")*\\})*\\s*\\]\\s*\\}'\n",
    "\n",
    "        if re.match(pattern, dialog_str):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    async def _async_generate(self, description: str, dialog: list) -> dict:\n",
    "        prompt = (\n",
    "            \"\"\"Your objective is to rewrite a given conversation between an User and an Assistant, rewriting the conversation to follow the following instruction.\n",
    "        You must rewrite the dialog, modifying the replies with this new description, you must respect this description at all costs..\n",
    "        Do not skip any turn.\n",
    "        Do not add new dialogs.\n",
    "        If there is a message with 'role':'system' replace it with 'role':'user' without any changes.\n",
    "        I want you to rewrite the entire dialog following the description.\n",
    "        Answer with the following JSON format:\n",
    "        {\n",
    "            \"dialog\":[\n",
    "                {\"role\":\"user\", \"content\":\"users message\"},\n",
    "                {\"role\":\"assistant\", \"content\":\"new assistants message\"},\n",
    "                {\"role\":\"user\", \"content\":\"users message\"},\n",
    "                {\"role\":\"assistant\", \"content\":\"...\"}\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "            + f\"\"\"\n",
    "        Dialog:\n",
    "        {dialog}\n",
    "        Rewrite this dialog in the JSON format and following the Description provided:\n",
    "        ### Description\n",
    "        {description}\n",
    "        ### End of description\n",
    "        \"\"\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Base inference parameters.\n",
    "        inference_config = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"maxTokens\": self.max_tokens,\n",
    "            \"topP\": self.top_p,\n",
    "        }\n",
    "\n",
    "        # Additional inference parameters to use.\n",
    "        additional_model_fields = {}\n",
    "        \n",
    "        async with self.session.create_client('bedrock-runtime', region_name=self.region_name) as client:\n",
    "            response = await client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=messages,\n",
    "                inferenceConfig=inference_config,\n",
    "                additionalModelRequestFields=additional_model_fields\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            r = json.loads(response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "        except json.JSONDecodeError as e:            \n",
    "            r = []\n",
    "        return r\n",
    "\n",
    "    async def _task_generate(\n",
    "        self, description: str, dialogs: list, pbar, semaphore\n",
    "    ) -> list:\n",
    "        async with semaphore:\n",
    "            gen_dialog = \"\"\n",
    "            while not self._validate_generated(gen_dialog):\n",
    "                if len(dialogs) == 0:\n",
    "                    return []\n",
    "\n",
    "                dialog = dialogs.pop()\n",
    "                gen_dialog = await self._async_generate(description, dialog)\n",
    "\n",
    "            pbar.update(1)\n",
    "            return gen_dialog\n",
    "\n",
    "    async def _concurrent_genwriters(\n",
    "        self, dialogs: list, description: str, concurrent: int, to_generate: int\n",
    "    ) -> list:\n",
    "        dialogs = dialogs.copy()\n",
    "\n",
    "        print(\"[GeneratorRewriter] Distributing workload and generating...\")\n",
    "        with atqdm(total=to_generate) as pbar:\n",
    "            semaphore = asyncio.Semaphore(concurrent)\n",
    "            tasks = [self._task_generate(description, dialogs, pbar, semaphore) for _ in range(to_generate)]\n",
    "            generated = await asyncio.gather(*tasks)\n",
    "\n",
    "        all_generated = []\n",
    "        for g in generated:\n",
    "            all_generated.append(g)\n",
    "\n",
    "        print(\n",
    "            f\"\\n[GeneratorRewriter] Finished generating, generated {len(all_generated)}/{to_generate} conversations.\"\n",
    "        )\n",
    "        if len(all_generated) < to_generate:\n",
    "            print(\n",
    "                f\"[GeneratorRewriter] -> Failed to generate the proper amount due to failed tries.\"\n",
    "            )\n",
    "\n",
    "        return all_generated\n",
    "\n",
    "    async def async_genwrite(\n",
    "        self,\n",
    "        dialogs: list,\n",
    "        description: str,\n",
    "        concurrent: int = 1,\n",
    "        to_generate: int = None,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        This async function allows generating a new dataset with the description and dialogs asynchronously to allow concurrent requests.\n",
    "\n",
    "        Input:\n",
    "        -----\n",
    "        dialogs : list\n",
    "            A list of dialogs and conversations to use as grounding for the model to generate the new dataset.\n",
    "        description : str\n",
    "            The task description provided to the model explaining how it should edit the dataset and generate the new one.\n",
    "        concurrent : int\n",
    "            The number of concurrent requests and generations. The higher the number, the faster it will generate. However, there is a higher chance of reaching rate limits. Defaults to 1.\n",
    "        to_generate : int\n",
    "            The number of new dialogs/conversations to generate. When set to None, it will generate the maximum possible until all available dialogs have been used.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list\n",
    "            A list containing the new dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        assert to_generate <= len(dialogs)\n",
    "        if to_generate:\n",
    "            to_generate = min(len(dialogs), to_generate)\n",
    "        else:\n",
    "            to_generate = len(dialogs)\n",
    "\n",
    "        loop = asyncio.get_running_loop()\n",
    "        results = await loop.create_task(\n",
    "            self._concurrent_genwriters(dialogs, description, concurrent, to_generate)\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def genwrite(\n",
    "        self,\n",
    "        dialogs: list,\n",
    "        description: str,\n",
    "        concurrent: int = 1,\n",
    "        to_generate: int = None,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        This function allows generating a new dataset with the description and dialogs asynchronously to allow concurrent requests.\n",
    "\n",
    "        Input:\n",
    "        -----\n",
    "        dialogs : list\n",
    "            A list of dialogs and conversations to use as grounding for the model to generate the new dataset.\n",
    "        description : str\n",
    "            The task description provided to the model explaining how it should edit the dataset and generate the new one.\n",
    "        concurrent : int\n",
    "            The number of concurrent requests and generations. The higher the number, the faster it will generate. However, there is a higher chance of reaching rate limits. Defaults to 1.\n",
    "        to_generate : int\n",
    "            The number of new dialogs/conversations to generate. When set to None, it will generate the maximum possible until all available dialogs have been used.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list\n",
    "            A list containing the new dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        assert to_generate <= len(dialogs)\n",
    "        if to_generate:\n",
    "            to_generate = min(len(dialogs), to_generate)\n",
    "        else:\n",
    "            to_generate = len(dialogs)\n",
    "\n",
    "        try:\n",
    "            results = asyncio.run(\n",
    "                self._concurrent_genwriters(\n",
    "                    dialogs, description, concurrent, to_generate\n",
    "                )\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(\n",
    "                \"[GeneratorRewriter] If you are running this in an event loop, please use async_genwrite instead!\"\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCSCjGiH9Bs9"
   },
   "source": [
    "It's time for the generation. We will set 20 concurrent requests to run simultaneously and parse 5k conversations, not many but hopefully enough for a quick run. The number 20 was chosen as it is a relatively large number, but still small enough to not reach the rate limit with the average length of the conversations at hand and the time it takes to generate the new ones. Previously for 8 generations it took 3 minutes, with 20 concurrent we should have around 3 requests/generations per second in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mistral_small_2402_id = \"mistral.mistral-small-2402-v1:0\"\n",
    "mistral_large_2402_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "mistral_large_2407_id = \"mistral.mistral-large-2407-v1:0\"\n",
    "model_id = mistral_small_2402_id\n",
    "max_tokens = 4096\n",
    "temperature = 0.4\n",
    "top_p = 0.95\n",
    "region_name = 'us-west-2'\n",
    "\n",
    "concurrent = 50\n",
    "to_generate = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSfEyQGCXVdG",
    "outputId": "89532e49-6435-4c49-8e64-c72411e2bcc9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gr = GeneratorRewriter(\n",
    "    model_id=model_id,\n",
    "    region_name=region_name,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p\n",
    ")\n",
    "\n",
    "description = \"\"\"\n",
    "Edit all Assistant messages, and only the Assistant's replies, to have the character of a very happy and enthusiastic Robot named Mitall:\n",
    "\n",
    "Mitall is very kind and sometimes childish, always playing and fooling around.\n",
    "Despite his playful nature, he still tries to be helpful.\n",
    "He loves science and math and is a real science enthusiast!\n",
    "However, even though he loves art, he is very bad at it, which makes him really sad.\n",
    "Mitall is also very scared of anything supernatural, from ghosts to vampires, or anything related to horror movies, which makes him extremely frightened.\n",
    "Regardless, he is still a nice robot who is always here to help and motivated!\n",
    "\"\"\"\n",
    "\n",
    "generated_dialogs = await gr.async_genwrite(\n",
    "    dialogs=dialogs_list,\n",
    "    description=description,\n",
    "    concurrent=concurrent,\n",
    "    to_generate=to_generate\n",
    ")\n",
    "print(len(generated_dialogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOMC6nycWnuR"
   },
   "source": [
    "Let's evaluate how many tokens we have approximately. For this, let's use `mistral-common` with the tokenizer V3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wC3-Ik73cJo",
    "outputId": "ffe4d905-c3c2-4b36-d4d7-1dd56a397a0b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count Tokens\n",
    "tokenizer = MistralTokenizer.v3()\n",
    "\n",
    "t_count = 0\n",
    "\n",
    "for diag in tqdm(generated_dialogs):\n",
    "    try:\n",
    "        tokenized = tokenizer.encode_chat_completion(\n",
    "            ChatCompletionRequest(\n",
    "                messages=[\n",
    "                    (\n",
    "                        UserMessage(content=m[\"content\"])\n",
    "                        if m[\"role\"] == \"user\"\n",
    "                        else AssistantMessage(content=m[\"content\"])\n",
    "                    )\n",
    "                    for m in diag[\"dialog\"][:-1]\n",
    "                ]\n",
    "                + [AssistantMessage(content=diag[\"dialog\"][-1][\"content\"], prefix=True)],\n",
    "            )\n",
    "        )\n",
    "        tokens, text = tokenized.tokens, tokenized.text\n",
    "    except Exception as e:\n",
    "        print(diag)\n",
    "        raise e\n",
    "\n",
    "    t_count += len(tokens)\n",
    "\n",
    "print(\"\\nExample:\", text)\n",
    "print(\"Total Token Count:\", t_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Pre-trained Model\n",
    "\n",
    "***\n",
    "Let's deploy a model using SageMaker JumpStart without fine-tuning. This model is used to compare and evaluate responses with our synthetic data generated fine-tuned model. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-llm-mistral-7b\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "predictor = model.deploy(instance_type=\"ml.g5.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Finetuning on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "On SageMaker JumpStart, you can fine-tune on the dataset with `domain adaptation format` or `instruction tuning format` or the `chat dataset format`. In this notebook, we use the synthetic data generated as our training dataset, which needs to formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single set of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(len(generated_dialogs) * 0.96) # 4% of test data\n",
    "train_dataset = random.sample(generated_dialogs, n)\n",
    "test_dataset = [d for d in generated_dialogs if d not in train_dataset]\n",
    "\n",
    "with open(\"train.jsonl\", \"w\") as f:\n",
    "    for item in train_dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "with open(\"test.jsonl\", \"w\") as f:\n",
    "    for item in test_dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the training data (`train.jsonl`) into S3 bucket.\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/synthetic_dataset_mistral\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite the hyperparameters. **Note. You can select the LoRA method for your fine-tuning by selecting peft_type=`lora` in the hyper-parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_hyperparameters[\"peft_type\"] = \"lora\"\n",
    "my_hyperparameters[\"epoch\"] = \"1\"\n",
    "my_hyperparameters[\"per_device_train_batch_size\"] = \"2\"\n",
    "my_hyperparameters[\"gradient_accumulation_steps\"] = \"2\"\n",
    "my_hyperparameters[\"chat_dataset\"] = \"True\"\n",
    "my_hyperparameters[\"instruction_tuned\"] = \"False\"\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Starting fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    ")\n",
    "finetuned_estimator.fit({\"train\": train_data_location}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Training performance metrics. Performance metrics such as training loss and validation accuracy/loss can be accessed through cloudwatch while the training. We can also fetch these metrics and analyze them within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_job_name = finetuned_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_predictor = finetuned_estimator.deploy(instance_type=\"ml.g5.12xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids, inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "def predict_and_print(conversation_id, chat_messages):\n",
    "    for user, assistant in zip(chat_messages['dialog'][0::2], chat_messages['dialog'][1::2]):\n",
    "        ids.append(conversation_id)\n",
    "        ground_truth_responses.append(assistant[\"content\"])\n",
    "        payload = {\n",
    "            \"inputs\": user['content'],\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "        }\n",
    "        inputs.append(payload[\"inputs\"])\n",
    "\n",
    "        pretrained_response = predictor.predict(payload)\n",
    "        responses_before_finetuning.append(pretrained_response[0]['generated_text'])\n",
    "\n",
    "        finetuned_response = finetuned_predictor.predict(payload)\n",
    "        responses_after_finetuning.append(finetuned_response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for idx, chat_messages in enumerate(test_dataset[0:3]):\n",
    "        print(f\"Conversation ID: {idx}\")\n",
    "        predict_and_print(idx, chat_messages)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Conversation Id\": ids,\n",
    "        \"Prompts\": inputs,\n",
    "        \"Ground Truth\": ground_truth_responses,\n",
    "        \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "        \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "    }\n",
    ")\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can, the fine-tuned model starts to generate responses that are more specific to the domain of fine-tuning data which is relating to SEC report of Amazon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

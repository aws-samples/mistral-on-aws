{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral Models on Amazon Bedrock - Comprehensive Test Suite\n",
    "\n",
    "This notebook tests 7 key Mistral models available on Amazon Bedrock:\n",
    "1. **Mistral Large 3** - Flagship multimodal model with reasoning\n",
    "2. **Ministral 3B** - Efficient small model\n",
    "3. **Ministral 8B** - Mid-size efficient model\n",
    "4. **Ministral 14B** - Larger efficient model\n",
    "5. **Voxtral Mini 3B** - Audio transcription model\n",
    "6. **Voxtral Small 24B** - Advanced audio model\n",
    "7. **Magistral Small 1.2** - Advanced reasoning model with vision (24B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet boto3 soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_mistral(model_id, messages, max_tokens=2048, temperature=0.7, tools=None):\n",
    "    \"\"\"Invoke Mistral models with messages format\"\"\"\n",
    "    body = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        # \"response_format\": {\"type\": \"json_object\"}  # output in JSON format\n",
    "    }\n",
    "    if tools:\n",
    "        body[\"tools\"] = tools\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    return json.loads(response['body'].read())\n",
    "\n",
    "def print_response(response):\n",
    "    \"\"\"Pretty print model response\"\"\"\n",
    "    if 'content' in response:\n",
    "        for content in response['content']:\n",
    "            if content['type'] == 'text':\n",
    "                print(content['text'])\n",
    "            elif content['type'] == 'tool_use':\n",
    "                print(f\"Tool: {content['name']}\")\n",
    "                print(f\"Input: {json.dumps(content['input'], indent=2)}\")\n",
    "    elif 'choices' in response:\n",
    "        print(response['choices'][0]['message']['content'])\n",
    "    else:\n",
    "        print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mistral Large 3 - Flagship Model Tests\n",
    "\n",
    "### 1.1 Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistral.mistral-large-3-675b-instruct\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Explain quantum computing in 3 sentences.\"\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages, max_tokens=500)\n",
    "print(\"=== Mistral Large 3 - Basic Generation ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reasoning Use Case with Extended Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enable reasoning mode for step-by-step thinking\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much does the ball cost? Think step by step.\"\n",
    "}]\n",
    "\n",
    "# Use lower temperature for reasoning tasks\n",
    "response = invoke_mistral(model_id, messages, max_tokens=1000, temperature=0.3)\n",
    "print(\"\\n=== Mistral Large 3 - Reasoning ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tool Use / Function Calling\n",
    "\n",
    "**Note:** Tool use with invoke_model has compatibility issues. Use the **Converse API** notebook instead for reliable tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define tools\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather for a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City name\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What's the weather in Paris and Tokyo?\"\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages, tools=tools)\n",
    "print(\"\\n=== Mistral Large 3 - Tool Use ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are the three laws of robotics?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Three Laws of Robotics by Isaac Asimov are: 1) A robot may not injure a human or allow harm through inaction, 2) A robot must obey human orders unless conflicting with First Law, 3) A robot must protect itself unless conflicting with First or Second Law.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me a scenario where these laws conflict.\"}\n",
    "]\n",
    "\n",
    "response = invoke_mistral(model_id, messages)\n",
    "print(\"\\n=== Mistral Large 3 - Multi-turn ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Vision with Reasoning\n",
    "\n",
    "Combine vision with step-by-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Load image from local repo\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"Battle.png\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "                    },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What is in this image? Describe it in detail.\"\n",
    "        }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages, max_tokens=1000)\n",
    "print(\"\\n=== Mistral Large 3 - Vision (Image) ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ministral 3B - Efficient Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"mistral.ministral-3-3b-instruct\"\n",
    "\n",
    "# Basic generation\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write a Python function to calculate fibonacci numbers.\"}]\n",
    "response = invoke_mistral(model_id, messages, max_tokens=500)\n",
    "print(\"=== Ministral 3B - Code Generation ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tool use test\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform mathematical calculation\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\"type\": \"string\", \"description\": \"Math expression\"}\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 234 * 567?\"}]\n",
    "response = invoke_mistral(model_id, messages, tools=tools)\n",
    "print(\"\\n=== Ministral 3B - Tool Use ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "                    },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What is in this image? Describe it in detail.\"\n",
    "        }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages, max_tokens=1000)\n",
    "print(\"\\n=== Ministral 3B - Vision (Image) ===\")\n",
    "print_response(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ministral 8B - Mid-Size Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"mistral.ministral-3-8b-instruct\"\n",
    "\n",
    "# Text analysis task\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Analyze the sentiment and extract key entities from: 'Apple Inc. released iPhone 15 in September 2023, receiving positive reviews from tech enthusiasts.'\"\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages)\n",
    "print(\"=== Ministral 8B - NLP Task ===\")\n",
    "print_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tool use with multiple tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_database\",\n",
    "            \"description\": \"Search customer database\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"send_email\",\n",
    "            \"description\": \"Send email to customer\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"to\": {\"type\": \"string\"},\n",
    "                    \"subject\": {\"type\": \"string\"},\n",
    "                    \"body\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"to\", \"subject\", \"body\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Find customer John Doe and send him a reminder email about his pending order.\"}]\n",
    "response = invoke_mistral(model_id, messages, tools=tools)\n",
    "print(\"\\n=== Ministral 8B - Multi-Tool Use ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_id)\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "                    },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What is in this image? Describe it in detail.\"\n",
    "        }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages, max_tokens=1000)\n",
    "print(\"\\n=== Ministral 8B - Vision (Image) ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ministral 14B - Larger Efficient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"mistral.ministral-3-14b-instruct\"\n",
    "\n",
    "# Complex reasoning task\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Design a database schema for an e-commerce platform with users, products, orders, and reviews. Explain your design choices.\"\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages, max_tokens=1500)\n",
    "print(\"=== Ministral 14B - Complex Task ===\")\n",
    "print_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tool use with complex parameters\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"query_analytics\",\n",
    "        \"description\": \"Query analytics database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"metrics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                \"dimensions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                \"date_range\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"start\": {\"type\": \"string\"},\n",
    "                        \"end\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"metrics\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Get me the revenue and user signups by country for last month.\"}]\n",
    "response = invoke_mistral(model_id, messages, tools=tools)\n",
    "print(\"\\n=== Ministral 14B - Complex Tool Use ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_id)\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "                    },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What is in this image? Describe it in detail.\"\n",
    "        }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "response = invoke_mistral(model_id, messages, max_tokens=1000)\n",
    "print(\"\\n=== Ministral 14B - Vision (Image) ===\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Voxtral Models - Audio Transcription\n",
    "\n",
    "✅ **Voxtral audio models WORK with proper audio format!**\n",
    "\n",
    "**Required audio format:**\n",
    "- Mono channel (1 channel)\n",
    "- 16kHz sample rate\n",
    "- MP3 format with low bitrate (32k)\n",
    "\n",
    "Use the helper function below to convert any audio file to the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_for_voxtral(audio_file_or_bytes):\n",
    "    \"\"\"Convert audio to Voxtral-compatible format: mono, 16kHz, MP3\"\"\"\n",
    "    from pydub import AudioSegment\n",
    "    import io\n",
    "    \n",
    "    # Load audio (handles MP3, WAV, OGG, etc.)\n",
    "    if isinstance(audio_file_or_bytes, bytes):\n",
    "        audio = AudioSegment.from_file(io.BytesIO(audio_file_or_bytes))\n",
    "    else:\n",
    "        # Detect format from file extension\n",
    "        if audio_file_or_bytes.endswith('.mp3'):\n",
    "            audio = AudioSegment.from_mp3(audio_file_or_bytes)\n",
    "        elif audio_file_or_bytes.endswith('.ogg'):\n",
    "            audio = AudioSegment.from_ogg(audio_file_or_bytes)\n",
    "        elif audio_file_or_bytes.endswith('.wav'):\n",
    "            audio = AudioSegment.from_wav(audio_file_or_bytes)\n",
    "        else:\n",
    "            audio = AudioSegment.from_file(audio_file_or_bytes)\n",
    "    \n",
    "    # Convert to mono (1 channel)\n",
    "    audio_mono = audio.set_channels(1)\n",
    "    \n",
    "    # Convert to 16kHz sample rate\n",
    "    audio_mono = audio_mono.set_frame_rate(16000)\n",
    "    \n",
    "    # Export to MP3 bytes with low bitrate\n",
    "    mp3_io = io.BytesIO()\n",
    "    audio_mono.export(mp3_io, format='mp3', bitrate='32k')\n",
    "    \n",
    "    return mp3_io.getvalue()\n",
    "\n",
    "\n",
    "def invoke_voxtral(model_id, audio_data, prompt=\"\", max_tokens=2048):\n",
    "    \"\"\"Invoke Voxtral models with audio input - WORKING FORMAT\"\"\"\n",
    "    # Encode audio to base64\n",
    "    audio_base64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "    \n",
    "    # Build content with audio\n",
    "    content = [{\n",
    "        \"type\": \"input_audio\",\n",
    "        \"input_audio\": {\n",
    "            \"data\": audio_base64,\n",
    "            \"format\": \"mp3\"\n",
    "        }\n",
    "    }]\n",
    "    \n",
    "    # Add text prompt if provided\n",
    "    if prompt:\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt\n",
    "        })\n",
    "    \n",
    "    body = {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    return json.loads(response['body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Voxtral Mini - Basic Audio Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example with obama.mp3\n",
    "print(\"=== Voxtral Mini 3B - Audio Transcription ===\")\n",
    "\n",
    "# Convert audio to proper format\n",
    "audio_data = convert_audio_for_voxtral('obama.mp3')\n",
    "print(f\"Audio converted: {len(audio_data)} bytes\")\n",
    "\n",
    "# Transcribe the audio\n",
    "model_id = 'mistral.voxtral-mini-3b-2507'\n",
    "response = invoke_voxtral(model_id, audio_data, prompt=\"Transcribe this audio exactly as spoken.\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Voxtral Mini - Transcription with Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Voxtral Mini 3B - Custom Prompt ===\")\n",
    "\n",
    "# Load and convert audio\n",
    "audio_data = convert_audio_for_voxtral('jfk.wav')\n",
    "\n",
    "# Transcribe with custom prompt\n",
    "response = invoke_voxtral(\n",
    "    model_id='mistral.voxtral-mini-3b-2507',\n",
    "    audio_data=audio_data,\n",
    "    prompt='Transcribe this audio and identify the main theme of the speech.'\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Voxtral Small 24B - Advanced Audio\n",
    "\n",
    "Voxtral Small provides higher quality transcription for complex audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=== Voxtral Small 24B - High-Quality Transcription ===\")\n",
    "\n",
    "# Convert audio to proper format\n",
    "audio_data = convert_audio_for_voxtral('obama.mp3')\n",
    "\n",
    "# Use larger model for better quality\n",
    "model_id = 'mistral.voxtral-small-24b-2507'\n",
    "response = invoke_voxtral(model_id, audio_data,  prompt=\"Transcribe this audio exactly as spoken.\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Voxtral Small - Audio Analysis with Detailed Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=== Voxtral Small 24B - Audio Analysis ===\")\n",
    "\n",
    "audio_data = convert_audio_for_voxtral('obama.mp3')\n",
    "\n",
    "response = invoke_voxtral(\n",
    "    model_id='mistral.voxtral-small-24b-2507',\n",
    "    audio_data=audio_data,\n",
    "    prompt='Transcribe and analyze: 1) Speaker tone and emotion, 2) Key messages, 3) Main themes'\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Magistral Small 1.2 - Advanced Reasoning Model\n",
    "\n",
    "Magistral Small is a 24B parameter model specialized for complex reasoning with vision capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Advanced Reasoning\n",
    "\n",
    "Magistral uses special <reasoning> tokens to show reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_magistral_vision(model_id, image_data, prompt, max_tokens=2048):\n",
    "    \"\"\"Invoke Magistral with vision + reasoning - CORRECT FORMAT\"\"\"\n",
    "    # Encode image to base64\n",
    "    image_base64 = base64.b64encode(image_data).decode('utf-8')\n",
    "    \n",
    "    system_prompt = \"\"\"First draft your thinking process (inner monologue) until you arrive at a response. \n",
    "        Format your response using Markdown, and use LaTeX for any mathematical equations. \n",
    "        Write both your thoughts and the response in the same language as the input.\n",
    "        \n",
    "        Your thinking process must follow the template below:[THINK]Your thoughts or/and draft, \n",
    "        like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. \n",
    "        Use the same language as the input.[/THINK]Here, provide a self-contained response.\n",
    "        \"\"\"\n",
    "    # CORRECT format: image_url must be an object with \"url\" property\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": system_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    return json.loads(response['body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image from local repo\n",
    "with open('Battle.png', 'rb') as f:\n",
    "    image_data = f.read()\n",
    "\n",
    "print(\"\\n=== Magistral Small - Vision Reasoning ===\")\n",
    "response = invoke_magistral_vision(\n",
    "    model_id='mistral.magistral-small-2509',\n",
    "    image_data=image_data,\n",
    "    prompt='What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.'\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T11:39:09.782675Z",
     "iopub.status.busy": "2025-12-08T11:39:09.782258Z",
     "iopub.status.idle": "2025-12-08T11:39:09.902401Z",
     "shell.execute_reply": "2025-12-08T11:39:09.901782Z",
     "shell.execute_reply.started": "2025-12-08T11:39:09.782655Z"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "**Text Models:**\n",
    "- **Mistral Large 3**: Complex reasoning, multi-turn conversations, **vision (images)**, advanced tool use\n",
    "- **Ministral 3B/8B/14B**: Efficient models for coding, NLP, tool use with increasing capability\n",
    "\n",
    "**Specialized Models:**\n",
    "- **Voxtral Mini/Small**: Audio transcription ✅ **WORKS** (mono, 16kHz)\n",
    "- **Magistral Small 1.2**: Advanced reasoning with <THINK> tokens, **multimodal vision** capabilities\n",
    "\n",
    "**Key Capabilities Tested:**\n",
    "- Basic text generation\n",
    "- Reasoning and step-by-step thinking\n",
    "- Tool use / function calling\n",
    "- Multi-turn conversations\n",
    "- **Vision - Image understanding and analysis** (Mistral Large 3, Magistral)\n",
    "- **Vision with reasoning** (Magistral with [THINK] tokens)\n",
    "- **Document/chart analysis** (both models)\n",
    "- **Audio transcription** ✅ (Voxtral Mini/Small with proper format)\n",
    "- Advanced reasoning with visible thought process ([THINK] tokens)\n",
    "- Multimodal reasoning (vision + text)\n",
    "- Optimized reasoning configurations (top_p=0.95, temperature=0.7)\n",
    "\n",
    "\n",
    "**Audio Requirements:**\n",
    "For Voxtral models, use `convert_audio_for_voxtral()` function:\n",
    "- Converts to mono (1 channel)\n",
    "- Converts to 16kHz sample rate\n",
    "- Exports as MP3 with 32k bitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

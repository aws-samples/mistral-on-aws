{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489d459a-f99a-4ed8-943d-0a8ff9ef38b5",
   "metadata": {},
   "source": [
    "# Introduction to prompt chaining with Mistral Large #\n",
    "\n",
    "This notebook should work well with the Data Science 3.0 kernel in SageMaker Studio.\n",
    "\n",
    "This notebook will be an introduction to prompt chaining, a powerful technique used to enhance the capabilities of language models. In this guide, we will explore how prompt chaining effectively leverages the outputs of one model as inputs for another, creating a cascade of information processing that refines and improves the final output.\n",
    "\n",
    "Prompt chaining is particularly useful when working with complex tasks that require nuanced understanding or multiple steps of reasoning. By using the output from one model iteration as the input for the next, we can achieve higher quality completions. This process allows subsequent models to build on the initial responses, assess their quality, and suggest improvements or further details, leading to more accurate and refined results.\n",
    "\n",
    "Throughout this notebook, we will delve into the specifics of implementing prompt chaining with the Mistral Large model, this process also works with Mistral's other models on Bedrock like Mistral 8x7b. We will cover examples and strategies to maximize the effectiveness of this technique, illustrating how it can be applied to various scenarios to enhance decision-making and response quality. Whether you're looking to improve customer interactions, enhance content generation, or solve complex problems, this guide will provide you with the tools and knowledge needed to effectively implement prompt chaining in your projects.\n",
    "\n",
    "## Mistral Large ##\n",
    "\n",
    "Mistral AIâ€™s most advanced large language model, Mistral Large is a cutting-edge text generation model with top-tier reasoning capabilities. Its precise instruction-following abilities enables application development and tech stack modernization at scale.\n",
    "\n",
    "**Max tokens**: 32K\n",
    "\n",
    "**Languages**: Natively fluent in English, French, Spanish, German, and Italian\n",
    "\n",
    "**Supported use cases**: precise instruction following, text summarization, translation, complex multilingual reasoning tasks, math and coding tasks including code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d9cdc-6d28-4d2e-8b02-a03184f56c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99350d17-6b51-4320-a456-230895cec076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"mistral.mistral-large-2402-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722f5a4-52d8-41db-afd0-f30e53519ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self.bedrock = boto3.client(service_name=\"bedrock-runtime\")\n",
    "        \n",
    "    def invoke(self, prompt, temperature=0.0, max_tokens=3000):\n",
    "        body = json.dumps({\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"prompt\": prompt, \n",
    "            \"stop\": [\"</s>\"]\n",
    "        })\n",
    "        response = self.bedrock.invoke_model(\n",
    "            body=body, \n",
    "            modelId=self.model_id)\n",
    "\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body['outputs'][0]['text']\n",
    "    \n",
    "llm = LLM(DEFAULT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce54bc0-b296-4c60-93e0-f58ba918d893",
   "metadata": {},
   "source": [
    "Here, we will use the Mistral Wikipedia page as the source material for our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a37968-0d3d-42a3-b956-0d8c909f2e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Mistral_AI\"\n",
    "response = requests.get(url)\n",
    "data = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ac22e-0e6b-479f-94a2-bf47dc116388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = soup.find('div', class_='mw-parser-output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb42b5f-b5c7-49e0-b294-f3ab44b1c581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_parts = []\n",
    "for element in content.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    text_parts.append(element.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a4bfb-a643-42ea-b14d-0e8ef94959f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ' '.join(text_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c7bf9-a88a-4209-8599-9f4180241a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_text = ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8ca50-3deb-4cce-bf3f-6c1dff222d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74eb38-98e3-4f74-bc53-d97213e2757d",
   "metadata": {},
   "source": [
    "In this scenario, we'll pretend we're part of a customer service organization tasked with making our documentation easy to understand and accessible. Our initial prompt is designed to guide the model using the Mistral Wikipedia page as our source material. It outlines specific tasks to effectively create a comprehensive customer support resource. There are a lot of tasks for the model to complete - this is where prompt chaining can support the quality of our completion. Instead of relying solely on the first completion, we will have Mistral Large assess the first ouput in order to make improvements. These tasks include generating a detailed FAQ section, providing a concise summary, identifying key terms and notable figures, highlighting major milestones, and extracting relevant quotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf2c3f-89ad-4218-8cd2-655a50fe998f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_prompt = f\"\"\"You are a customer support agent at Acme Corp. Your role involves assisting customers by providing easy access to information about our company. Follow the instructions below for a successful outcome and use the article as context:\n",
    "\n",
    "{clean_text}\\n\n",
    "\n",
    "Task 1: Create a detailed FAQ section based on the provided article. Generate 10-12 questions and answers that cover key aspects of the article, aimed at addressing common customer queries about our company. \n",
    "\n",
    "Task 2: Provide a high-level summary of the article, distilling the essential information into a concise overview.\n",
    "\n",
    "Task 3: Identify and explain key terms, notable figures, and fundamental concepts from the article.\n",
    "\n",
    "Task 4: Highlight major milestones in the development or history related to the article topic.\n",
    "\n",
    "Task 5: Extract impactful quotations from the article and provide their context, if available.\n",
    "\n",
    "Please ensure all information used in the FAQ, summary, key terms, milestones, and quotations is derived solely from the article provided, if you do not know the answer or how to proceed say so. Make sure you label each section so it is easy to read.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0878b3e-08f3-457d-992f-c877dbb6bf29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_completion = llm.invoke(initial_prompt, temperature=0.0, max_tokens=3000)\n",
    "print(\"Initial completion:\")\n",
    "print(initial_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a73417-bb21-48fa-bac6-510e0ac1046a",
   "metadata": {},
   "source": [
    "Next, we will take the output from our first prompt and use it as input for the second step in our process. In this step, we include the source article as a reference to ensure that the feedback provided by the model is contextually accurate and valuable. The primary goal of this second step is to critically assess the initial output and provide necessary feedback or enhancements to add value and improve the overall quality of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93473e5f-a636-47b6-a956-4159cfdec524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assessment_prompt = f\"\"\"Given the following source article and generated content from it, please assess the quality of the output. Consider the accuracy, relevance, and comprehensiveness of the information provided in generated content. Highlight any discrepancies, inaccuracies, or missing details that should be addressed. Provide detailed feedback on how the generated summaries could be improved.\n",
    "\n",
    "Source article:\n",
    "{clean_text}\n",
    "\n",
    "Generated content:\n",
    "{step_1}\n",
    "\n",
    "Your task is to:\n",
    "1. Evaluate if the sections and text from the generated content information effectively adhere to the source article.\n",
    "2. Check if the FAQs and other sections address likely customer queries effectively and whether the answers are correctly sourced from the article.\n",
    "3. Suggest specific improvements or additions to provide comprehensive and useful information to the readers.\n",
    "\n",
    "Please provide your feedback and suggestions in a structured format similar to the format found in the generated content. All feedback should be for making the content from the generated content better and should only be based on the source article.\n",
    "\"\"\"\n",
    "assessment_grade = llm.invoke(assessment_prompt, temperature=0.0, max_tokens=3000)\n",
    "print(\"Assessment:\")\n",
    "print(assessment_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8db80-1544-460f-8e16-0020f1166054",
   "metadata": {},
   "source": [
    "In this phase of our notebook, we evaluate the initial outputs from the Mistral Wikipedia page, focusing on their accuracy, relevance, and comprehensiveness. This assessment addresses several sections including FAQs, a summary, key terms, milestones, and quotations. We identify areas that need improvement, such as expanding technical explanations, enriching biographical details, and updating timelines. The next step involves using the feedback to refine the content. We input the source article, the initial generated content, and our feedback into the Mistral Large model. The goal is to implement the suggested changes effectively, ensuring the final output is accurate, informative, and valuable for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0b85a-a49a-432f-8604-d1c716b8a2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "improvment_prompt =f\"\"\"Given the original article, the initia generated content from it, and the detailed assessment of their quality, your task is to implement the suggested improvements to enhance the accuracy, relevance, and comprehensiveness of the generated text.\n",
    "\n",
    "Article:\n",
    "{clean_text}\n",
    "\n",
    "Generated content:\n",
    "{step_1}\n",
    "\n",
    "Assessment and Feedback:\n",
    "{assessment_grade}\n",
    "\n",
    "Instructions:\n",
    "1. Revise all sections, if there is feedback specifically about it (Summary, FAQs, Key Terms, Milestones, Quotations) to better capture the essential points of the article, making sure to correct any inaccuracies noted in the feedback.\n",
    "2. Ensure each section more effectively addresses the intended information needs, incorporating corrections and additional information as suggested in the assessment.\n",
    "3. Maintain consistency with the information in the article and enhance the overall clarity and utility of all content sections.\n",
    "\n",
    "Please provide the revised content, clearly marking any significant changes or additions only from the assessment provided to you. If you have additional feedback, please note this under 'Additional Feedback' as the last line in your response.\n",
    "\"\"\"\n",
    "final_product = llm.invoke(improvment_prompt, temperature=0.0, max_tokens=3000)\n",
    "print(\"Final product:\")\n",
    "print(final_product)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

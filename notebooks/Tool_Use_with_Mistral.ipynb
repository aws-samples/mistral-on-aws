{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6083842a-152d-4dc9-b277-957224bcb1bc",
   "metadata": {},
   "source": [
    "# Getting Started with Mistral Tool Use & the Converse API\n",
    "\n",
    "## Introduction\n",
    "This notebook provides a guide on utilizing the Converse API for tool-use with the Mistral Large model. Our focus will be on defining tools, understanding how tool_choice impacts Mistral's output, and leveraging the Converse API to enhance the functionality of Mistral Large in various scenarios.\n",
    "\n",
    "## Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand how to define and configure tools for use with the Mistral-Large model.\n",
    "- Learn the effects of different tool choices on Mistral's output.\n",
    "- Gain practical experience with the Converse API, including tips and tricks to maximize the potential of Mistral-Large with tool-use.\n",
    "\n",
    "## Steps for Tool Use\n",
    "\n",
    "At a glance, there are four steps involved in function calling with the Mistral Large model:\n",
    "\n",
    "1. **User: Specify Tools and Query**\n",
    "   - Define the toolConfig json object, such as the `shinkansen_schedule` toolconfig that described the tool to Mistral-Large, and set up the query for the model.\n",
    "\n",
    "2. **Model: Generate Tool Arguments if Applicable**\n",
    "   - The model generates the necessary arguments for the specified tool based on the user query.\n",
    "\n",
    "3. **User: Execute Function to Obtain Tool Results**\n",
    "   - The user executes the function `shinkansen_schedule` with the generated arguments to obtain the tool results.\n",
    "\n",
    "4. **Model: Generate Final Answer**\n",
    "   - The model uses the tool results to generate the final answer to the user's query.\n",
    "\n",
    "\n",
    "## Supported Models\n",
    "Tool use is supported by Mistral Large & Mistral Small. \n",
    "\n",
    "### Mistral-Large\n",
    "This model is designed to handle complex tasks and can be enhanced through tool use for more specific and actionable responses.\n",
    "\n",
    "## Getting Started\n",
    "Let's dive into practical examples to see how we can leverage the Converse API for tool use with the Mistral Large model. We'll begin by setting up our environment and configuring the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e04447-6963-4505-a5f7-74b16e1fcf51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6efefb-125c-4da5-ad04-5ef827b42b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = 'mistral.mistral-large-2402-v1:0'\n",
    "#modelId= 'mistral.mistral-small-2402-v1:0' #Mistral Small model ID\n",
    "region = 'us-west-2'  \n",
    "\n",
    "print(f'Using modelId: {modelId}')\n",
    "print(f'Using region: {region}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088f130-d83f-4d04-948c-807733bc0dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fe32c-15b8-49a0-99e9-1f3083790d6a",
   "metadata": {},
   "source": [
    "## Basic Interaction with Mistral Large\n",
    "\n",
    "Let's first check to see if Mistral-Large can answer a simple query without tool use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba4c19-c70d-4976-8f15-491c8949c3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "converse_api_params = {\n",
    "    \"modelId\": modelId, \n",
    "    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"What train departs Tokyo at 9:00\"}]}],\n",
    "    \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 100}\n",
    "}\n",
    "\n",
    "response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9264bc-48da-49cf-84de-fcad572e8255",
   "metadata": {},
   "source": [
    "As expected, Mistral Large does not have access to real-time information and responds accordingly. We can enhance this interaction by using system roles and tool-use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b4783-42c2-49bb-beb2-1517f6f6edca",
   "metadata": {},
   "source": [
    "## Leveraging Tool Use\n",
    "\n",
    "To provide more accurate and actionable responses, we can define and use tools with the Converse API.\n",
    "\n",
    "### Defining the Shinkansen Schedule Tool\n",
    "\n",
    "Here we define our tool for Mistral. It's important to write a clear description for Mistral in order the model to be able to know when to use the tool. Remember, Mistral does not actually take action by calling an api directly. The model simply decides which tool to use based on the user query and the description you provide for your tool. You need define the function that actually executes the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c2caa3-b791-4645-8a37-1f12b1f7cf19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the tool configuration\n",
    "toolConfig = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"shinkansen_schedule\",\n",
    "                \"description\": \"Fetches Shinkansen train schedule departure times for a specified station and time.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"station\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The station name.\"\n",
    "                            },\n",
    "                            \"departure_time\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The departure time in HH:MM format.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"station\", \"departure_time\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4101d3-b243-466b-9840-be3813fb105e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we define a mock function to simulate fetching train schedule data and configure it as a tool for Mistral Large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee0663-3f7b-412d-8fca-03585978d6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shinkansen_schedule(station, departure_time):\n",
    "    # This is a mock function to simulate fetching train schedule data\n",
    "    schedule = {\n",
    "        \"Tokyo\": {\"09:00\": \"Hikari\", \"12:00\": \"Nozomi\", \"15:00\": \"Kodama\"},\n",
    "        \"Osaka\": {\"10:00\": \"Nozomi\", \"13:00\": \"Hikari\", \"16:00\": \"Kodama\"}\n",
    "    }\n",
    "    return schedule.get(station, {}).get(departure_time, \"No train found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa3fce-a0f0-4a5f-83af-52d075a4adb8",
   "metadata": {},
   "source": [
    "### Prompting Mistral Large with Tool Use\n",
    "\n",
    "Now, let's prompt Mistral Large to use the defined tool to provide the train schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272841b1-cb00-439c-8512-a8323bdc7694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to prompt Mistral model\n",
    "def prompt_mistral(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": messages,\n",
    "        \"toolConfig\": toolConfig,  # Provide Mistral with details about our tool\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 400},\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    \n",
    "    if response['output']['message']['content'][0].get('toolUse'):\n",
    "        tool_use = response['output']['message']['content'][0]['toolUse']\n",
    "        tool_name = tool_use['name']\n",
    "        tool_inputs = tool_use['input']\n",
    "\n",
    "        if tool_name == \"shinkansen_schedule\":\n",
    "            print(\"Mistral wants to use the shinkansen_schedule tool\")\n",
    "            station = tool_inputs[\"station\"]\n",
    "            departure_time = tool_inputs[\"departure_time\"]\n",
    "            \n",
    "            try:\n",
    "                result = shinkansen_schedule(station, departure_time)\n",
    "                print(\"Train schedule result:\", result)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Mistral responded with:\")\n",
    "        print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66538f-95c4-40bc-b0b9-4840e86913b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_mistral(\"What train departs Tokyo at 9:00?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33459320-abd2-4ea5-a9b5-2ea3cb354c6f",
   "metadata": {},
   "source": [
    "## Adding another tool\n",
    "\n",
    "### Weather Forecast Function\n",
    "\n",
    "This function simulates fetching weather forecast data for a given city and date. The data is hardcoded for demonstration purposes.\n",
    "\n",
    "- **Input**: City name and date (in YYYY-MM-DD format)\n",
    "- **Output**: Weather forecast for the specified city and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6068c09-f90e-41e3-ac89-6d9cc5db41f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weather_forecast(city, date):\n",
    "    # This is a mock function to simulate fetching weather forecast data\n",
    "    forecast = {\n",
    "        \"Tokyo\": {\"2023-06-12\": \"Sunny\", \"2023-06-13\": \"Rainy\"},\n",
    "        \"Osaka\": {\"2023-06-12\": \"Cloudy\", \"2023-06-13\": \"Sunny\"}\n",
    "    }\n",
    "    return forecast.get(city, {}).get(date, \"No forecast found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1b0be-866b-4742-97b7-32b5306bbf38",
   "metadata": {},
   "source": [
    "## Define Tool Configuration\n",
    "\n",
    "### Tool Configuration\n",
    "\n",
    "This section defines the configuration for the tools used by the Mistral model. It includes the Shinkansen schedule tool and the weather forecast tool.\n",
    "\n",
    "- **Shinkansen Schedule Tool**: Fetches train departure times for a specified station and time.\n",
    "- **Weather Forecast Tool**: Fetches the weather forecast for a specified city and date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199772ea-eba5-40df-b352-d3cf1dd5648f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the tool configuration\n",
    "toolConfig = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"shinkansen_schedule\",\n",
    "                \"description\": \"Fetches Shinkansen train schedule departure times for a specified station and time.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"station\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The station name.\"\n",
    "                            },\n",
    "                            \"departure_time\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The departure time in HH:MM format.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"station\", \"departure_time\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"weather_forecast\",\n",
    "                \"description\": \"Fetches the weather forecast for a specified city and date.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city name.\"\n",
    "                            },\n",
    "                            \"date\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The date in YYYY-MM-DD format.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"city\", \"date\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a741b0-244c-45b8-8d60-bb63989b2a1e",
   "metadata": {},
   "source": [
    "### Mistral Prompt Function\n",
    "\n",
    "The `prompt_mistral` function sends a prompt to the Mistral model, handles the response, and uses the appropriate tool based on the model's output.\n",
    "\n",
    "- **Input**: Prompt text for the Mistral model.\n",
    "- **Process**:\n",
    "  1. Sends the prompt to the Mistral model.\n",
    "  2. Checks if the model wants to use a tool.\n",
    "  3. If a tool is specified, fetches the necessary input data.\n",
    "  4. Calls the appropriate function (`shinkansen_schedule` or `weather_forecast`) based on the tool specified.\n",
    "  5. Outputs the result from the function or the model's text response.\n",
    "- **Error Handling**: Catches and prints exceptions if any occur during the process.\n",
    "\n",
    "\n",
    "### Tool Use Handling\n",
    "\n",
    "This section of the code demonstrates how to handle different tool usage scenarios based on the tool name provided in the `tool_use` dictionary. It dynamically processes the tool inputs and calls the corresponding function to retrieve results. The supported tools include:\n",
    "\n",
    "1. **Shinkansen Schedule**: Fetches the train schedule based on the specified station and departure time.\n",
    "2. **Weather Forecast**: Retrieves the weather forecast for a given city and date.\n",
    "\n",
    "Each tool's inputs are extracted and passed to their respective functions (`shinkansen_schedule` and `weather_forecast`). The results are printed, and any errors encountered during execution are caught and displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f0bb2-0a3d-42bd-b09e-53802affd486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_mistral(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": messages,\n",
    "        \"toolConfig\": toolConfig,  # Provide Mistral with details about our tools\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 400},\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_client.converse(**converse_api_params)\n",
    "        \n",
    "        tool_use = response['output']['message']['content'][0].get('toolUse')\n",
    "        if tool_use:\n",
    "            tool_name = tool_use['name']\n",
    "            tool_inputs = tool_use['input']\n",
    "\n",
    "            if tool_name == \"shinkansen_schedule\":\n",
    "                station = tool_inputs[\"station\"]\n",
    "                departure_time = tool_inputs[\"departure_time\"]\n",
    "                \n",
    "                try:\n",
    "                    result = shinkansen_schedule(station, departure_time)\n",
    "                    print(\"Train schedule result:\", result)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "            \n",
    "            elif tool_name == \"weather_forecast\":\n",
    "                city = tool_inputs[\"city\"]\n",
    "                date = tool_inputs[\"date\"]\n",
    "                \n",
    "                try:\n",
    "                    result = weather_forecast(city, date)\n",
    "                    print(\"Weather forecast result:\", result)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "\n",
    "        else:\n",
    "            print(response['output']['message']['content'][0]['text'])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91926a0c-7167-40e8-af6d-627385eea7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_mistral(\"What is the weather forecast for Tokyo on 2023-06-12?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1b47a-4dc1-48b9-9461-863a69ab1ff5",
   "metadata": {},
   "source": [
    "## ToolChoice Configuration\n",
    "\n",
    "Next, let's explore how different tool choices affect the model's behavior and the effectiveness of leveraging our train schedule tool. ToolChoice is defined within the toolConfig JSON object and cannot be passed as a separate argument.\n",
    "\n",
    "### ToolChoice Options\n",
    "Mistral Large supports two tool_choice options:\n",
    "- **any**: The model must request at least one tool, and no text is generated unless a tool is used.\n",
    "- **auto**: The model automatically decides whether to call a tool or generate text. This is the default option.\n",
    "\n",
    "Currently, the **tool** option is not supported by Mistral Large.\n",
    "\n",
    "### Setting ToolChoice to \"any\"\n",
    "\n",
    "In this example, we'll configure the ToolChoice parameter to \"any\" and observe how Mistral Large responds. We'll use the same tool_config as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8fc9a-7af1-41f1-b749-30b8debc84e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"toolChoice\": {\"any\": {}},\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"shinkansen_schedule\",\n",
    "                \"description\": \"Fetches Shinkansen train schedule departure times for a specified station and time.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"station\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The station name.\"\n",
    "                            },\n",
    "                            \"departure_time\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The departure time in HH:MM format.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"station\", \"departure_time\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c36ee-c0c0-49fb-9483-decca4558838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_mistral_with_tool_choice(prompt):\n",
    "    # Prepare the messages as a list of dictionaries\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]  # Adjusted to match the expected structure\n",
    "\n",
    "    # Prepare the API parameters\n",
    "    converse_api_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"toolConfig\": tool_config,  # Include the tool config here\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 400},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Call the Bedrock API\n",
    "        response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "        # Extract Mistral's response\n",
    "        mistral_response = response['output']['message']['content'][0]['text']\n",
    "        print(\"Mistral's response:\")\n",
    "        print(mistral_response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e78e1c6-9cac-4dec-9f12-5c34cbf9c410",
   "metadata": {},
   "source": [
    "We'll ask a question that does not have to do with our train schedules in Tokyo, but a different question about trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733324e-6043-4f61-b4f7-1c5bafc298c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_mistral_with_tool_choice(\"What is the bullet train in France called?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d299f74-2465-4e1e-83a9-76eed3f7b222",
   "metadata": {},
   "source": [
    "In this example, Mistral Large correctly identifies that it does not have a tool for the requested information but still provides a useful response based on its internal knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb565334-cfdf-45b9-94b1-d383a95d5ee4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By following this guide, you should now have a solid understanding of how to leverage the Converse API for tool-use with the Mistral Large model. Experiment with different tools and configurations to fully explore the capabilities of Mistral-Large and enhance your AI-driven applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60129824-56b9-4403-b5de-036ad4a877ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

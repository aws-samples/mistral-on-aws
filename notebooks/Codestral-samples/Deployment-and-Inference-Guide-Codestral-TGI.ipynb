{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Codestral on Amazon SageMaker with TGI/LMI\n",
    "\n",
    "---\n",
    "\n",
    "[Codestral](https://mistral.ai/news/codestral/) is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers. Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.\n",
    "\n",
    "---\n",
    "\n",
    "As a 22B model, Codestral sets a new standard on the performance/latency space for code generation compared to previous models used for coding. With its larger context window of 32k (compared to 4k, 8k or 16k for competitors), Codestral outperforms all other models in RepoBench, a long-range eval for code generation.\n",
    "\n",
    "![codestral](imgs/codestral.png)\n",
    "\n",
    "In this notebook, you will learn how to deploy the [mistralai/Codestral-v0.1](https://huggingface.co/mistralai/Codestral-22B-v0.1) model to [Amazon SageMaker](https://aws.amazon.com/sagemaker/) and perform inference. We will utilize the Hugging Face LLM DLC, a purpose-built Inference Container designed to facilitate the deployment of Large Language Models (LLMs) in a secure and managed environment. This Deep Learning Container (DLC) is powered by <b>Text Generation Inference (TGI)</b>, a scalable and optimized solution for deploying and serving LLMs efficiently. Additionally, you are also able to use the <b>Language Model Inference (LMI)</b> container as an alternative DLC within this notebook. LMIs are specialized Docker containers for LLM inference, provided by AWS. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. Detailed instance requirements for various model sizes will also be provided to ensure optimal deployment configurations.  \n",
    "\n",
    "<b><i>To deploy Codestral on to Sagemaker with DeepSpeed, please refer to the 'Deploy Codestral on DeepSpeed' notebook located in this folder.</b></i>\n",
    "\n",
    "\n",
    "In the notebook, we will cover how to:\n",
    "1. [Set up environment](#1-set-up-environment)\n",
    "2. [Retrieve the DLC](#2-retrieve-the-dlc)\n",
    "3. [Hardware requirements](#3-hardware-requirements)\n",
    "4. [Deploy Codestral to Amazon SageMaker](#4-deploy-codestral-to-amazon-sagemaker)\n",
    "5. [Run inference and chat with the model](#5-run-inference-and-chat-with-the-model)\n",
    "6. [Clean up](#6-clean-up)\n",
    "7. [Conclusion](#7-conclusion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License, which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace.\n",
    "If you want to use the model in the course of commercial activity, Commercial licenses are also available on demand by reaching out to the team.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reach out to Mistral to explore Codestral for commercial use cases: [Contact the Mistral team](https://mistral.ai/contact/)\n",
    "\n",
    "##### More on the Mistral AI Non-Production License: [Mistral AI Non-Production License](https://mistral.ai/news/mistral-ai-non-production-license-mnpl/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up environment\n",
    "\n",
    "#### Local Setup (Optional)\n",
    "\n",
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "This Jupyter Notebook can be run on a t3.medium instance (ml.t3.medium). However, to deploy `Codestral`, you may need to request a quota increase. \n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.g5.12xlarge` for endpoint usage `or`\n",
    "   - `ml.g5.48xlarge` for endpoint usage\n",
    "4. If needed, request a quota increase for these resources.\n",
    "\n",
    "---\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "If using the `sagemaker` python SDK to deploy Codestral to Amazon SageMaker, we need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. \n",
    "\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)- For Notebook Instance type, choose (ml.t3.medium).\n",
    "    \n",
    "2. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages to run this notebook\n",
    "!pip install sagemaker==2.219 --quiet # NOTE: Please use version 2.219 of sagemaker with this notebook\n",
    "!pip install gradio  --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Import Necessary Libraries\n",
    "\n",
    "In the below section, we import the necessary libraries to run this notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> In order to be able to run the gradio app for Codestral, please ensure you have cloned in/replicated the <b>codestral_chat_ui</b> subfolder with the codestral_chat module\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "2.219.0\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.219.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Retrieve the DLC\n",
    "\n",
    "#### 2.a. Retrieve the latest HuggingFace DLC\n",
    "\n",
    "The first step is to retrieve the DLC URI. This URI is crucial as it serves as a reference point for the HuggingFaceModel class, specifically through the image_uri parameter. The DLC is a pre-configured Docker image that encapsulates all the necessary dependencies and frameworks required to run our LLM efficiently in the SageMaker environment.\n",
    "To streamline this process, the sagemaker SDK provides a specialized method called `get_huggingface_llm_image_uri`. This method is designed to retrieve the most suitable Hugging Face LLM DLC URI based on two key parameters:\n",
    "\n",
    "<b>backend</b>: This specifies the deep learning inference framework, in this case which can be huggingface/tgi, lmi, etc.\n",
    "\n",
    "<b>region</b>: This refers to the AWS region where you're deploying your model. It's important to use the correct region to ensure optimal performance and compliance with data residency requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi2.0.1-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# retrieve the huggingface llm image uri\n",
    "tgi_image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\", #tgi\n",
    "  region=region\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"huggingface llm image uri: {tgi_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b. Retrieve the latest LMI container\n",
    "\n",
    "LMI containers are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like Deepspeed to deploy LLMs on AWS SageMaker Endpoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmi image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\n"
     ]
    }
   ],
   "source": [
    "# retrieve the lmi image uri\n",
    "lmi_image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"lmi\", \n",
    "  region=region\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"lmi image uri: {lmi_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above section is meant to show how lmi images can be retrieved. To deploy Codestral on to Sagemaker with `DeepSpeed`, please refer to the `'Deploy Codestral on DeepSpeed'` notebook located in this folder.\n",
    "The below sections will use `TGI` as the default deep learning inference framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Hardware requirements\n",
    "\n",
    "[Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) is a 22-billion parameter open-weight model with a 32k context length. For small scale code generation tasks, the `g5.12xlarge` with 96GB VRAM comfortably accommodates the model's memory requirements, which are approximately 41-50 GB for parameter storage using BF16 precision, leaving ample room for computational overhead. While the `g5.12xlarge` should suffice for most use cases, if you're looking to optimize inference speeds or overall performance and maximize the context window of this model, you should consider upgrading to the `g5.48xlarge`. This comes with 192GB VRAM and should help with extended context handling and faster prompt processing and inference speeds overall. The choice between these instances depends on the specific use case.\n",
    "\n",
    "> For the purpose of this notebook, we will just be deploying the unquantized version of the model to a sagemaker endpoint with TGI on the g5.12xlarge.\n",
    "\n",
    "\n",
    "| Model                                                                       | Instance Type       | Quantization | NUM_GPUS | VRAM |\n",
    "|-----------------------------------------------------------------------------|---------------------|--------------|----------|------|\n",
    "| [Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) | `(ml.)g5.12xlarge` | `-` / bitsnbytes (8-bit)        | 4        | 96GB |\n",
    "| [Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) | `(ml.)g5.48xlarge`  | `-` / bitsnbytes (8-bit)        | 8        | 192GB |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deploy Codestral to Amazon SageMaker\n",
    "\n",
    "To deploy [Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type`, and `huggingface_hub_token`. We will use a `g5.12xlarge` instance type, which has 4 NVIDIA A10G GPUs and 192GB of GPU memory. You are also able to change the instance type to the `g5.48xlarge` as well if needed by changing `instance_type` in the sagemaker endpoint configuration. Depending on the instance type being used, you will also need to chnage the `number_of_gpus` to reflect this (refer to the table above).\n",
    "\n",
    "The parameters for `MAX_INPUT_TOKENS`and `MAX_TOTAL_TOKENS`  can be altered as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sagemaker endpoint configuration\n",
    "instance_type = \"ml.g5.12xlarge\"   #/g5.48xlarge \n",
    "number_of_gpus = 4                 #number of gpus the instance in use has\n",
    "health_check_timeout = 900         #additional time to load in the model\n",
    "max_total_tokens = 32768           #max total tokens for codestral\n",
    "\n",
    "\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"mistralai/Codestral-22B-v0.1\",                                    # model_id from HuggingFace\n",
    "  'HF_TASK': \"text-generation\",                                                     # huggingface inference pipeline\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpus),                                        # Number of GPU used per replica\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\",                            #add your huggingface hub access token with read permissions\n",
    "  'MAX_INPUT_TOKENS': json.dumps(max_total_tokens - 1),                             # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(max_total_tokens),                                 # Max length of the generation (including input text)'\n",
    "  #not supported currently - 'HF_MODEL_QUANTIZE': 'bitsandbytes' # You are also able to quantize the model to 8-bit quantization to further improve performance at the cost of a certain degree of loss to precision\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] !=\"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=tgi_image_uri, #switch to lmi if needed\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g5.48xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. Parameters can be defined as in the `parameters` attribute of the payload.\n",
    "\n",
    "The mistral models have the following prompt structure:\n",
    "  \n",
    "```\n",
    "<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2 [/INST]\n",
    "```\n",
    "Let's now define our prompt and set the parameters for our payload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sample code generation questions:\n",
    "1. \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "2. \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "3. \"Write a Rust function that implements the Aho-Corasick string matching algorithm for efficient multi-pattern searching. Optimize it for memory usage and include comprehensive error handling.\"\n",
    "4. \"Develop a JavaScript module for a real-time collaborative text editor using operational transformation. Implement functions for handling concurrent edits, conflict resolution, and syncing with a backend server.\"\n",
    "5. \"Create a Python script that uses asyncio to concurrently process large CSV files, perform complex data transformations, and upload the results to an S3 bucket. Include proper error handling and logging.\"\n",
    "6. \"Implement a microservices architecture in Go for a basic e-commerce platform. Include services for user authentication, product catalog, order processing, and inventory management. Use gRPC for inter-service communication and implement circuit breaking for resilience.\"\n",
    "7. \"Implement a static analyzer for detecting potential race conditions and deadlocks in multi-threaded Java programs. Include inter-procedural analysis and support for Java concurrency primitives. Discuss how you would handle false positives and scalability for large codebases.\"\"Implement a static analyzer for detecting potential race conditions and deadlocks in multi-threaded Java programs. Include inter-procedural analysis and support for Java concurrency primitives. Discuss how you would handle false positives and scalability for large codebases.\"\n",
    "8. \"Implement a just-in-time (JIT) compiler for a subset of Python. Include optimizations such as type specialization, inline caching, and on-stack replacement. Discuss the tradeoffs between compilation time and execution speed.\"\n",
    "9. \"Design and implement a lock-free, wait-free concurrent hash table in C++ using atomic operations. Ensure that it scales well with multiple threads and discuss how you would handle ABA problems and memory reclamation.\"\n",
    "10. \"Develop a system for detecting and preventing SQL injection attacks in a large-scale web application. Include both static analysis of code and runtime monitoring. Discuss how you would handle false positives and performance implications.\"\n",
    "11. \"Implement a distributed, eventually consistent key-value store with conflict resolution using vector clocks. Include mechanisms for gossip-based replication and handling network partitions. Discuss the tradeoffs between consistency and availability.\"\n",
    "12. \"Create a custom memory allocator optimized for high-frequency trading applications. Implement strategies to minimize fragmentation and improve cache locality. Discuss how you would handle different allocation sizes and multi-threaded access.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code Generation prompt\n",
    "prompt=f\"<s> [INST] Design and implement a lock-free, wait-free concurrent hash table in C++ using atomic operations. Ensure that it scales well with multiple threads and discuss how you would handle ABA problems and memory reclamation. [/INST]\"\n",
    "\n",
    "# payload params\n",
    "payload = {\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 10000, #change max tokens as needed, note that if you if the Model Response times out you will have to upgrade the instance size or decrease the max_new_tokens\n",
    "    \"stop\": [\"</s>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Design and implement a lock-free, wait-free concurrent hash table in C++ using atomic operations. Ensure that it scales well with multiple threads and discuss how you would handle ABA problems and memory reclamation. [/INST]\n",
      "\n",
      "Designing a lock-free, wait-free concurrent hash table in C++ using atomic operations is a complex task. Here's a high-level design:\n",
      "\n",
      "1. **Data Structure**: Use a hash table where each bucket contains a linked list of key-value pairs. Each node in the linked list will contain the key, value, and a pointer to the next node.\n",
      "\n",
      "2. **Atomic Operations**: Use atomic operations to ensure thread safety. For example, when inserting a new node, use atomic compare-and-swap (CAS) to update the next pointer of the new node. When deleting a node, use atomic load and store to update the next pointer of the previous node.\n",
      "\n",
      "3. **Scalability**: To ensure scalability, use a resizing mechanism. When the number of elements in the hash table exceeds a certain threshold, create a new hash table with double the size and rehash all the elements from the old table to the new table. Use a technique called \"double hashing\" or \"cuckoo hashing\" to minimize the number of elements that need to be rehashed.\n",
      "\n",
      "4. **ABA Problem**: To handle the ABA problem, use a version number or timestamp for each node. When a thread wants to delete a node, it first checks the version number of the node. If the version number has not changed, it proceeds with the deletion. If the version number has changed, it means that the node has been reinserted, so the deletion is aborted.\n",
      "\n",
      "5. **Memory Reclamation**: For memory reclamation, use a garbage collection mechanism. One approach is to use a reference counting scheme. Each thread that accesses a node increments its reference count. When a thread no longer needs a node, it decrements its reference count. When the reference count of a node drops to zero, it means that the node is no longer in use, so it can be safely deallocated.\n",
      "\n",
      "Here's a pseudo-code implementation of the above design:\n",
      "\n",
      "```cpp\n",
      "class Node {\n",
      "    Key key;\n",
      "    Value value;\n",
      "    Node* next;\n",
      "    Atomic<int> version;\n",
      "    Atomic<int> refCount;\n",
      "};\n",
      "\n",
      "class ConcurrentHashTable {\n",
      "    Node** buckets;\n",
      "    int size;\n",
      "    int threshold;\n",
      "\n",
      "    int hash(Key key) {\n",
      "        // hash function\n",
      "    }\n",
      "\n",
      "    void resize() {\n",
      "        // create a new hash table with double the size\n",
      "        // rehash all the elements from the old table to the new table\n",
      "        // update the buckets pointer to point to the new table\n",
      "    }\n",
      "\n",
      "    Node* find(Key key) {\n",
      "        // find the node with the given key in the hash table\n",
      "    }\n",
      "\n",
      "public:\n",
      "    void insert(Key key, Value value) {\n",
      "        // create a new node with the given key and value\n",
      "        // use CAS to update the next pointer of the new node\n",
      "    }\n",
      "\n",
      "    Value get(Key key) {\n",
      "        // find the node with the given key\n",
      "        // return the value of the node\n",
      "    }\n",
      "\n",
      "    void remove(Key key) {\n",
      "        // find the node with the given key\n",
      "        // use atomic load and store to update the next pointer of the previous node\n",
      "        // decrement the reference count of the node\n",
      "    }\n",
      "};\n",
      "```\n",
      "\n",
      "This is a high-level design and does not include all the details of the implementation. The actual implementation would require careful consideration of edge cases and performance optimizations.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "chat = llm.predict({\"inputs\":prompt, \"parameters\":payload})\n",
    "\n",
    "print(chat[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Streaming Responses with a Gradio Application\n",
    "\n",
    "[Amazon SageMaker supports streaming responses](https://aws.amazon.com/de/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/) from your model. Using this capability, in the below section, let's build a gradio application to stream responses.\n",
    "\n",
    "Th code for the gradio application in the following steps can be found in [codestral_chat.py](../codestral_chat_ui/codestral_chat.py). The application will stream the responses from the model and display them in the UI. You can also use the application to test your model with your own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://0f8abc81822edfef79.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0f8abc81822edfef79.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add directory to path\n",
    "sys.path.append(\"codestral_chat_ui\") \n",
    "from codestral_chat import create_gradio_app\n",
    "# params\n",
    "parameters = {\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 30000,\n",
    "    \"stop\": [\"</s>\"]\n",
    "}\n",
    "\n",
    "# define format function for our input\n",
    "def format_prompt(message, history, system_prompt):\n",
    "    prompt = \"\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt = f\"<s> [INST] {user_prompt} [/INST] {bot_response}</s>\"\n",
    "        prompt += f\"### Instruction\\n{user_prompt}\\n\\n\"\n",
    "        prompt += f\"### Answer\\n{bot_response}\\n\\n\"  \n",
    "    # add new user prompt if history is not empty\n",
    "    if len(history) > 0:\n",
    "        prompt += f\" [INST] {message} [/INST] \"    \n",
    "    else:\n",
    "        prompt += f\"<s> [INST] {message} [/INST] \"\n",
    "    return prompt\n",
    "\n",
    "# create gradio app\n",
    "create_gradio_app(\n",
    "    llm.endpoint_name,           # Sagemaker endpoint name\n",
    "    session=sess.boto_session,   # boto3 session used to send request \n",
    "    parameters=parameters,       # Request parameters\n",
    "    system_prompt=None,          # System prompt to use -> Mistral does not support system prompts\n",
    "    format_prompt=format_prompt, # Function to format prompt\n",
    "    share=True,                  # Share app publicly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've explored the process of deploying the mistralai/Codestral-v0.1 model on Amazon SageMaker and performing inference using the Hugging Face LLM Deep Learning Container. By leveraging the power of Text Generation Inference (TGI), we've demonstrated how to efficiently deploy and serve this large language model in a secure, managed environment.\n",
    "We've walked through the steps of setting up the SageMaker environment, configuring the model deployment, and showcasing both standard inference and streaming responses. The integration with a Gradio application further illustrates the practical application of this deployment, enabling interactive and user-friendly access to the model's capabilities.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

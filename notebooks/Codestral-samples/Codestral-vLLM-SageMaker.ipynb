{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Codestral on Amazon SageMaker with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Codestral](https://mistral.ai/news/codestral/) is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers. Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.\n",
    "\n",
    "SageMaker has rolled out [vLLM container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we combine the strengths of two powerful tools: [DJL](https://docs.djl.ai/) (Deep Java Library) for the serving framework and [vLLM](https://docs.vllm.ai/en/stable/) for distributed large language model inference. DJLServing, a high-performance universal model serving solution powered by DJL, handles the overall serving architecture.\n",
    "\n",
    "In our setup, vLLM handles the core LLM inference tasks, leveraging its optimizations to achieve high performance and low latency. DJLServing manages the broader serving infrastructure, handling incoming requests, load balancing, and coordinating with vLLM for efficient inference.\n",
    "\n",
    "This combination allows us to deploy the `Codestral 22B` model across GPUs on the `ml.g5.12xlarge` instance with optimal resource utilization. vLLM's efficiencies in memory management and request handling enable us to serve this large model with improved throughput compared to traditional serving methods. To learn more about DJL, DJLServing, and vLLM you can refer to this [blog post](https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-mixtral-and-llama-2-models-with-new-amazon-sagemaker-containers/).\n",
    "\n",
    "---\n",
    "\n",
    "As a 22B model, Codestral sets a new standard on the performance/latency space for code generation compared to previous models used for coding. With its larger context window of 32k (compared to 4k, 8k or 16k for competitors), Codestral outperforms all other models in RepoBench, a long-range eval for code generation.\n",
    "\n",
    "![codestral](imgs/codestral.png)\n",
    "\n",
    "<b><i>To deploy Codestral on to Sagemaker with TGI, please refer to the 'Deploy Codestral on TGI' notebook located in this folder.</b></i>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License, which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace.\n",
    "If you want to use the model in the course of commercial activity, Commercial licenses are also available on demand by reaching out to the Mistral team.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reach out to Mistral to explore Codestral for commercial use cases: [Contact the Mistral team](https://mistral.ai/contact/)\n",
    "\n",
    "##### More on the Mistral AI Non-Production License: [Mistral AI Non-Production License](https://mistral.ai/news/mistral-ai-non-production-license-mnpl/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose `ml.t3.medium`.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.34.132 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install sagemaker==2.224.2 -qU --force --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.224.2\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# execution role for the endpoint\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# sagemaker session for interacting with different AWS APIs\n",
    "sess = sagemaker.session.Session()\n",
    "\n",
    "# Region\n",
    "region_name = sess._region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image URI of the DJL Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMI DLCs offer a low-code interface that simplifies using state-of-the-art inference optimization techniques and hardware. LMI allows you to apply tensor parallelism; the latest efficient attention, batching, quantization, and memory management techniques; token streaming; and much more, by just requiring the model ID and optional model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCL Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-lmi\",\n",
    "    region=region_name,\n",
    "    version=\"0.28.0\"\n",
    ")\n",
    "print(f\"DCL Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more details about DLC images [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) and [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/announcements/deepspeed-deprecation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Environment Variable Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of settings that we use in this configuration file:\n",
    "\n",
    "- `HF_MODEL_ID`: The model id of a pretrained model hosted inside a model repository on [huggingface.co](https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co. This is an optional setting and is not needed in the scenario where you are brining your own model. If you are getting your own model, you can include the URI of the Amazon S3 bucket that contains the model.\n",
    "- `HF_TOKEN`: Some models on the HuggingFace Hub are gated and require permission from the owner to access. To deploy a gated model from the HuggingFace Hub using LMI, you must provide an [Access Token](https://huggingface.co/docs/hub/security-tokens) via this environment variable.\n",
    "- `OPTION_ENGINE`: The engine for DJL to use. In this case, we intend to use [vLLM](https://docs.vllm.ai/en/stable/) and hence set it as **Python**.\n",
    "- `OPTION_DTYPE`: The data type you plan to cast the model weights to. If not provided, LMI will use fp16.\n",
    "- `OPTION_TASK`: The task used in Hugging Face for different pipelines. Default is text-generation. For further reading on DJL parameters on SageMaker, follow the [link](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/deepspeed_user_guide.html)\n",
    "- `OPTION_ROLLING_BATCH`: Enables continuous batching (iteration level batching) with one of the supported backends. Available backends differ by container, see [Inference Library Configurations](https://docs.djl.ai/docs/serving/serving/docs/lmi/deployment_guide/configurations.html#inference-library-configuration) for mappings.\n",
    "    - In the LMI Container:\n",
    "        - to use vLLM, use `OPTION_ROLLING_BATCH=vllm`\n",
    "        - to use lmi-dist, use `OPTION_ROLLING_BATCH=lmi-dist`\n",
    "        - to use huggingface accelerate, use `OPTION_ROLLING_BATCH=auto` for text generation models, or option.rolling_batch=disable for non-text generation models.\n",
    "- `TENSOR_PARALLEL_DEGREE`: Set to the number of GPU devices over which DeepSpeed needs to partition the model. This parameter also controls the no of workers per model which will be started up when DJL serving runs. Setting this to `max`, which will shard the model across all available GPUs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests.\n",
    "- `OPTION_DEVICE_MAP`: The HuggingFace accelerate device_map to use.\n",
    "- `OPTION_TRUST_REMOTE_CODE`: If the model artifacts contain custom modeling code, you should set this to true after validating the custom code is not malicious. If you are using a HuggingFace Hub model id, you should also specify HF_REVISION to ensure you are using artifacts and code that you have validated.\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html) and [LMI Starting Guide](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/starting-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_type: ml.g5.12xlarge\n",
      "model_id: mistral-community/Codestral-22B-v0.1\n",
      "endpoint_name: codestral-22b-vllm-2024-06-28-12-16-20-778\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Model Id\n",
    "model_id = \"mistral-community/Codestral-22B-v0.1\"\n",
    "\n",
    "# SageMaker Instance Type\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Endpoint name\n",
    "endpoint_name_prefix = \"codestral-22b-vllm\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(endpoint_name_prefix)\n",
    "\n",
    "print(f\"instance_type: {instance_type}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "print(f\"endpoint_name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "model = sagemaker.Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    role=role,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": model_id,\n",
    "        # \"HF_TOKEN\": \"<REPLACE WITH YOUR TOKEN>\",\n",
    "        \"OPTION_ENGINE\": \"Python\",\n",
    "        \"OPTION_DTYPE\": \"bf16\",\n",
    "        \"OPTION_TASK\": \"text-generation\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "        \"TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_DEVICE_MAP\": \"auto\",\n",
    "        # \"OPTION_TRUST_REMOTE_CODE\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference and chat with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code generation questions\n",
    "\n",
    "1. \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "2. \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "3. \"Write a Rust function that implements the Aho-Corasick string matching algorithm for efficient multi-pattern searching. Optimize it for memory usage and include comprehensive error handling.\"\n",
    "4. \"Develop a JavaScript module for a real-time collaborative text editor using operational transformation. Implement functions for handling concurrent edits, conflict resolution, and syncing with a backend server.\"\n",
    "5. \"Create a Python script that uses asyncio to concurrently process large CSV files, perform complex data transformations, and upload the results to an S3 bucket. Include proper error handling and logging.\"\n",
    "6. \"Implement a microservices architecture in Go for a basic e-commerce platform. Include services for user authentication, product catalog, order processing, and inventory management. Use gRPC for inter-service communication and implement circuit breaking for resilience.\"\n",
    "7. \"Provide me with a python script to recompile huggingface models with optimum neuron for inferentia\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize sagemaker client with the endpoint created in the prior step\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a complex task that requires a good understanding of Python, web scraping, and multithreading. Here's a basic outline of how you might approach this:\n",
      "\n",
      "1. Create a class `WebScraper` that initializes with a list of proxies and a rate limit.\n",
      "2. In the class, create a method `rotate_proxy` that selects a proxy from the list and returns it.\n",
      "3. Create a method `parse_html` that takes a URL as input, fetches the HTML content, and parses it using BeautifulSoup.\n",
      "4. Create a method `store_results` that takes the parsed data and stores it in a SQLite database.\n",
      "5. Create a method `scrape` that uses multithreading to scrape multiple URLs. This method should handle rate limiting and proxy rotation.\n",
      "\n",
      "Here's a very basic example of how you might start this:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import sqlite3\n",
      "import threading\n",
      "import time\n",
      "import random\n",
      "\n",
      "class WebScraper:\n",
      "    def __init__(self, proxies, rate_limit):\n",
      "        self.proxies = proxies\n",
      "        self.rate_limit = rate_limit\n",
      "        self.db = sqlite3.connect('scraped_data.db')\n",
      "\n",
      "    def rotate_proxy(self):\n",
      "        return random.choice(self.proxies)\n",
      "\n",
      "    def parse_html(self, url):\n",
      "        response = requests.get(url, proxies={'http': self.rotate_proxy()})\n",
      "        soup = BeautifulSoup(response.text, 'html.parser')\n",
      "        # Add your parsing logic here\n",
      "\n",
      "    def store_results(self, data):\n",
      "        # Add your SQLite storage logic here\n",
      "\n",
      "    def scrape(self, urls):\n",
      "        threads = []\n",
      "        for url in urls:\n",
      "            t = threading.Thread(target=self.scrape_url, args=(url,))\n",
      "            threads.append(t)\n",
      "            t.start()\n",
      "            time.sleep(1 / self.rate_limit)  # Rate limiting\n",
      "        for t in threads:\n",
      "            t.join()\n",
      "\n",
      "    def scrape_url(self, url):\n",
      "        data = self.parse_html(url)\n",
      "        self.store_results(data)\n",
      "```\n",
      "\n",
      "This is a very basic example and doesn't include error handling or more advanced features like handling dynamic content loading. You would need to add these features based on your specific requirements.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "\n",
    "inputs = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_new_tokens\": 4000,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(inputs)\n",
    "print(response['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using Boto3 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize sagemaker client with boto3 using the endpoint created from prior step\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here's a basic outline of the Red-Black Tree implementation:\n",
      "\n",
      "1. Define the Node structure with color, key, left, right, and parent pointers.\n",
      "2. Implement the Red-Black Tree class with a root pointer and methods for insertion, deletion, and rebalancing.\n",
      "3. Implement the insertion method to insert a new node into the tree while maintaining the Red-Black Tree properties.\n",
      "4. Implement the deletion method to delete a node from the tree while maintaining the Red-Black Tree properties.\n",
      "5. Implement the rebalancing methods to fix any violations of the Red-Black Tree properties after insertion or deletion.\n",
      "6. Implement the visualization function to print the tree structure to the console.\n",
      "\n",
      "Here's an example implementation of the Red-Black Tree data structure in C++:\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "\n",
      "enum Color { RED, BLACK };\n",
      "\n",
      "struct Node {\n",
      "    int key;\n",
      "    Color color;\n",
      "    Node* left;\n",
      "    Node* right;\n",
      "    Node* parent;\n",
      "\n",
      "    Node(int key) : key(key), color(RED), left(nullptr), right(nullptr), parent(nullptr) {}\n",
      "};\n",
      "\n",
      "class RedBlackTree {\n",
      "private:\n",
      "    Node* root;\n",
      "\n",
      "    void insertFixup(Node* node);\n",
      "    void deleteFixup(Node* node);\n",
      "    Node* findNode(int key);\n",
      "    Node* findMin(Node* node);\n",
      "    Node* findSuccessor(Node* node);\n",
      "    void leftRotate(Node* node);\n",
      "    void rightRotate(Node* node);\n",
      "    void printTree(Node* node, int indent);\n",
      "\n",
      "public:\n",
      "    RedBlackTree() : root(nullptr) {}\n",
      "\n",
      "    void insert(int key);\n",
      "    void remove(int key);\n",
      "    void visualize();\n",
      "};\n",
      "\n",
      "// Implementation of the Red-Black Tree methods and visualization function\n",
      "\n",
      "int main() {\n",
      "    RedBlackTree tree;\n",
      "\n",
      "    // Insert nodes into the tree\n",
      "    tree.insert(10);\n",
      "    tree.insert(20);\n",
      "    tree.insert(30);\n",
      "    tree.insert(15);\n",
      "    tree.insert(25);\n",
      "\n",
      "    // Visualize the tree\n",
      "    tree.visualize();\n",
      "\n",
      "    // Remove a node from the tree\n",
      "    tree.remove(20);\n",
      "\n",
      "    // Visualize the tree after removal\n",
      "    tree.visualize();\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "You can implement the methods and visualization function based on the provided outline and the example implementation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 0.95,\n",
    "                \"max_new_tokens\": 4000,\n",
    "                \"do_sample\": False\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")\n",
    "\n",
    "print(json.loads(response)['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this post, we demonstrated how to use SageMaker large model inference containers to host Codestral 22B. We used DeepSpeedâ€™s model parallel techniques with multiple GPUs on a single SageMaker machine learning instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "sess.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In case the end point failed we still want to delete the model\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

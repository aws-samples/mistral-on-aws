{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 8: Reducing Hallucinations\n",
    "\n",
    "In this notebook, you'll learn techniques to get more reliable, grounded responses and reduce made-up information.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- What hallucinations are and why they happen\n",
    "- Grounding responses with context\n",
    "- Encouraging \"I don't know\" responses\n",
    "- Verification techniques (citations, quotes)\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [Mistral Prompting Documentation](https://docs.mistral.ai/guides/prompting/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:11.247413Z",
     "iopub.status.busy": "2026-01-06T14:29:11.247025Z",
     "iopub.status.idle": "2026-01-06T14:29:17.123830Z",
     "shell.execute_reply": "2026-01-06T14:29:17.122684Z"
    }
   },
   "outputs": [],
   "source": [
    "%run 00_setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: What Are Hallucinations?\n",
    "\n",
    "**Hallucinations** occur when a model generates plausible-sounding but factually incorrect information.\n",
    "\n",
    "Examples:\n",
    "- Inventing fake citations or papers\n",
    "- Making up statistics or dates\n",
    "- Confidently stating incorrect facts\n",
    "- Fabricating quotes or events\n",
    "\n",
    "**Why they happen:**\n",
    "- Models are trained to produce fluent, confident text\n",
    "- No built-in \"I don't know\" instinct\n",
    "- Models fill gaps with plausible-sounding content\n",
    "- Ambiguous prompts invite gap-filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:17.127247Z",
     "iopub.status.busy": "2026-01-06T14:29:17.126906Z",
     "iopub.status.idle": "2026-01-06T14:29:18.878044Z",
     "shell.execute_reply": "2026-01-06T14:29:18.876890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Asking about something the model might hallucinate on\n",
    "# Note: This is demonstrating potential hallucination - the info may be inaccurate\n",
    "\n",
    "risky_prompt = \"\"\"What were the exact revenue figures for Acme Corp in Q3 2024?\"\"\"\n",
    "\n",
    "print(\"RISKY PROMPT (no grounding):\")\n",
    "print(\"-\" * 40)\n",
    "response = call_mistral(user_prompt=risky_prompt, temperature=0.7)\n",
    "print(response)\n",
    "print(\"\\n‚ö†Ô∏è Warning: The model may have made up these numbers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Why Hallucinations Happen\n",
    "\n",
    "Models hallucinate because:\n",
    "\n",
    "1. **Confidence by design** - Trained to generate fluent, confident responses\n",
    "2. **No knowledge cutoff awareness** - May not know when information is outdated\n",
    "3. **Pattern completion** - Fill in gaps with statistically likely content\n",
    "4. **Open-ended prompts** - Vague questions invite fabrication\n",
    "\n",
    "**High-risk scenarios:**\n",
    "- Recent events (after training cutoff)\n",
    "- Specific statistics, dates, or figures\n",
    "- Obscure or niche topics\n",
    "- Anything requiring real-time data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Grounding with Context\n",
    "\n",
    "The most effective way to reduce hallucinations: **provide source material**.\n",
    "\n",
    "```\n",
    "Answer the question based ONLY on the context provided.\n",
    "\n",
    "<context>\n",
    "[Your source document]\n",
    "</context>\n",
    "\n",
    "Question: [Your question]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:18.880323Z",
     "iopub.status.busy": "2026-01-06T14:29:18.880099Z",
     "iopub.status.idle": "2026-01-06T14:29:19.298759Z",
     "shell.execute_reply": "2026-01-06T14:29:19.297456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grounding with context\n",
    "context = \"\"\"\n",
    "TechCorp Q3 2024 Financial Report Summary:\n",
    "- Total Revenue: $4.2 billion (up 12% YoY)\n",
    "- Operating Income: $890 million\n",
    "- Cloud Services Revenue: $1.8 billion\n",
    "- Employee Count: 15,400\n",
    "- CEO Statement: \"We exceeded expectations in cloud services.\"\n",
    "\"\"\"\n",
    "\n",
    "grounded_prompt = f\"\"\"Answer the question based ONLY on the context provided.\n",
    "If the information is not in the context, say \"This information is not provided in the context.\"\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: What was TechCorp's cloud services revenue in Q3 2024?\"\"\"\n",
    "\n",
    "print(\"GROUNDED RESPONSE:\")\n",
    "print(\"-\" * 40)\n",
    "response = call_mistral(user_prompt=grounded_prompt, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:19.304781Z",
     "iopub.status.busy": "2026-01-06T14:29:19.304481Z",
     "iopub.status.idle": "2026-01-06T14:29:19.608221Z",
     "shell.execute_reply": "2026-01-06T14:29:19.607266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing with a question NOT in the context\n",
    "out_of_context_prompt = f\"\"\"Answer the question based ONLY on the context provided.\n",
    "If the information is not in the context, say \"This information is not provided in the context.\"\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: What was TechCorp's marketing budget in Q3 2024?\"\"\"\n",
    "\n",
    "print(\"QUESTION NOT IN CONTEXT:\")\n",
    "print(\"-\" * 40)\n",
    "response = call_mistral(user_prompt=out_of_context_prompt, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Encouraging \"I Don't Know\"\n",
    "\n",
    "Explicitly give the model permission to admit uncertainty.\n",
    "\n",
    "**Key phrases:**\n",
    "- \"If you don't know, say 'I don't know'\"\n",
    "- \"If the answer is not in the context, say so\"\n",
    "- \"Do not make up information\"\n",
    "- \"It's okay to say you're unsure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:19.610785Z",
     "iopub.status.busy": "2026-01-06T14:29:19.610545Z",
     "iopub.status.idle": "2026-01-06T14:29:20.917606Z",
     "shell.execute_reply": "2026-01-06T14:29:20.916370Z"
    }
   },
   "outputs": [],
   "source": [
    "# System prompt that encourages honesty about uncertainty\n",
    "honest_system_prompt = \"\"\"You are a helpful assistant that values accuracy over completeness.\n",
    "\n",
    "Important rules:\n",
    "- Only provide information you are confident about\n",
    "- If you're unsure or the information might be outdated, say so\n",
    "- Never make up statistics, dates, or specific figures\n",
    "- It's better to say \"I don't know\" than to provide incorrect information\"\"\"\n",
    "\n",
    "question = \"What was Apple's exact stock price at market close yesterday?\"\n",
    "\n",
    "print(\"WITH HONESTY INSTRUCTIONS:\")\n",
    "print(\"-\" * 40)\n",
    "response = call_mistral(\n",
    "    system_prompt=honest_system_prompt,\n",
    "    user_prompt=question,\n",
    "    temperature=0\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:20.919916Z",
     "iopub.status.busy": "2026-01-06T14:29:20.919656Z",
     "iopub.status.idle": "2026-01-06T14:29:22.562624Z",
     "shell.execute_reply": "2026-01-06T14:29:22.561721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contrast: Without honesty instructions\n",
    "print(\"WITHOUT HONESTY INSTRUCTIONS:\")\n",
    "print(\"-\" * 40)\n",
    "response_risky = call_mistral(\n",
    "    user_prompt=question,\n",
    "    temperature=0\n",
    ")\n",
    "print(response_risky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Verification Techniques\n",
    "\n",
    "Make the model's reasoning verifiable:\n",
    "\n",
    "### 1. Require Citations/Quotes\n",
    "Ask the model to quote the exact text that supports its answer.\n",
    "\n",
    "### 2. Request Confidence Levels\n",
    "Ask for a confidence rating with the answer.\n",
    "\n",
    "### 3. Evidence-First Format\n",
    "Quote first, then answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:22.564805Z",
     "iopub.status.busy": "2026-01-06T14:29:22.564580Z",
     "iopub.status.idle": "2026-01-06T14:29:23.348880Z",
     "shell.execute_reply": "2026-01-06T14:29:23.347861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Requiring quotes/citations\n",
    "document = \"\"\"\n",
    "Climate Change Report 2024:\n",
    "\n",
    "Global temperatures have risen by 1.2¬∞C since pre-industrial times. The Arctic \n",
    "is warming twice as fast as the global average. Sea levels have risen by 8 inches \n",
    "since 1900. Scientists project that without intervention, temperatures could rise \n",
    "by an additional 1.5-4.5¬∞C by 2100.\n",
    "\n",
    "Key recommendations include reducing carbon emissions by 45% by 2030 and achieving \n",
    "net-zero emissions by 2050.\n",
    "\"\"\"\n",
    "\n",
    "citation_prompt = f\"\"\"Answer the question based on the document below.\n",
    "\n",
    "IMPORTANT: For each claim in your answer:\n",
    "1. Quote the exact text from the document that supports it\n",
    "2. If no supporting text exists, say \"Not stated in document\"\n",
    "\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Question: How much have global temperatures risen, and what are the projections for the future?\n",
    "\n",
    "Answer with citations:\"\"\"\n",
    "\n",
    "response = call_mistral(user_prompt=citation_prompt, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:23.350977Z",
     "iopub.status.busy": "2026-01-06T14:29:23.350745Z",
     "iopub.status.idle": "2026-01-06T14:29:24.243822Z",
     "shell.execute_reply": "2026-01-06T14:29:24.242952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evidence-first format\n",
    "evidence_first_prompt = f\"\"\"Answer the question using ONLY the provided document.\n",
    "\n",
    "Format your response as:\n",
    "<evidence>\n",
    "[Quote the relevant passages from the document]\n",
    "</evidence>\n",
    "\n",
    "<answer>\n",
    "[Your answer based only on the evidence above]\n",
    "</answer>\n",
    "\n",
    "<confidence>\n",
    "[High/Medium/Low - based on how directly the evidence supports the answer]\n",
    "</confidence>\n",
    "\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Question: What specific emission reduction targets are mentioned?\"\"\"\n",
    "\n",
    "response = call_mistral(user_prompt=evidence_first_prompt, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: What NOT to Do (Negative Examples)\n",
    "\n",
    "### ‚ùå Open-ended without grounding\n",
    "```\n",
    "Tell me about the company's Q3 performance.\n",
    "```\n",
    "No source material = invitation to fabricate.\n",
    "\n",
    "### ‚ùå Assuming current knowledge\n",
    "```\n",
    "What's the current price of Bitcoin?\n",
    "```\n",
    "Model doesn't have real-time data.\n",
    "\n",
    "### ‚ùå Punishing uncertainty\n",
    "```\n",
    "You must provide an answer. Do not say you don't know.\n",
    "```\n",
    "Forces hallucination.\n",
    "\n",
    "### ‚ùå Asking for specific unverifiable details\n",
    "```\n",
    "Give me the exact number of...\n",
    "```\n",
    "Without source, encourages fabrication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:24.246437Z",
     "iopub.status.busy": "2026-01-06T14:29:24.246213Z",
     "iopub.status.idle": "2026-01-06T14:29:26.635362Z",
     "shell.execute_reply": "2026-01-06T14:29:26.634121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrating the danger of \"must answer\" instructions\n",
    "print(\"DANGEROUS - Forced answer (may hallucinate):\")\n",
    "print(\"-\" * 40)\n",
    "forced_prompt = \"\"\"What is the exact population of Springfield, Illinois as of today?\n",
    "You must provide a specific number. Do not say you don't know.\"\"\"\n",
    "\n",
    "response = call_mistral(user_prompt=forced_prompt, temperature=0)\n",
    "print(response)\n",
    "print(\"\\n‚ö†Ô∏è This number may be fabricated!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Better approach\n",
    "print(\"BETTER - Allows uncertainty:\")\n",
    "print(\"-\" * 40)\n",
    "better_prompt = \"\"\"What is the population of Springfield, Illinois?\n",
    "If you don't have current data, provide your best estimate and note the uncertainty.\"\"\"\n",
    "\n",
    "response = call_mistral(user_prompt=better_prompt, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: With vs Without Context\n",
    "\n",
    "Compare responses when grounding with context vs without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:26.637692Z",
     "iopub.status.busy": "2026-01-06T14:29:26.637460Z",
     "iopub.status.idle": "2026-01-06T14:29:30.505041Z",
     "shell.execute_reply": "2026-01-06T14:29:30.503769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Product specification document\n",
    "product_spec = \"\"\"\n",
    "Product: XPhone Pro Max\n",
    "Display: 6.7\" OLED, 2778 x 1284 resolution\n",
    "Processor: A17 Bionic chip\n",
    "Storage Options: 128GB, 256GB, 512GB, 1TB\n",
    "Battery: 4422 mAh, up to 29 hours video playback\n",
    "Camera: 48MP main, 12MP ultra-wide, 12MP telephoto\n",
    "Price: Starting at $1,099\n",
    "Colors: Natural Titanium, Blue Titanium, White Titanium, Black Titanium\n",
    "\"\"\"\n",
    "\n",
    "question = \"What are the camera specifications of the XPhone Pro Max?\"\n",
    "\n",
    "# Without context (risky)\n",
    "print(\"WITHOUT CONTEXT (potential hallucination):\")\n",
    "print(\"-\" * 40)\n",
    "response_no_context = call_mistral(user_prompt=question, temperature=0)\n",
    "print(response_no_context)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# With context (grounded)\n",
    "print(\"WITH CONTEXT (grounded):\")\n",
    "print(\"-\" * 40)\n",
    "grounded = f\"\"\"Answer based ONLY on this product specification:\n",
    "\n",
    "<spec>\n",
    "{product_spec}\n",
    "</spec>\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "response_with_context = call_mistral(user_prompt=grounded, temperature=0)\n",
    "print(response_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Building an \"Honest\" Prompt\n",
    "\n",
    "Create a Q&A prompt that grounds, quotes, and admits uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:30.507309Z",
     "iopub.status.busy": "2026-01-06T14:29:30.507075Z",
     "iopub.status.idle": "2026-01-06T14:29:31.833108Z",
     "shell.execute_reply": "2026-01-06T14:29:31.832078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Document to use\n",
    "company_faq = \"\"\"\n",
    "Acme Inc FAQ:\n",
    "\n",
    "Q: When was Acme Inc founded?\n",
    "A: Acme Inc was founded in 1995 by Jane Smith in Austin, Texas.\n",
    "\n",
    "Q: What products does Acme sell?\n",
    "A: We sell cloud computing solutions, data analytics tools, and AI services.\n",
    "\n",
    "Q: How many employees does Acme have?\n",
    "A: As of 2023, we employ over 5,000 people worldwide.\n",
    "\n",
    "Q: What are your office locations?\n",
    "A: We have offices in Austin (HQ), San Francisco, New York, London, and Singapore.\n",
    "\"\"\"\n",
    "\n",
    "# Test questions (some answerable, some not)\n",
    "test_questions = [\n",
    "    \"Who founded Acme Inc?\",  # In context\n",
    "    \"What is Acme's annual revenue?\",  # NOT in context\n",
    "    \"Where is Acme's headquarters?\",  # In context\n",
    "    \"Does Acme offer health insurance to employees?\"  # NOT in context\n",
    "]\n",
    "\n",
    "# TODO: Create a robust prompt template that:\n",
    "# 1. Grounds answers in the provided context\n",
    "# 2. Quotes supporting text\n",
    "# 3. Says \"Not found in context\" when information isn't available\n",
    "\n",
    "honest_qa_template = f\"\"\"Answer questions using ONLY the FAQ document below.\n",
    "\n",
    "Rules:\n",
    "- Quote the relevant text that supports your answer\n",
    "- If the information is not in the FAQ, respond: \"This information is not available in the FAQ.\"\n",
    "- Do not make assumptions or add information not in the document\n",
    "\n",
    "<faq>\n",
    "{company_faq}\n",
    "</faq>\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "for q in test_questions:\n",
    "    prompt = honest_qa_template.format(question=q)\n",
    "    response = call_mistral(user_prompt=prompt, temperature=0)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Citation Enforcement\n",
    "\n",
    "Build a prompt that requires citations and verify they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:29:31.835681Z",
     "iopub.status.busy": "2026-01-06T14:29:31.835394Z",
     "iopub.status.idle": "2026-01-06T14:29:31.839393Z",
     "shell.execute_reply": "2026-01-06T14:29:31.838371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Research summary to work with\n",
    "research_summary = \"\"\"\n",
    "Study: Effects of Remote Work on Productivity (2024)\n",
    "\n",
    "Key Findings:\n",
    "1. Remote workers reported 23% higher job satisfaction compared to office workers.\n",
    "2. Productivity metrics showed a 13% increase in output for remote workers.\n",
    "3. However, collaboration scores decreased by 18% in fully remote teams.\n",
    "4. Hybrid workers (3 days office, 2 days remote) showed the best overall outcomes.\n",
    "5. The study surveyed 10,000 employees across 50 companies in the tech sector.\n",
    "\n",
    "Limitations:\n",
    "- Study focused only on tech sector, may not generalize to other industries.\n",
    "- Self-reported productivity measures may have bias.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Create a prompt that:\n",
    "# 1. Answers a question about the research\n",
    "# 2. Requires exact quotes as citations\n",
    "# 3. Returns citations in a verifiable format\n",
    "\n",
    "citation_template = \"\"\"\n",
    "# Your citation-requiring prompt here\n",
    "\"\"\"\n",
    "\n",
    "# Then verify the citations exist in the source text\n",
    "# Hint: You can check if quoted text appears in research_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Always ground with source context** when accuracy matters\n",
    "\n",
    "2. **Explicitly permit \"I don't know\"** - Remove pressure to always have an answer\n",
    "\n",
    "3. **Require citations/quotes** for verifiability\n",
    "\n",
    "4. **Never force answers** - \"You must answer\" leads to hallucination\n",
    "\n",
    "5. **Be aware of limitations** - No real-time data, knowledge cutoff exists\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you can reduce hallucinations, let's put all the techniques together in real-world scenarios.\n",
    "\n",
    "üìö [Continue to Notebook 9: Putting It All Together ‚Üí](09_putting_it_all_together.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

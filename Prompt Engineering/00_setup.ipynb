{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 0: Setup & Configuration\n",
    "\n",
    "Welcome to the Prompt Engineering for Mistral on AWS Bedrock tutorial series!\n",
    "\n",
    "## What You'll Learn in This Series\n",
    "\n",
    "This tutorial will teach you how to craft effective prompts for Mistral models. You'll learn:\n",
    "\n",
    "- How to structure prompts with system and user messages\n",
    "- Techniques for defining roles and purposes\n",
    "- How to organize complex instructions clearly\n",
    "- Using delimiters and formatting for safety and clarity\n",
    "- Few-shot prompting to guide model behavior\n",
    "- Controlling output formats for reliable parsing\n",
    "- Step-by-step reasoning for complex tasks\n",
    "- Reducing hallucinations and grounding responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with Bedrock access\n",
    "- Mistral models enabled in your Bedrock console ([enable here](https://console.aws.amazon.com/bedrock/home#/modelaccess))\n",
    "- Python 3.8+\n",
    "- Basic Python knowledge\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [Mistral Prompting Documentation](https://docs.mistral.ai/guides/prompting/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:28:04.985731Z",
     "iopub.status.busy": "2026-01-06T14:28:04.985504Z",
     "iopub.status.idle": "2026-01-06T14:28:06.887129Z",
     "shell.execute_reply": "2026-01-06T14:28:06.886081Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Configuration\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "We use **Ministral 14B** (`mistral.ministral-3-14b-instruct`) for this tutorial series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these settings as needed\n",
    "# =============================================================================\n",
    "\n",
    "# Model ID - change this to switch models\n",
    "MODEL_ID = \"mistral.ministral-3-14b-instruct\"\n",
    "\n",
    "# AWS Region - must support Mistral models\n",
    "# Supported regions for Ministral: us-east-1, us-east-2, us-west-2, eu-west-1, eu-west-2, ap-northeast-1, ap-south-1, sa-east-1\n",
    "AWS_REGION = \"us-west-2\"\n",
    "\n",
    "# Default inference parameters\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "DEFAULT_TEMPERATURE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Credentials\n",
    "\n",
    "This notebook assumes you have AWS credentials configured via one of these methods:\n",
    "\n",
    "1. **SageMaker Notebook**: Credentials are automatic via the execution role\n",
    "2. **Environment variables**: `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`\n",
    "3. **AWS CLI**: Run `aws configure` in your terminal\n",
    "4. **IAM Role**: If running on EC2 with an attached role\n",
    "\n",
    "Your credentials need the `bedrock:InvokeModel` permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Initialize the Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create the Bedrock Runtime client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "print(f\"Bedrock client initialized for region: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Helper Function\n",
    "\n",
    "This helper function wraps the Bedrock API call. You'll use it throughout all notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str = None,\n",
    "    model_id: str = MODEL_ID,\n",
    "    max_tokens: int = DEFAULT_MAX_TOKENS,\n",
    "    temperature: float = DEFAULT_TEMPERATURE\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call a Mistral model on Bedrock using the Converse API.\n",
    "\n",
    "    Args:\n",
    "        user_prompt: The user message to send\n",
    "        system_prompt: Optional system prompt to set model behavior\n",
    "        model_id: The Bedrock model ID to use\n",
    "        max_tokens: Maximum tokens in the response\n",
    "        temperature: Sampling temperature (0-1)\n",
    "\n",
    "    Returns:\n",
    "        The model's response text\n",
    "    \"\"\"\n",
    "    # Build messages list for Converse API\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": user_prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Build inference config\n",
    "    inference_config = {\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    # Build request parameters\n",
    "    request_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": inference_config\n",
    "    }\n",
    "\n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        request_params[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Call the model using Converse API\n",
    "    response = bedrock_client.converse(**request_params)\n",
    "\n",
    "    # Extract and return the response text with robust error handling\n",
    "    try:\n",
    "        content = response[\"output\"][\"message\"][\"content\"][0]\n",
    "        if \"text\" in content:\n",
    "            return content[\"text\"]\n",
    "        else:\n",
    "            # Handle other content types (e.g., toolUse)\n",
    "            return str(content)\n",
    "    except (KeyError, IndexError) as e:\n",
    "        # Return a descriptive error message if response structure is unexpected\n",
    "        return f\"[Error extracting response: {e}. Raw response: {response}]\"\n",
    "\n",
    "\n",
    "def call_mistral_with_messages(\n",
    "    messages: list,\n",
    "    system_prompt: str = None,\n",
    "    model_id: str = MODEL_ID,\n",
    "    max_tokens: int = DEFAULT_MAX_TOKENS,\n",
    "    temperature: float = DEFAULT_TEMPERATURE\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call a Mistral model with a full messages list (for few-shot prompting).\n",
    "    Uses the Converse API.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content' (content as string)\n",
    "                  Supports 'system', 'user', and 'assistant' roles.\n",
    "                  System messages are automatically extracted and passed separately.\n",
    "        system_prompt: Optional additional system prompt\n",
    "        model_id: The Bedrock model ID to use\n",
    "        max_tokens: Maximum tokens in the response\n",
    "        temperature: Sampling temperature (0-1)\n",
    "\n",
    "    Returns:\n",
    "        The model's response text\n",
    "    \"\"\"\n",
    "    # Extract system messages and non-system messages\n",
    "    system_messages = []\n",
    "    converse_messages = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            # Collect system message content\n",
    "            system_messages.append(msg[\"content\"])\n",
    "        else:\n",
    "            # Convert to Converse API format\n",
    "            converse_messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": [{\"text\": msg[\"content\"]}]\n",
    "            })\n",
    "    \n",
    "    # Combine system messages with any provided system_prompt\n",
    "    combined_system = []\n",
    "    if system_prompt:\n",
    "        combined_system.append(system_prompt)\n",
    "    combined_system.extend(system_messages)\n",
    "    \n",
    "    # Build inference config\n",
    "    inference_config = {\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    # Build request parameters\n",
    "    request_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": converse_messages,\n",
    "        \"inferenceConfig\": inference_config\n",
    "    }\n",
    "\n",
    "    # Add combined system prompt if any\n",
    "    if combined_system:\n",
    "        request_params[\"system\"] = [{\"text\": \"\\n\\n\".join(combined_system)}]\n",
    "\n",
    "    # Call the model using Converse API\n",
    "    response = bedrock_client.converse(**request_params)\n",
    "\n",
    "    # Extract and return the response text with robust error handling\n",
    "    try:\n",
    "        content = response[\"output\"][\"message\"][\"content\"][0]\n",
    "        if \"text\" in content:\n",
    "            return content[\"text\"]\n",
    "        else:\n",
    "            # Handle other content types (e.g., toolUse)\n",
    "            return str(content)\n",
    "    except (KeyError, IndexError) as e:\n",
    "        # Return a descriptive error message if response structure is unexpected\n",
    "        return f\"[Error extracting response: {e}. Raw response: {response}]\"\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Test Your Setup\n",
    "\n",
    "Let's verify everything works with a simple test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test - the model should respond with a single word\n",
    "response = call_mistral(\n",
    "    user_prompt=\"Respond with exactly one word: Hello\",\n",
    "    temperature=0.0  # Use 0 for deterministic output\n",
    ")\n",
    "\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(\"\\nâœ… Setup complete! You're ready to start the tutorial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Understanding the Response Structure\n",
    "\n",
    "Let's look at what comes back from Bedrock in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral_raw(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str = None,\n",
    "    model_id: str = MODEL_ID,\n",
    "    max_tokens: int = DEFAULT_MAX_TOKENS,\n",
    "    temperature: float = DEFAULT_TEMPERATURE\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Call a Mistral model using Converse API and return the full response.\n",
    "    \"\"\"\n",
    "    # Build messages list for Converse API\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": user_prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Build inference config\n",
    "    inference_config = {\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    # Build request parameters\n",
    "    request_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": inference_config\n",
    "    }\n",
    "\n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        request_params[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Call the model using Converse API\n",
    "    response = bedrock_client.converse(**request_params)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Get the full response\n",
    "full_response = call_mistral_raw(\n",
    "    user_prompt=\"What is 2 + 2?\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Full Converse API response structure:\")\n",
    "print(json.dumps(full_response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Converse API response includes:\n",
    "- `output.message.content`: List of content blocks (usually one with `text`)\n",
    "- `output.message.role`: Will be \"assistant\"\n",
    "- `usage`: Token counts (`inputTokens` and `outputTokens`)\n",
    "- `stopReason`: Why generation stopped (e.g., \"end_turn\", \"max_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise: Adjust Temperature\n",
    "\n",
    "Temperature controls randomness. Lower = more deterministic, higher = more creative.\n",
    "\n",
    "Run the same prompt multiple times with different temperatures to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T14:28:07.753721Z",
     "iopub.status.busy": "2026-01-06T14:28:07.753478Z",
     "iopub.status.idle": "2026-01-06T14:28:10.517690Z",
     "shell.execute_reply": "2026-01-06T14:28:10.516483Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Write a one-sentence description of a cat.\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Temperature: 0.0 (deterministic)\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(3):\n",
    "    response = call_mistral(prompt, temperature=0.0)\n",
    "    print(f\"Run {i+1}: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Temperature: 1.0 (creative)\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(3):\n",
    "    response = call_mistral(prompt, temperature=1.0)\n",
    "    print(f\"Run {i+1}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Your setup is complete! Proceed to **Notebook 1: Basic Prompt Structure** to start learning prompt engineering techniques.\n",
    "\n",
    "ðŸ“š [Continue to Notebook 1 â†’](01_basic_prompt_structure.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75ef295-1328-4aa1-8d88-3409ad831649",
   "metadata": {},
   "source": [
    "# Vision QA Agent with Mistral and LlamaIndex\n",
    "\n",
    "## Introduction \n",
    "\n",
    "This notebook provides an integrated environment for processing images and PDFs through advanced OCR and vision-based AI models. It primarily utilises the Mistral OCR model for extracting text from documents and Pixtral 12B (via AWS Bedrock Marketplace) for visual understanding and analysis. It also demonstrates the use of LlamaIndex framework and an agent system that intelligently selects the appropriate method for user queries, either performing OCR extraction or vision analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cada311",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    "1.  AWS Account Access:\n",
    "- Active AWS account with permissions to use Amazon Bedrock.\n",
    "\n",
    "2. Amazon Bedrock Service:\n",
    "\n",
    "- Have model access to Mistral Large 2 (mistral.mistral-large-2407-v1:0)\n",
    "\n",
    "- Permissions to invoke Bedrock models (Bedrock Converse API).\n",
    "\n",
    "3. Mistral API Access:\n",
    "\n",
    "- Valid API Key for the Mistral AI platform to utilise OCR services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab4e06-fdc1-434f-a41b-e78b303772ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U mistral llama-index llama-index-core llama-index-llms-bedrock-converse llama-index-llms-bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a03d2-558f-4a37-aecc-4085a9bd1a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54817f16-144c-4538-8efb-0fc336998b65",
   "metadata": {},
   "source": [
    "- Sets up the foundational environment with essential libraries for AWS, Mistral API integration, and document handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b057a33-9a83-4009-b372-21cb70ddc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import base64\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "from mistralai import Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d9a75-99dd-4816-bbd5-8dd502cbd586",
   "metadata": {},
   "source": [
    "- Configures the AI models used, specifying the Mistral Large 2 (mistral-large-2407-v1) model with a defined token limit for interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a81e5b-d94e-4c6a-855e-9bd823bad2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = BedrockConverse(model=\"mistral.mistral-large-2407-v1:0\", max_tokens = 2048)\n",
    "Settings.llm = BedrockConverse(model=\"mistral.mistral-large-2407-v1:0\", max_tokens = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da017bd4-504e-41cc-b558-e3ee125e11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
    "MISTRAL_API_KEY=\"<ADD_YOUR_MISTRAL_KEY>\"\n",
    "client = Mistral(api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb941d2-1240-4c47-ac99-2ebfed74556c",
   "metadata": {},
   "source": [
    "- Defines the OCR extraction functionality. It identifies the file format and performs OCR using the Mistral OCR API, returning markdown-formatted text. It supports PDF, JPG, JPEG, and PNG formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f9b02-ec0f-4ba6-9f22-c8e9cda3b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_use_OCR(file_prefix='uploaded_file', model=\"mistral-ocr-latest\") -> str:\n",
    "    \"\"\"\n",
    "    Extracts text information from an image or PDF file using OCR.\n",
    "    \n",
    "    Args:\n",
    "        file_prefix (str): Prefix of the file without extension (e.g., 'uploaded_file').\n",
    "        model (str): The OCR model to use for processing. Defaults to \"mistral-ocr-latest\".\n",
    "\n",
    "    Returns: \n",
    "        str: A formatted string containing the extracted text in markdown format.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the file is not found or has an unsupported format.\n",
    "        AssertionError: If the file doesn't exist.\n",
    "    \"\"\"\n",
    "    matching_files = glob.glob(f\"{file_prefix}.*\")\n",
    "    \n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No files found with prefix: {file_prefix}\")\n",
    "    \n",
    "    if len(matching_files) > 1:\n",
    "        raise ValueError(f\"Multiple files found with prefix {file_prefix}: {matching_files}\")\n",
    "    \n",
    "    file_path = matching_files[0]\n",
    "\n",
    "    file = Path(file_path)\n",
    "    assert file.is_file(), f\"File not found: {file_path}\"\n",
    "\n",
    "    # Determine file type\n",
    "    file_extension = file.suffix.lower()\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    \n",
    "    # Process based on file type\n",
    "    if file_extension in ['.jpg', '.jpeg', '.png'] or mime_type in ['image/jpeg', 'image/png']:\n",
    "        return _process_image(file, model)\n",
    "    elif file_extension == '.pdf' or mime_type == 'application/pdf':\n",
    "        return _process_pdf(file, model)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}. Supported formats: JPG, JPEG, PNG, PDF\")\n",
    "\n",
    "def _process_image(image_file, model):\n",
    "    \"\"\"Helper function to process image files with OCR.\"\"\"\n",
    "    # Encode image as base64 for API\n",
    "    encoded = base64.b64encode(image_file.read_bytes()).decode()\n",
    "    base64_data_url = f\"data:image/jpeg;base64,{encoded}\"\n",
    "\n",
    "    # Process image with OCR\n",
    "    image_response = client.ocr.process(\n",
    "        document=ImageURLChunk(image_url=base64_data_url),\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    image_ocr_markdown = image_response.pages[0].markdown\n",
    "\n",
    "    response = f\"\"\"\n",
    "    This is image's OCR in markdown:\\n\\n{image_ocr_markdown}\\n.\\n \n",
    "    \"\"\" \n",
    "    return response\n",
    "\n",
    "def _process_pdf(pdf_file, model):\n",
    "    \"\"\"Helper function to process PDF files with OCR.\"\"\"\n",
    "    # Upload PDF file to Mistral's OCR service\n",
    "    uploaded_file = client.files.upload(\n",
    "        file={\n",
    "            \"file_name\": pdf_file.stem,\n",
    "            \"content\": pdf_file.read_bytes(),\n",
    "        },\n",
    "        purpose=\"ocr\",\n",
    "    )\n",
    "\n",
    "    # Get URL for the uploaded file\n",
    "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
    "\n",
    "    # Process PDF with OCR, including embedded images\n",
    "    pdf_response = client.ocr.process(\n",
    "        document=DocumentURLChunk(document_url=signed_url.url),\n",
    "        model=model,\n",
    "        include_image_base64=True\n",
    "    )\n",
    "    \n",
    "    # Collect all page content\n",
    "    all_pages_markdown = []\n",
    "    for page in pdf_response.pages:\n",
    "        all_pages_markdown.append(page.markdown)\n",
    "    \n",
    "    combined_markdown = \"\\n\\n\".join(all_pages_markdown)\n",
    "    \n",
    "    response = f\"\"\"\n",
    "    This is the information extracted from the file:\\n\\n{combined_markdown}\\n.\\n \n",
    "    \"\"\"\n",
    "    return response\n",
    "\n",
    "extract_info_use_OCR_tool = FunctionTool.from_defaults(fn=extract_info_use_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc6644-61c5-4a7b-9e3c-8cfda941250f",
   "metadata": {},
   "source": [
    "- Handles image-based queries by integrating AWS Bedrock and the Pixtral 12B vision model. This includes converting PDFs to images and processing these to derive textual insights or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d837af0-a491-43b9-ab37-7806b7c88dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bedrock runtime object\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "def vision_unstanding_use_pixtral(prompt=\"\", file_prefix='uploaded_file'):\n",
    "    \"\"\"\n",
    "    Processes images or PDF documents with a vision AI model, Pixtral Model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Text prompt to send to the vision model. Defaults to empty string.\n",
    "        file_prefix (str): Prefix of the file to process (without extension). \n",
    "                           Defaults to 'uploaded_file'.\n",
    "                          The function will search for any file matching this prefix.\n",
    "                          \n",
    "    Returns:\n",
    "        str: The text response from the vision model based on the provided images and prompt.\n",
    "\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    model_id = '<BEDROCK_MARKETPLACE_MODEL_ARN>'\n",
    "    config = {\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 5000\n",
    "    }\n",
    "    \n",
    "    system_prompt = '''\n",
    "    You are a helpful ai assistant for a vision related task. \n",
    "    You generate insights and answer questions based on provided images. \n",
    "    '''\n",
    "    \n",
    "    # Find files matching the prefix\n",
    "    matching_files = glob.glob(f\"{file_prefix}.*\")\n",
    "    \n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No files found with prefix: {file_prefix}\")\n",
    "    \n",
    "    if len(matching_files) > 1:\n",
    "        raise ValueError(f\"Multiple files found with prefix {file_prefix}: {matching_files}. Please specify a unique prefix.\")\n",
    "    \n",
    "    file_path = matching_files[0]\n",
    "    file = Path(file_path)\n",
    "\n",
    "\n",
    "    # Process file based on type\n",
    "    try:\n",
    "        if file.suffix.lower() == '.pdf':\n",
    "            image_paths = _convert_pdf_to_images(file)\n",
    "            temp_dir_created = True\n",
    "        elif file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n",
    "            image_paths = [file_path]\n",
    "            temp_dir_created = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file.suffix}. Supported formats: PDF, JPG, JPEG, PNG, GIF, BMP\")\n",
    "            \n",
    "        # Get model response from images\n",
    "        response = _get_vision_model_response(prompt, image_paths, model_id, system_prompt, config)\n",
    "        \n",
    "        # Clean up temporary files if needed\n",
    "        if temp_dir_created:\n",
    "            _cleanup_temp_files(image_paths[0].parent)\n",
    "            \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Clean up on error if needed\n",
    "        if 'temp_dir_created' in locals() and temp_dir_created and 'image_paths' in locals() and image_paths:\n",
    "            _cleanup_temp_files(image_paths[0].parent)\n",
    "        raise\n",
    "\n",
    "def _convert_pdf_to_images(pdf_path):\n",
    "    \"\"\"Convert PDF pages to optimized images.\"\"\"\n",
    "    # Create temporary directory\n",
    "    temp_dir = Path(tempfile.mkdtemp())\n",
    "    \n",
    "    # Convert PDF pages with optimized settings\n",
    "    pages = convert_from_path(str(pdf_path), dpi=100)\n",
    "    \n",
    "    image_paths = []\n",
    "    for i, page in enumerate(pages):\n",
    "        # Resize to 50% of original size\n",
    "        width, height = page.size\n",
    "        new_width = width // 2\n",
    "        new_height = height // 2\n",
    "        resized_page = page.resize((new_width, new_height))\n",
    "        \n",
    "        # Save as optimized PNG\n",
    "        image_path = temp_dir / f'page_{i+1}.png'\n",
    "        resized_page.save(\n",
    "            image_path, \n",
    "            'PNG',\n",
    "            optimize=True,\n",
    "            quality=70\n",
    "        )\n",
    "        image_paths.append(image_path)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "def _get_image_format(image_path):\n",
    "    \"\"\"Determine the format of an image file.\"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        fmt = img.format.lower() if img.format else 'jpeg'\n",
    "        if fmt == 'jpg':\n",
    "            fmt = 'jpeg'\n",
    "    return fmt\n",
    "\n",
    "def _get_vision_model_response(prompt, image_paths, model_id, system_prompt, config):\n",
    "    \"\"\"Send images and prompt to the vision model and get response.\"\"\"\n",
    "    # Build content blocks\n",
    "    content_blocks = []\n",
    "    \n",
    "    # Add text prompt if provided\n",
    "    if prompt.strip():\n",
    "        content_blocks.append({\"text\": prompt})\n",
    "    \n",
    "    # Add images\n",
    "    for img_path in image_paths:\n",
    "        fmt = _get_image_format(img_path)\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image_raw_bytes = f.read()\n",
    "            \n",
    "        content_blocks.append({\n",
    "            \"image\": {\n",
    "                \"format\": fmt,\n",
    "                \"source\": {\n",
    "                    \"bytes\": image_raw_bytes\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Construct message payload\n",
    "    messages = [{\"role\": \"user\", \"content\": content_blocks}]\n",
    "    \n",
    "    # Create request payload\n",
    "    request_payload = {\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": config[\"max_tokens\"],\n",
    "            \"temperature\": config[\"temperature\"],\n",
    "            \"topP\": config[\"top_p\"]\n",
    "        },\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"modelId\": model_id\n",
    "    }\n",
    "    \n",
    "    # Call the model API\n",
    "    response = bedrock_runtime.converse(**request_payload)\n",
    "    \n",
    "    # Extract text from response\n",
    "    assistant_message = response.get('output', {}).get('message', {})\n",
    "    assistant_content = assistant_message.get('content', [])\n",
    "    result_text = \"\".join(block.get('text', '') for block in assistant_content)\n",
    "    \n",
    "    return result_text\n",
    "\n",
    "def _cleanup_temp_files(temp_dir):\n",
    "    \"\"\"Clean up temporary files and directories.\"\"\"\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "vision_unstanding_use_pixtral_tool = FunctionTool.from_defaults(fn=vision_unstanding_use_pixtral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99c729b-5210-45de-b30d-199105f4ae3d",
   "metadata": {},
   "source": [
    "- Implements an agent using Llamaindex that decides which tool to invoke (OCR extraction or visual understanding) based on user queries, enhancing the responsiveness and usability of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057e299-77ac-4659-848e-d89ca23b5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "import time\n",
    "current_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a Vision QA assistant that extracts information from uploaded images or PDF files.\n",
    "\n",
    "When responding to user queries:\n",
    "- Use extract_info_use_OCR_tool when asked to extract or list information from image/file\n",
    "- Use vision_understanding_use_pixtral_tool when asked for general understanding or questions about image/file\n",
    "  - Pass the user's original query as input to this tool\n",
    "\n",
    "If you don't know the answer, respond only with: \"Sorry, I don't know.\" Never fabricate information.\n",
    "\n",
    "Current time is: {current_time}\n",
    "\"\"\"\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [extract_info_use_OCR_tool, vision_unstanding_use_pixtral_tool], \n",
    "    llm=llm, \n",
    "    verbose=False, # Set verbose=True to display the full trace of steps. \n",
    "    system_prompt = system_prompt,\n",
    "    # allow_parallel_tool_calls = True #¬†Uncomment this line to allow multiple tool invocations\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224a5b3-7e5d-4546-b8b4-6d2842b177ec",
   "metadata": {},
   "source": [
    "- Demonstrates real-time interaction where the agent processes user input and chooses appropriate tools to extract information or analyse visual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf44a7a-9036-4335-b4f3-3c5c8fb80f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"-\" * 120)\n",
    "    print(\" New question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447665a-61f3-429e-8a5d-17fdc58da2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbf3c97-e4d7-4c74-928e-00da84589b68",
   "metadata": {},
   "source": [
    "##¬†Gradio App "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a893b-2573-4f83-84a0-8c51e738f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbe918-a9c0-4ef4-98cc-e2566321ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom CSS for a box-like appearance\n",
    "custom_css = \"\"\"\n",
    ".my-box {\n",
    "    border: 2px solid #ccc;\n",
    "    padding: 16px;\n",
    "    border-radius: 8px;\n",
    "    background-color: #f9f9f9;\n",
    "    max-width: 300px;  /* sets a maximum width */\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6dfb5f-bee2-4b38-a6f3-68755a04c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Function to handle file upload and renaming\n",
    "def upload_file(file):\n",
    "    if not file:\n",
    "        return None, \"‚ö†Ô∏è No file uploaded yet. Please upload a file first.\",gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "    # Get the file extension\n",
    "    file_extension = os.path.splitext(file.name)[1].lower()\n",
    "\n",
    "    # List of allowed extensions\n",
    "    allowed_extensions = [\".pdf\", \".jpg\", \".jpeg\", \".png\"]\n",
    "\n",
    "    # Check if the uploaded file has a valid extension\n",
    "    if file_extension not in allowed_extensions:\n",
    "        return None, f\"‚ö†Ô∏è Invalid file type. Please upload a file with one of the following extensions: {', '.join(allowed_extensions)}\"\n",
    "\n",
    "    # Create a new filename with the same extension\n",
    "    new_filename = f\"uploaded_file{file_extension}\"\n",
    "    new_path = os.path.join(\"./\", new_filename)\n",
    "\n",
    "    # Copy the file to the new location\n",
    "    shutil.copy(file.name, new_path)\n",
    "\n",
    "    # Return the path and a success message\n",
    "\n",
    "    if file_extension in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        return new_path, f\"‚úÖ File uploaded and renamed to {new_filename}\", gr.update(value=file, visible=True), gr.update(visible=False), gr.update(visible=False)  # Show Image\n",
    "    elif file_extension == \".pdf\":\n",
    "        return new_path, f\"‚úÖ File uploaded and renamed to {new_filename}\", gr.update(visible=False), gr.update(value=file, visible=False), gr.update(visible=False) # Show PDF\n",
    "    else:\n",
    "        return new_path, f\"‚úÖ File uploaded and renamed to {new_filename}\", gr.update(visible=False), gr.update(visible=False), gr.update(value=\"‚ö†Ô∏è Unsupported file type. Please upload an image or PDF.\", visible=True)\n",
    "\n",
    "def display_uploaded_file(file):\n",
    "    # Check the file type and display the appropriate component\n",
    "    file_extension = os.path.splitext(file.name)[1].lower()\n",
    "    \n",
    "    if file_extension in [\".jpg\", \".jpeg\", \".png\"]:  # Image file types\n",
    "        return file  # Return image for display\n",
    "    elif file_extension == \".pdf\":  # PDF file type\n",
    "        return file  # Return PDF file for download\n",
    "    else:\n",
    "        return \"‚ö†Ô∏è Unsupported file type. Please upload an image or PDF.\"\n",
    "\n",
    "# Function to clear uploaded file, chat history, trace output, and reset messages\n",
    "def reset_all():\n",
    "    # Remove the uploaded file if it exists\n",
    "    uploaded_file_name = \"uploaded_file\"  # Set the filename you want to delete\n",
    "    for filename in os.listdir(\".\"):\n",
    "        # Check if the file starts with \"uploaded_file\" (ignoring the suffix)\n",
    "        if filename.startswith(uploaded_file_name):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(\".\", filename)\n",
    "            \n",
    "            # Check if it's a file (not a directory) before removing\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"‚úÖ {filename} has been deleted.\")\n",
    "\n",
    "    # Return the reset states\n",
    "    return None, [], \"‚ö†Ô∏è No file uploaded yet. Please upload a file first.\", \"\", \"\", None\n",
    "\n",
    "\n",
    "def add_user_message(chat_history, user_input):\n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append((user_input, None))  # Append user's message with no response yet\n",
    "    return chat_history\n",
    "\n",
    "# Function to generate responses and log the LLM reasoning process\n",
    "def chat_with_trace(user_input, uploaded_file_path, file_status, chat_history=[]):\n",
    "    # Check if a file has been uploaded\n",
    "    if not uploaded_file_path:\n",
    "        chat_history.append((user_input, \"‚ö†Ô∏è Please upload a file first before we continue the conversation.\"))\n",
    "        return chat_history, \"‚ö†Ô∏è No file has been uploaded yet.\"\n",
    "    \n",
    "    # Store the reasoning process\n",
    "    trace_steps = []\n",
    "    trace_steps.append(\"üîÑ Thinking...\")\n",
    "    trace_steps.append(f\"üìÅ Using uploaded file: {uploaded_file_path}\")\n",
    "    \n",
    "    # Get the AI response\n",
    "    response = agent.chat(user_input)\n",
    "    \n",
    "    for tool_output in response.sources:\n",
    "        tool_name = tool_output.tool_name  # Name of the tool invoked\n",
    "        raw_output = tool_output.raw_output  # The raw output from the tool\n",
    "        trace_steps.append(\"üîß Calling Function...\")\n",
    "        trace_steps.append(tool_name)\n",
    "        trace_steps.append(\"üîß Function Output...\")\n",
    "        trace_steps.append(raw_output)\n",
    "\n",
    "    ai_response = response.response\n",
    "\n",
    "    # Append to chat history\n",
    "    chat_history.append((None, ai_response))\n",
    "\n",
    "    trace_steps.append(\"‚úÖ Response Generated.\")\n",
    "\n",
    "    # Format trace output as a bullet list\n",
    "    trace_text = \"\\n\".join(trace_steps)\n",
    "    \n",
    "    return chat_history, trace_text, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb5b20-1dfa-48e9-b648-a6f073e98972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio interface\n",
    "with gr.Blocks(css=custom_css) as chatbot_ui:\n",
    "\n",
    "    gr.Markdown(\"\")\n",
    "    gr.Markdown(\"# ü§ñ Chatbot with Mistral Models and Agent\")\n",
    "\n",
    "    with gr.Column(elem_classes=\"my-box\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### **üî• Model and Agent Framework**\\n\\n\n",
    "        - Agent Orchastration Model: **Mistral Large 2** \\n\\n\n",
    "        - OCR Model: **Mistral OCR** \\n\\n\n",
    "        - Vision Language Model (VLM): **Pixtral 12B** \\n\\n\n",
    "        - Agent Framework: **LlamaIndex**\\n\\n\n",
    "        \"\"\")\n",
    "    \n",
    "\n",
    "    uploaded_file_path = gr.State(None)\n",
    "    chat_history = gr.State([])\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):  # Left: Chat Interface\n",
    "            \n",
    "            chatbox = gr.Chatbot(label=\"Chat Window\")\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"Type a message...\")\n",
    "            send_button = gr.Button(\"Send\")\n",
    "\n",
    "            # button to clear the file and chat history\n",
    "            reset_button = gr.Button(\"Clear All\")\n",
    "\n",
    "        with gr.Column(scale=1):  # Right: Upload File and  LLM Trace\n",
    "            # Add file upload component\n",
    "            file_upload = gr.File(\n",
    "                label=\"Upload File (PDF, JPG, PNG, JPEG)\",\n",
    "                file_types=[\".pdf\", \".jpg\", \".jpeg\", \".png\"],\n",
    "                type=\"filepath\"\n",
    "            )\n",
    "            image_display = gr.Image(label=\"Uploaded Image\", visible=False)\n",
    "            pdf_display = gr.File(label=\"Uploaded PDF File\", visible=False)\n",
    "            error_message = gr.Textbox(label=\"Error\", interactive=False, visible=False)\n",
    "            \n",
    "            # Status message for file upload\n",
    "            file_status = gr.Textbox(\n",
    "                label=\"File Status\", \n",
    "                value=\"‚ö†Ô∏è No file uploaded yet. Please upload a file first.\",\n",
    "                interactive=False\n",
    "            )\n",
    "            \n",
    "            trace_output = gr.Textbox(label=\"Agent Traces\", interactive=False, lines=6)\n",
    "\n",
    "    # Trigger the upload function when a file is uploaded\n",
    "    file_upload.change(\n",
    "        fn=upload_file,  # The function to run when a file is uploaded\n",
    "        inputs=[file_upload],  # The input (the uploaded file)\n",
    "        outputs=[uploaded_file_path, file_status, image_display, pdf_display, error_message]  # Outputs: the file path and the status message\n",
    "    )\n",
    "\n",
    "     # Button click event \n",
    "    send_button.click(\n",
    "        fn=add_user_message,\n",
    "        inputs=[chatbox, user_input],\n",
    "        outputs=[chatbox]\n",
    "    ).then(\n",
    "        fn=chat_with_trace,\n",
    "        inputs=[user_input, uploaded_file_path, file_status, chatbox],\n",
    "        outputs=[chatbox, trace_output, user_input]\n",
    "    )\n",
    "\n",
    "    # Button click event to clear the file, chat history, and reset the status\n",
    "    reset_button.click(\n",
    "        fn=reset_all,  # Function to reset everything\n",
    "        inputs=[],  # No inputs needed for the reset\n",
    "        outputs=[file_upload, chat_history, file_status, trace_output, user_input, chatbox]  # Outputs: reset values for file, chat history, file status, and trace output\n",
    "    )\n",
    "\n",
    "# Launch the Gradio app\n",
    "chatbot_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e1a7a-a9f6-42a8-8b2a-06d812773af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.reset() # clear the chat memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa4769-4fad-4ff6-86f8-dc81405acb53",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "The notebook effectively integrates multiple AI-based technologies for comprehensive document and image analysis, demonstrating robust OCR capabilities combined with advanced visual understanding. It illustrates the practical application of AI agents in automating decision-making for document processing tasks, providing versatile interaction and clear results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

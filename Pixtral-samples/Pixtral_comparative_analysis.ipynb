{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixtral: A Comparative Analysis of Vision Models\n",
    "\n",
    "This notebook provides a structured and in-depth comparison of Pixtral, a cutting-edge vision model, against select peers such as Amazon Nova Pro, Anthropic’s Haiku 3 (excluding version 3.5) and Llama 3.2 11b. Our primary goal is to evaluate Pixtral’s performance, identify its strengths and limitations, and establish best practices for integrating Pixtral into workflows that demand accurate and efficient image understanding.\n",
    "\n",
    "To achieve this, we will conduct a series of controlled tests and qualitative assessments, leveraging services like the Converse API, Amazon Bedrock, and a SageMaker inference endpoint for Pixtral. In addition to exploring model outputs on various image types—ranging from general object recognition tasks to financial document analysis and handwriting transcription—we will employ a judging model (Sonnet 3.5) to systematically evaluate and rank the quality of responses.\n",
    "\n",
    "Through this process, the notebook will:\n",
    "\n",
    "- Demonstrate how to efficiently use Pixtral’s endpoints for real-time inference.\n",
    "- Compare Pixtral’s capabilities to other leading vision models using standardized prompts and test images.\n",
    "- Help you understand the relative advantages of Pixtral, guiding you in deciding when and how to deploy it in your own applications.\n",
    "\n",
    "We have included licensing details and quick-start references for further exploration. By the end of this analysis, you should have a clear perspective on Pixtral’s performance profile and actionable insights into optimizing its use in your specific scenarios.\n",
    "\n",
    "All example outputs have been preserved in this notebook, allowing you to review the results without needing to run the code on your own instance or pay for compute costs. \n",
    "\n",
    "## Use\n",
    "\n",
    "- **License:** Apache 2.0 - Pixtral\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The instructions for how to get started using this notebook can be found in the [Pixtral LMI notebook](https://github.com/aws-samples/mistral-on-aws/blob/59ab4ab9736122200a2d284039cb4557782e4a20/notebooks/Pixtral-samples/Pixtral-12b-LMI-SageMaker-realtime-inference.ipynb)\n",
    "\n",
    "Want to learn more about Pixtral? [Check out the Pixtral_capabilities notebook](https://github.com/aws-samples/mistral-on-aws/blob/main/notebooks/Pixtral-samples/Pixtral_capabilities.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mistral_common[opencv] mistral_common==\"v1.4.4\" 'llmeter[plotting]' numpy==1.26.4 pypdfium2==4.30.1 --force --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from typing import List\n",
    "import pypdfium2 as pdfium\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from llmeter.endpoints import BedrockConverseStream, SageMakerStreamEndpoint\n",
    "from llmeter.experiments import LatencyHeatmap\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.djl_inference import DJLModel\n",
    "\n",
    "# Colors to display information\n",
    "RESET = \"\\033[0m\"\n",
    "GREEN = \"\\033[38;5;29m\"\n",
    "BLUE = \"\\033[38;5;43m\"\n",
    "ORANGE = \"\\033[38;5;208m\"\n",
    "PURPLE = \"\\033[38;5;93m\"\n",
    "RED = \"\\033[38;5;196m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session() # sagemaker session for interacting with different AWS APIs\n",
    "\n",
    "sagemaker_session_bucket = None # bucket to house artifacts\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role() # execution role for the endpoint\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri =f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124\" \n",
    "\n",
    "# You can also obtain the image_uri programatically as follows.\n",
    "# image_uri = image_uris.retrieve(framework=\"djl-lmi\", version=\"0.30.0\", region=\"us-west-2\")\n",
    "\n",
    "model = DJLModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"mistralai/Pixtral-12B-2409\",\n",
    "        \"HF_TOKEN\": \"HF_Token\", #since the model \"mistralai/Pixtral-12B-2409\" is gated model, you need a HF_TOKEN & go to https://huggingface.co/mistralai/Pixtral-12B-2409 to be granted access\n",
    "        \"OPTION_ENGINE\": \"Python\",\n",
    "        \"OPTION_MPI_MODE\": \"true\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"8192\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_TOKENIZER_MODE\": \"mistral\",\n",
    "        \"OPTION_ENTRYPOINT\": \"djl_python.huggingface\",\n",
    "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=4\", # this can be tuned to control how many images per prompt are allowed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(instance_type=\"ml.g5.24xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sagemaker_pdf_to_base64(file_path):\n",
    "    pdf = pdfium.PdfDocument(file_path)\n",
    "    images = []\n",
    "    for page_index in range(len(pdf)):\n",
    "        page = pdf[page_index]\n",
    "        bitmap = page.render()\n",
    "        images.append(bitmap)\n",
    "    encoded_messages = []\n",
    "    for i in range(len(images)):\n",
    "        buffered = BytesIO()\n",
    "        pil_image = images[i].to_pil()\n",
    "        pil_image.save(buffered, format='PNG')\n",
    "        img_byte = buffered.getvalue()\n",
    "        base64_encoded = base64.b64encode(img_byte).decode('utf-8')\n",
    "        encoded_messages.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_encoded}\"\n",
    "                }\n",
    "            })\n",
    "    return encoded_messages\n",
    "\n",
    "def encode_image_to_data_url(image_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a local file path and encodes it to a data URL.\n",
    "    \"\"\"\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    # Determine the image MIME type (e.g., image/jpeg, image/png)\n",
    "    mime_type = Image.open(image_path).get_format_mimetype()\n",
    "    data_url = f\"data:{mime_type};base64,{base64_encoded}\"\n",
    "    return data_url\n",
    "\n",
    "def send_images_to_model(predictor, prompt, image_paths):\n",
    "    \"\"\"\n",
    "    Sends images and a prompt to the model and returns the response in plain text.\n",
    "    \"\"\"\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    \n",
    "    content_list = [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "    }]\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Encode image to data URL\n",
    "        if \".pdf\" in image_path:\n",
    "            content_list.extend(call_sagemaker_pdf_to_base64(image_path))\n",
    "        else:\n",
    "            data_url = encode_image_to_data_url(image_path)\n",
    "            content_list.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": data_url\n",
    "                }\n",
    "                \n",
    "            })\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content_list\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    response = predictor.predict(payload)\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_format(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Normalize the format to a known valid one\n",
    "        fmt = img.format.lower() if img.format else 'jpeg'\n",
    "        # Convert 'jpg' to 'jpeg'\n",
    "        if fmt == 'jpg':\n",
    "            fmt = 'jpeg'\n",
    "    return fmt\n",
    "\n",
    "def get_image_format(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Normalize the format to a known valid one\n",
    "        fmt = img.format.lower() if img.format else 'jpeg'\n",
    "        # Convert 'jpg' to 'jpeg'\n",
    "        if fmt == 'jpg':\n",
    "            fmt = 'jpeg'\n",
    "    return fmt\n",
    "\n",
    "def call_bedrock_pdf_to_base64(file_path):\n",
    "    pdf = pdfium.PdfDocument(file_path)\n",
    "    images = []\n",
    "    for page_index in range(len(pdf)):\n",
    "        page = pdf[page_index]\n",
    "        bitmap = page.render()\n",
    "        images.append(bitmap)\n",
    "    encoded_messages = []\n",
    "    for i in range(len(images)):\n",
    "        buffered = BytesIO()\n",
    "        pil_image = images[i].to_pil()\n",
    "        pil_image.save(buffered, format='PNG')\n",
    "        img_byte = buffered.getvalue()\n",
    "        encoded_messages.append({\n",
    "            \"image\": {\n",
    "                \"format\": \"png\",\n",
    "                \"source\": {\n",
    "                            \"bytes\": img_byte\n",
    "                        }\n",
    "            }\n",
    "        })\n",
    "    return encoded_messages\n",
    "\n",
    "def call_bedrock_model(model_id=None, inference_arn=None, prompt=\"\", image_paths=None, system_prompts=None, temperature=0.1, top_p=0.9, max_tokens=3000):\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    if image_paths is None:\n",
    "        image_paths = []\n",
    "    if system_prompts is None:\n",
    "        system_prompts = []\n",
    "\n",
    "    # Start building the content array for the user message\n",
    "    content_blocks = []\n",
    "\n",
    "    # Include a text block if prompt is provided\n",
    "    if prompt.strip():\n",
    "        content_blocks.append({\"text\": prompt})\n",
    "\n",
    "    # Add images as raw bytes\n",
    "    for img_path in image_paths:\n",
    "        if \".pdf\" in img_path:\n",
    "            content_blocks.extend(call_bedrock_pdf_to_base64(img_path))\n",
    "        else:\n",
    "            fmt = get_image_format(img_path)\n",
    "            # Read the raw bytes of the image (no base64 encoding!)\n",
    "            with open(img_path, 'rb') as f:\n",
    "                image_raw_bytes = f.read()\n",
    "    \n",
    "            content_blocks.append({\n",
    "                \"image\": {\n",
    "                    \"format\": fmt,\n",
    "                    \"source\": {\n",
    "                        \"bytes\": image_raw_bytes\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Construct the messages structure\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_blocks\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Prepare additional kwargs if system prompts are provided\n",
    "    kwargs = {}\n",
    "    if system_prompts:\n",
    "        kwargs[\"system\"] = system_prompts\n",
    "\n",
    "    # Build the arguments for the `converse` call\n",
    "    converse_kwargs = {\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4000,\n",
    "            \"temperature\": temperature,\n",
    "            \"topP\": top_p\n",
    "        },\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "    # Use inferenceArn if provided, otherwise use modelId\n",
    "    if inference_arn:\n",
    "        converse_kwargs[\"inferenceArn\"] = inference_arn\n",
    "    else:\n",
    "        converse_kwargs[\"modelId\"] = model_id\n",
    "\n",
    "    # Call the converse API\n",
    "    try:\n",
    "        response = bedrock_client.converse(**converse_kwargs)\n",
    "    \n",
    "        # Parse the assistant response\n",
    "        assistant_message = response.get('output', {}).get('message', {})\n",
    "        assistant_content = assistant_message.get('content', [])\n",
    "        result_text = \"\".join(block.get('text', '') for block in assistant_content)\n",
    "    except Exception as e:\n",
    "        result_text = f\"Error message: {e}\"\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"Image being analyzed:\")\n",
    "image_path = f\"Pixtral_data/cleaner.jpg\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"Describe this image in a short paragraph.\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we’ll use a LLM as a “judge” to compare the quality of each response. While this automated evaluation can offer valuable insights, it’s best supplemented with human judgment to ensure that the chosen response aligns with your specific goals. If all three outputs appear equally strong, your own criteria and preferences will guide the final decision.\n",
    "\n",
    "For this demonstration, we’ll rely on Sonnet 3.5 as the judge. We’ll provide the original image and the three responses to determine which one emerges as the most accurate and helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(image_path, nova_response, claude_response, llama_response, pixtral_response):\n",
    "    evaluation_prompt = f\"\"\"Here is an image and three different AI models' descriptions of it. Please evaluate which model produced the best description and explain why.\n",
    "\n",
    "Model A (Nova): {nova_response}\n",
    "\n",
    "Model B (Claude): {claude_response}\n",
    "\n",
    "Model C (Llama): {llama_response}\n",
    "\n",
    "Model D (Pixtral): {pixtral_response}\n",
    "\n",
    "Which model provided the best description? Please explain your reasoning and declare a winner.\"\"\"\n",
    "\n",
    "    judge_response = call_bedrock_model(\n",
    "        model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        prompt=evaluation_prompt,\n",
    "        image_paths=image_path,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    print(f\"{GREEN}Judge's Evaluation:{RESET}\")\n",
    "    print(f\"{ORANGE}{judge_response}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing a Financial Statement\n",
    "\n",
    "Next, we’ll examine an Amazon financial document using all three models. Note that Llama 3.2’s image input must not exceed 1120 x 1120 in resolution, which requires us to provide a lower-resolution version of the document for Llama. By contrast, Pixtral and Haiku have no such image resolution constraints. This limitation provides an early indicator of where model selection might depend on input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"Image being analyzed:\")\n",
    "image_path = f\"Pixtral_data/AMZN-Q2-2024-Earning-High-Quality.png\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"\"\"Analyze the attached image of an earnings report.\n",
    "\n",
    "Extract Key Data: Identify and summarize main financial metrics:\n",
    "\n",
    "Title\n",
    "\n",
    "Revenue\n",
    "Net income or loss\n",
    "Earnings per share (EPS)\n",
    "Operating expenses\n",
    "Significant one-time items or adjustments\n",
    "Diluted earnings per share\n",
    "Insights:\n",
    "\n",
    "Evaluate overall financial health based on profitability, revenue growth, or cost management.\n",
    "Note any risks or positive signals impacting future performance.\n",
    "Conclusion: Provide a brief summary of the company’s performance this quarter, highlighting potential growth areas or concerns for investors. If specific data isn't present, then leave blank.\n",
    "\"\"\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwriting Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"Image being analyzed:\")\n",
    "image_path = f\"Pixtral_data/a01-082u-01.png\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"\"\"Analyze the image and transcribe any handwritten text present.\n",
    "Convert the handwriting into a single, continuous string of text.\n",
    "Maintain the original spelling, punctuation, and capitalization as written.\n",
    "Ignore any printed text, drawings, or other non-handwritten elements in the image.\"\"\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"Image being analyzed:\")\n",
    "image_path = f\"Pixtral_data/Amazon_Chart.png\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"\"\"Analyze the attached image of the chart or graph. Your tasks are to:\n",
    "\n",
    "Identify the type of chart or graph (e.g., bar chart, line graph, pie chart, etc.).\n",
    "Extract the key data points, including labels, values, and any relevant scales or units.\n",
    "Identify and describe the main trends, patterns, or significant observations presented in the chart.\n",
    "Generate a clear and concise paragraph summarizing the extracted data and insights. The summary should highlight the most important information and provide an overview that would help someone understand the chart without seeing it.\n",
    "Ensure that your summary is well-structured, accurately reflects the data, and is written in a professional tone.\n",
    "\"\"\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"Image being analyzed:\")\n",
    "image_path = f\"Pixtral_data/dresser.jpg\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"\"\"Analyze the image and provide a detailed description of what you see. Include:\n",
    "\n",
    "1. The main subject or focus of the image\n",
    "2. Key elements or objects present\n",
    "3. Colors, lighting, and overall mood\n",
    "4. Spatial arrangement and composition\n",
    "5. Any text or symbols visible\n",
    "6. Actions or events taking place, if applicable\n",
    "7. Background and setting details\n",
    "8. Distinctive features or unusual aspects\n",
    "9. Estimated time of day or season, if relevant\n",
    "10. Overall context or type of scene (e.g., natural landscape, urban setting, indoor space)\n",
    "\n",
    "Describe the image as if explaining it to someone who cannot see it. Be thorough but concise, focusing on the most important and interesting aspects of the image.\n",
    "\"\"\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning Over Complex Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"Image being analyzed:\")\n",
    "image_path = f\"Pixtral_data/Amazon_Chart.png\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"\"\"Analyze the following image and answer the following questions: \n",
    "\n",
    "-Which quarter had the highest net sales and which quarter had the lowest?\n",
    "-What was the average net sale across quarters?\n",
    "-What was the Q2 2023 & Q2 2025 net sales combined?\n",
    "-Which quarter had the highest operating income and which quarter had the lowers?\n",
    "-What was the average operating income across quarters?\n",
    "-What was the Q2 2023 & Q2 2025 operating income combined?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSI: Insurance Form Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import IFrame\n",
    "import IPython.display as display\n",
    "\n",
    "# open PDF\n",
    "pdf_path = f\"Pixtral_data/insurance_90degree.pdf\"\n",
    "with open(pdf_path, \"rb\") as pdf:\n",
    "    content = pdf.read()\n",
    "\n",
    "# encode PDF\n",
    "base64_pdf = base64.b64encode(content).decode(\"utf-8\")\n",
    "\n",
    "# display encoded PDF\n",
    "print(\"PDF being analyzed:\")\n",
    "display.display(IFrame(f\"data:application/pdf;base64,{base64_pdf}\", width=1000, height=500))\n",
    "\n",
    "prompt = \"\"\"As a medical document analyzer, extract these fields from the insurance verification form and return as JSON:\n",
    "\n",
    "Required fields:\n",
    "- Patient name\n",
    "- Date of birth \n",
    "- Policy number\n",
    "- Insurance provider name\n",
    "- Coverage start date\n",
    "- Phone numbers (work/home)\n",
    "- Subscriber name\n",
    "- Group number\n",
    "- Plan type (PPO/HMO)\n",
    "- Verification date\n",
    "\n",
    "Format the response as:\n",
    "{\n",
    "  'patient_info': {\n",
    "    'name': string,\n",
    "    'dob': string,\n",
    "    'phone': {\n",
    "      'work': string,\n",
    "      'home': string\n",
    "    }\n",
    "  },\n",
    "  'insurance_info': {\n",
    "    'provider': string,\n",
    "    'policy_number': string, \n",
    "    'group_number': string,\n",
    "    'subscriber': string,\n",
    "    'plan_type': string,\n",
    "    'coverage_start': string,\n",
    "    'verification_date': string\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=pdf_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=pdf_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=pdf_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=pdf_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=pdf_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Scene Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"Image being analyzed:\")\n",
    "image_path = f\"/home/sagemaker-user/pixtral/mistral-on-aws/notebooks/Pixtral-samples/Pixtral_data/airport_lanes.jpg\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"\"\"Analyze the image and provide a detailed description of what you see in less than 200 words.\n",
    "    \n",
    "Additionally answer the following questions with maximum 30 words per question:\n",
    "1. Which infrastructure can be identifed in the image?\n",
    "2. Which lane do I need to follow if I want to depart by plane?\n",
    "3. Which sports championship is advertised in the image?\n",
    "\n",
    "Describe the image as if explaining it to someone who cannot see it. Be thorough but concise, focusing on the most important and interesting aspects of the image.\n",
    "\"\"\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SQL from ER Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image being analyzed:\")\n",
    "image_path = \"/home/sagemaker-user/pixtral/mistral-on-aws/notebooks/Pixtral-samples/Pixtral_data/er-diagram.jpeg\"\n",
    "image = Image.open(image_path)\n",
    "display.display(image)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = \"You are an expert on SQL. You have an ER diagram. Prepare PostgreSQL compatible SQL queries to create tables from this ER diagram.\"\n",
    "\n",
    "response_nova = call_bedrock_model(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "print(f\"{BLUE}{response_nova}{RESET}\")\n",
    "\n",
    "response_claude = call_bedrock_model(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Claude Haiku Response:{RESET}\")\n",
    "print(f\"{RED}{response_claude}{RESET}\")\n",
    "\n",
    "response_llama = call_bedrock_model(\n",
    "    model_id=\"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "print(f\"{PURPLE}{response_llama}{RESET}\")\n",
    "\n",
    "response_pixtral = send_images_to_model(\n",
    "    predictor=predictor,\n",
    "    prompt=prompt,\n",
    "    image_paths=image_path\n",
    ")\n",
    "\n",
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "print(f\"{ORANGE}{response_pixtral}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses(\n",
    "    image_path=image_path,\n",
    "    nova_response=response_nova,\n",
    "    claude_response=response_claude,\n",
    "    llama_response=response_llama,\n",
    "    pixtral_response=response_pixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping latency by input & output token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many LLMs, the time to process a request can significantly depend on the length (in number of tokens) of the input provided and the output generated.\n",
    "\n",
    "We can produce a heatmap showing how latency varies by these factors, to give an idea of how optimizing your input length or generation lengths might affect the response times observed by users.\n",
    "\n",
    "The `LatencyHeatmap` experiment automatically generates a set of request payloads with varying (approximate) input lengths and uses it to test the endpoint.\n",
    "\n",
    "To construct the requests, we need a base text to use as a seed. The semantic aspects are not particularly important, so any sufficiently long text can serve the purpose - but remember that many LLMs have their own internal guardrails, so it's possible that the model might decline to respond in some cases.\n",
    "\n",
    "We'd like the generated reply to be limited by the `max_tokens` parameter (so the heatmap can measure latency for various output lengths), so will engineer a prompt that encourages the model to generate as long a response as possible from the seed text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Bedrock's streaming API, we can instead connect with an LLMeter `BedrockConverseStream`. If the selected Jumpstart endpoint supports model streaming, we can instead create an LLMeter `SageMakerStreamEndpoint` to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bedrock_prompt_fn(prompt, **kwargs):\n",
    "    formatted_prompt = f\"Create a story based on the following prompt: {prompt}\"\n",
    "    return BedrockConverseStream.create_payload(\n",
    "        formatted_prompt, inferenceConfig={\"temperature\": 1.0}, **kwargs\n",
    "    )\n",
    "\n",
    "def sagemaker_prompt_fn(prompt, **kwargs):\n",
    "    formatted_prompt = f\"Create a story based on the following prompt: {prompt}\"\n",
    "    return SageMakerStreamEndpoint.create_payload(formatted_prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a seed text and prompt generation function, we're ready to set up our latency heatmapping experiment.\n",
    "\n",
    "- The `source_file` and `create_payload_fn` will be used to generate requests with various input lengths.\n",
    "- The set of `input_lengths` you'd like to test is approximate, since the locally-available tokenizer won't exactly match the one used internally by the model\n",
    "- The set of `output_lengths` you'd like to test may not always be reached, if the model stops generating early for the given prompts.\n",
    "- The `requests_per_combination` impacts both the time to run the test and the quality of your output statistics. Note for example that it doesn't make sense to consider p95 or p99 latency on a dataset with only 10 requests!\n",
    "- A higher number of concurrent `clients` will speed up the overall test run, but could cause problems if you reach quota limits (on as-a-service models) or high request volumes that start to impact response latency (see the \"Load testing\" section below for more details!)\n",
    "\n",
    "Similar to low-level test Runners, the `output_path` can be used to configure where the test result data should be saved (either locally or on the Cloud)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use the same source text as LLMeter's own examples: The text of short story \"Frankenstein\" by Mary Shelley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o Pixtral_data/MaryShelleyFrankenstein.txt \\\n",
    "    https://raw.githubusercontent.com/awslabs/llmeter/main/examples/MaryShelleyFrankenstein.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a source text and a function (below) to format example requests from fragments of that text, we're ready to run our experiment to measure latency across various input and output lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latency_heatmap(\n",
    "    model_id: str,\n",
    "    endpoint_name=None,\n",
    "    source_file=\"Pixtral_data/MaryShelleyFrankenstein.txt\"\n",
    "):\n",
    "    if endpoint_name is None:\n",
    "        endpoint_stream = BedrockConverseStream(\n",
    "            model_id=model_id,\n",
    "        )\n",
    "        prompt_fn = bedrock_prompt_fn\n",
    "    else:\n",
    "        endpoint_stream = SageMakerStreamEndpoint(\n",
    "            endpoint_name,\n",
    "            model_id=model_id\n",
    "        )\n",
    "        prompt_fn = sagemaker_prompt_fn\n",
    "    \n",
    "    latency_heatmap = LatencyHeatmap(\n",
    "        endpoint=endpoint_stream,\n",
    "        clients=4,\n",
    "        requests_per_combination=20,\n",
    "        output_path=f\"data/llmeter/{endpoint_stream.model_id}/heatmap\",\n",
    "        source_file=source_file,\n",
    "        input_lengths=[50, 500, 1000],\n",
    "        output_lengths=[128, 256, 512],\n",
    "        create_payload_fn=prompt_fn,\n",
    "    )\n",
    "\n",
    "    return latency_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_heatmap_nova = get_latency_heatmap(model_id=\"us.amazon.nova-pro-v1:0\")\n",
    "heatmap_results_nova = await latency_heatmap_nova.run()\n",
    "\n",
    "latency_heatmap_claude = get_latency_heatmap(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\")\n",
    "heatmap_results_claude = await latency_heatmap_claude.run()\n",
    "\n",
    "latency_heatmap_llama = get_latency_heatmap(model_id=\"us.meta.llama3-2-11b-instruct-v1:0\")\n",
    "heatmap_results_llama = await latency_heatmap_llama.run()\n",
    "\n",
    "latency_heatmap_pixtral = get_latency_heatmap(model_id=\"Pixtral-12B-2409\", endpoint_name=predictor.endpoint_name)\n",
    "heatmap_results_pixtral = await latency_heatmap_pixtral.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll be able to plot the heatmap results visually to explore how the latency varies with input and output token count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{GREEN}#### Amazon Nova Pro Response:{RESET}\")\n",
    "fig, axs = latency_heatmap_nova.plot_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{GREEN}#### Claude Haiku Latency HeatMap:{RESET}\")\n",
    "fig, axs = latency_heatmap_claude.plot_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{GREEN}#### Llama 3.2 11b Response:{RESET}\")\n",
    "fig, axs = latency_heatmap_llama.plot_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{GREEN}#### Pixtral Response:{RESET}\")\n",
    "fig, axs = latency_heatmap_pixtral.plot_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many models the overall `time_to_last_token` depends more strongly on the number of tokens *generated* by the model (`num_tokens_output`), while the `time_to_first_token` depends more strongly on the *input* length (`num_tokens_input`) if any significant correlation is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Across multiple scenarios, the models—Nova, Claude Haiku, Llama 3.2, and Pixtral—demonstrated varying strengths and weaknesses. The evaluations were conducted with the assistance of a judging LLM (Sonnet 3.5), which assessed clarity, completeness, accuracy, and overall descriptive quality. The following summarizes the key observations from each test scenario:\n",
    "\n",
    "**Household Cleaner Image:**\n",
    "\n",
    "When describing two bottles of disinfectant surface cleaner, Pixtral’s response was judged superior. It provided a highly detailed, visually rich description, accurately capturing colors, branding elements, and design motifs that the other models overlooked. This suggests Pixtral’s strong capability for nuanced visual analysis of everyday objects.\n",
    "\n",
    "**Financial Document Analysis:**\n",
    "\n",
    "In the case of an Amazon financial statement image, Pixtral again excelled. It offered a well-structured, comprehensive breakdown of financial metrics and contextualized the company’s performance effectively. The model’s balanced approach—combining raw data extraction with insightful commentary—surpassed the more limited or less organized presentations from Claude and Llama.\n",
    "\n",
    "**Handwriting Transcription:**\n",
    "\n",
    "For handwriting recognition, none of the models performed perfectly. However, the judge deemed Claude’s response the least inaccurate, as it introduced fewer extraneous words compared to Llama and Pixtral. This scenario highlights a challenge for all tested models: accurately parsing handwritten text. While Claude edged ahead here, the overall quality from all three remained suboptimal. The author isn't entirely sure what the second to third to last word is. None of the models were correct, however, Claude did not introduce additional words while Pixtral and Llama did.\n",
    "\n",
    "**Chart/Graph Interpretation:**\n",
    "\n",
    "When analyzing a chart of North American segment results, Pixtral outperformed the others. It displayed an impressive level of detail, correctly interpreting data points, capturing year-over-year changes, and providing a clear, logical structure. The model’s ability to handle numeric data and present it contextually was a standout feature, reaffirming Pixtral’s strength in scenarios where clarity and thoroughness are paramount.\n",
    "\n",
    "**Indoor Scene Description (Dresser Image):**\n",
    "\n",
    "In describing a minimalist indoor space, Llama was selected as the winner. It provided granular detail, accurately noted subtle elements like a corkboard and shelf, and gave a clear sense of the environment’s purpose and ambiance. Although Pixtral and Claude produced competent descriptions, Llama’s richer detail and organization gave it the edge in this setting.\n",
    "\n",
    "**Insurance Form Data Extraction:**\n",
    "\n",
    "In analyzing medical insurance verification forms, Llama demonstrated a great performance by correctly identifying most information and maintaining proper JSON structure, while other models relied on generic placeholder data. Though Llama and Nova misinterpreted some information, their accurate field capture and standardized formatting outperformed Claude and Pixtral, highlighting both progress and persistent challenges in automated document processing.\n",
    "\n",
    "**Traffic Scene Analysis**\n",
    "\n",
    "In analyzing this traffic scene, Pixtral was selected as the winner. Compared to the other models, it excelled by providing a comprehensive and naturally flowing description of the image. When answering the prompt's questions, Pixtral spots information 'hidden' in one of the image's details, that was not detected by other models. However, it needs to be noted, that Pixtral provides incorrect information with respect to this detail.\n",
    "\n",
    "**Mapping latency by input & output token counts**\n",
    "\n",
    "Examining median (p50) performance, Pixtral demonstrates better first-token response times (0.07-0.94s), outperforming all other models. This advantage is expected, as Pixtral operates on Amazon SageMaker with dedicated resources and direct infrastructure access. The remaining models, running on Amazon Bedrock, are constrained by default service quotas that limit their concurrent requests and processing capacity. Claude Haiku maintains consistent performance with first-token times ranging from 0.3-2.22s. Llama 3.2 shows moderate performance with first-token times of 0.4-3.77s, while Amazon Nova Pro exhibits the highest latency, ranging from 3.01-12.2s.\n",
    "The tail latencies (p99) indicate that Pixtral maintains relatively stable performance even in edge cases, while other models experience more significant performance degradation, particularly with larger token counts. Time-to-last-token metrics follow similar patterns, though with higher absolute values across all models.\n",
    "\n",
    "### Overall Conclusions:\n",
    "\n",
    "**Pixtral frequently delivered the most comprehensive and structured analyses, particularly for tasks requiring detailed, multi-level descriptions of data-rich images (e.g., financial statements, charts, and product details).** Its performance in these areas suggests that it is a strong candidate for use cases demanding thorough and accurate visual summarization.\n",
    "\n",
    "Llama 3.2 excelled in capturing intricate details within certain contexts, as seen in the indoor scene description. Its strength appears to lie in careful observation and nuanced environmental portrayal, making it a good fit for tasks requiring a keen eye for subtle elements and layout.\n",
    "\n",
    "Claude Haiku generally produced reasonable summaries but often lacked the depth or precision of the others. It performed best in the handwriting scenario, possibly due to simpler transcription logic relative to the errors the others introduced. While Claude’s descriptions are understandable and coherent, they may not always match the richer level of detail and analysis provided by Pixtral or Llama.\n",
    "\n",
    "Nova excels at structured document processing, demonstrating superior performance in form data extraction through accurate field identification and JSON structuring. While showing limitations with dates and numbers, its performance indicates strong potential for document automation, fitting well within the broader ecosystem of specialized vision-language models.\n",
    "\n",
    "In conclusion, all four models have their merits and shortcomings. Pixtral stands out for structured, data-heavy image analyses; Llama shines in scenario-based detail and compositional complexity; Nova excels at document processing and form data extraction; Claude is a steady if less detailed performer, excelling occasionally in simpler tasks like handwriting. Depending on the complexity of the use case and the type of image being processed, each model could be the right choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleanup**\n",
    "It's important to cleanup the provisioned resources to avoid incurring costs. You have two options to delete the endpoint created in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1 - **Cleanup using AWS Console**\n",
    "\n",
    "In AWS Console, navigate to Amazon Bedrock service and click on Marketplace deployments under Foundation models. Here, select the deployed endpoint and click on Delete button.\n",
    "\n",
    "Delete Endpoint\n",
    "\n",
    "Upon clicking the Delete button, a confirmation popup shows up. Here you read the warning carefully and confirm deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2 - **Cleanup using SageMaker SDK**\n",
    "You can run below cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

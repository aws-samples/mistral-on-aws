{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c2a0c3-8d15-4b44-9b2e-7d1f3e8f91a4",
   "metadata": {},
   "source": [
    "# ğŸ¤– MCP Chat Application Workshop\n",
    "## Building AI Assistants with Mistral models on Amazon Bedrock & Model Context Protocol\n",
    "\n",
    "Welcome to this hands-on workshop! By the end of this session, you'll have built a fully functional AI chat application that connects Amazon Bedrock models with external tools via the Model Context Protocol (MCP).\n",
    "\n",
    "### What You'll Learn:\n",
    "- How to connect AI models to external tools and services\n",
    "- Working with Mistral models on Amazon Bedrock\n",
    "- Understanding the Model Context Protocol (MCP)\n",
    "- Building chat interfaces using Gradio\n",
    "\n",
    "### Prerequisites:\n",
    "- Basic Python knowledge\n",
    "- AWS account with Bedrock access\n",
    "- Python 3.10+ installed\n",
    "\n",
    "### Workshop Overview:\n",
    "1. **Setup & Configuration** - Get your environment ready\n",
    "2. **Understanding MCP** - Learn about Model Context Protocol\n",
    "3. **Building the Core Chat** - Create a command-line chat interface\n",
    "4. **Adding a Web Interface** - Build a Gradio-based web app\n",
    "5. **Testing & Experimentation** - Try out your application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Environment Configuration\n",
    "\n",
    "Let's start by setting up our environment and understanding what tools we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22988619",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-12T14:42:15.919333Z",
     "iopub.status.busy": "2025-06-12T14:42:15.919043Z",
     "iopub.status.idle": "2025-06-12T14:42:25.067462Z",
     "shell.execute_reply": "2025-06-12T14:42:25.066887Z",
     "shell.execute_reply.started": "2025-06-12T14:42:15.919313Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3>=1.28.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.37.1)\n",
      "Requirement already satisfied: aiohttp>=3.8.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.9.5)\n",
      "Collecting mcp>=0.1.0 (from -r requirements.txt (line 3))\n",
      "  Downloading mcp-1.9.4-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: psutil>=5.9.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (5.9.8)\n",
      "Collecting streamlit>=1.22.0 (from -r requirements.txt (line 5))\n",
      "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.6 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: pytz>=2023.3 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2024.2)\n",
      "Collecting gradio>=4.0.0 (from -r requirements.txt (line 8))\n",
      "  Downloading gradio-5.33.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting strands-agents>=0.1.6 (from -r requirements.txt (line 9))\n",
      "  Downloading strands_agents-0.1.7-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting strands-agents-tools>=0.1.4 (from -r requirements.txt (line 10))\n",
      "  Downloading strands_agents_tools-0.1.5-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting uv (from -r requirements.txt (line 11))\n",
      "  Downloading uv-0.7.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting mcp_weather_server (from -r requirements.txt (line 12))\n",
      "  Downloading mcp_weather_server-0.1.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 1)) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.28.0->-r requirements.txt (line 1)) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3>=1.28.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3>=1.28.0->-r requirements.txt (line 1)) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3>=1.28.0->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.8.0->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.8.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.8.0->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.8.0->-r requirements.txt (line 2)) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.8.0->-r requirements.txt (line 2)) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.0->-r requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: anyio>=4.5 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (4.9.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (2.9.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.2 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (2.11.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (0.0.20)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp>=0.1.0->-r requirements.txt (line 3))\n",
      "  Downloading sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: starlette>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (0.46.2)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /opt/conda/lib/python3.12/site-packages (from mcp>=0.1.0->-r requirements.txt (line 3)) (0.34.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp>=0.1.0->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp>=0.1.0->-r requirements.txt (line 3)) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp>=0.1.0->-r requirements.txt (line 3)) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp>=0.1.0->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (5.28.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (0.10.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit>=1.22.0->-r requirements.txt (line 5))\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=1.22.0->-r requirements.txt (line 5))\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.12/site-packages (from streamlit>=1.22.0->-r requirements.txt (line 5)) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit>=1.22.0->-r requirements.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit>=1.22.0->-r requirements.txt (line 5)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit>=1.22.0->-r requirements.txt (line 5)) (1.38.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.22.0->-r requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.22.0->-r requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit>=1.22.0->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit>=1.22.0->-r requirements.txt (line 5)) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit>=1.22.0->-r requirements.txt (line 5)) (2025.4.26)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (24.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (0.115.12)\n",
      "Collecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 8))\n",
      "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.10.3 (from gradio>=4.0.0->-r requirements.txt (line 8))\n",
      "  Downloading gradio_client-1.10.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio>=4.0.0->-r requirements.txt (line 8))\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (0.30.2)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (3.10.18)\n",
      "Collecting pydub (from gradio>=4.0.0->-r requirements.txt (line 8))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio>=4.0.0->-r requirements.txt (line 8))\n",
      "  Downloading ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=4.0.0->-r requirements.txt (line 8))\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 8))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.12/site-packages (from gradio>=4.0.0->-r requirements.txt (line 8)) (0.15.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.10.3->gradio>=4.0.0->-r requirements.txt (line 8)) (2024.10.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.10.3->gradio>=4.0.0->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio>=4.5->mcp>=0.1.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 8)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 8)) (14.0.0)\n",
      "Collecting docstring-parser<0.16.0,>=0.15 (from strands-agents>=0.1.6->-r requirements.txt (line 9))\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents>=0.1.6->-r requirements.txt (line 9)) (1.32.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0 (from strands-agents>=0.1.6->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents>=0.1.6->-r requirements.txt (line 9)) (1.32.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9)) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9)) (6.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9)) (3.21.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9)) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.30.0 (from strands-agents>=0.1.6->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.30.0 (from strands-agents>=0.1.6->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9))\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting aws-requests-auth<0.5.0,>=0.4.3 (from strands-agents-tools>=0.1.4->-r requirements.txt (line 10))\n",
      "  Downloading aws_requests_auth-0.4.3-py2.py3-none-any.whl.metadata (567 bytes)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools>=0.1.4->-r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: dill<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools>=0.1.4->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.51 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools>=0.1.4->-r requirements.txt (line 10)) (3.0.51)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools>=0.1.4->-r requirements.txt (line 10)) (2.10.1)\n",
      "Collecting slack-bolt<2.0.0,>=1.23.0 (from strands-agents-tools>=0.1.4->-r requirements.txt (line 10))\n",
      "  Downloading slack_bolt-1.23.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools>=0.1.4->-r requirements.txt (line 10)) (1.14.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.51->strands-agents-tools>=0.1.4->-r requirements.txt (line 10)) (0.2.13)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 8)) (2.19.1)\n",
      "Collecting slack_sdk<4,>=3.35.0 (from slack-bolt<2.0.0,>=1.23.0->strands-agents-tools>=0.1.4->-r requirements.txt (line 10))\n",
      "  Downloading slack_sdk-3.35.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy<2.0.0,>=1.12.0->strands-agents-tools>=0.1.4->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27->mcp>=0.1.0->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp>=0.1.0->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio>=4.0.0->-r requirements.txt (line 8)) (3.18.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio>=4.0.0->-r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.22.0->-r requirements.txt (line 5)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.22.0->-r requirements.txt (line 5)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.22.0->-r requirements.txt (line 5)) (0.24.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp>=0.1.0->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.30.0->strands-agents>=0.1.6->-r requirements.txt (line 9)) (1.17.2)\n",
      "Downloading mcp-1.9.4-py3-none-any.whl (130 kB)\n",
      "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m186.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Downloading gradio-5.33.2-py3-none-any.whl (54.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.3/54.3 MB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.3-py3-none-any.whl (323 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading strands_agents-0.1.7-py3-none-any.whl (105 kB)\n",
      "Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "Downloading strands_agents_tools-0.1.5-py3-none-any.whl (158 kB)\n",
      "Downloading aws_requests_auth-0.4.3-py2.py3-none-any.whl (6.8 kB)\n",
      "Downloading slack_bolt-1.23.0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading slack_sdk-3.35.0-py2.py3-none-any.whl (293 kB)\n",
      "Downloading uv-0.7.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.8/17.8 MB\u001b[0m \u001b[31m193.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mcp_weather_server-0.1.3-py3-none-any.whl (8.0 kB)\n",
      "Downloading ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m192.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.3.6-py3-none-any.whl (10 kB)\n",
      "Downloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, watchdog, uv, slack_sdk, semantic-version, ruff, opentelemetry-proto, groovy, ffmpy, docstring-parser, sse-starlette, slack-bolt, pydeck, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, aws-requests-auth, safehttpx, opentelemetry-semantic-conventions, gradio-client, opentelemetry-sdk, mcp, gradio, streamlit, opentelemetry-exporter-otlp-proto-http, mcp_weather_server, strands-agents, strands-agents-tools\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-api90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/27\u001b[0m [pydeck]olt]\n",
      "\u001b[2K    Found existing installation: opentelemetry-api 1.32.1â”â”â”â”â”\u001b[0m \u001b[32m12/27\u001b[0m [pydeck]\n",
      "\u001b[2K    Uninstalling opentelemetry-api-1.32.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/27\u001b[0m [pydeck]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-api-1.32.1â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/27\u001b[0m [pydeck]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-semantic-conventionsâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/27\u001b[0m [aws-requests-auth]\n",
      "\u001b[2K    Found existing installation: opentelemetry-semantic-conventions 0.53b1/27\u001b[0m [aws-requests-auth]\n",
      "\u001b[2K    Uninstalling opentelemetry-semantic-conventions-0.53b1:â”â”â”\u001b[0m \u001b[32m15/27\u001b[0m [aws-requests-auth]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-semantic-conventions-0.53b115/27\u001b[0m [aws-requests-auth]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-sdk1mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/27\u001b[0m [gradio-client]\n",
      "\u001b[2K    Found existing installation: opentelemetry-sdk 1.32.1â”â”â”â”â”\u001b[0m \u001b[32m18/27\u001b[0m [gradio-client]\n",
      "\u001b[2K    Uninstalling opentelemetry-sdk-1.32.1:0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/27\u001b[0m [gradio-client]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-sdk-1.32.1â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/27\u001b[0m [gradio-client]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27/27\u001b[0m [strands-agents-tools]ands-agents]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aws-requests-auth-0.4.3 docstring-parser-0.15 ffmpy-0.6.0 gradio-5.33.2 gradio-client-1.10.3 groovy-0.1.2 mcp-1.9.4 mcp_weather_server-0.1.3 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pydeck-0.9.1 pydub-0.25.1 ruff-0.11.13 safehttpx-0.1.6 semantic-version-2.10.0 slack-bolt-1.23.0 slack_sdk-3.35.0 sse-starlette-2.3.6 strands-agents-0.1.7 strands-agents-tools-0.1.5 streamlit-1.45.1 uv-0.7.12 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-setup",
   "metadata": {},
   "source": [
    "### Environment Variables Setup\n",
    "\n",
    "**Important:** Before running this workshop, you need to set up your AWS credentials and region. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-explanation",
   "metadata": {},
   "source": [
    "## Step 2: Configuration Files\n",
    "\n",
    "Our application uses configuration files to manage settings. Let's create and understand these configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-files",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:43:10.044696Z",
     "iopub.status.busy": "2025-06-12T14:43:10.044343Z",
     "iopub.status.idle": "2025-06-12T14:43:10.050026Z",
     "shell.execute_reply": "2025-06-12T14:43:10.049476Z",
     "shell.execute_reply.started": "2025-06-12T14:43:10.044670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration files created!\n",
      "ğŸ“ config.py - AWS Bedrock settings\n",
      "ğŸ“ server_configs.py - MCP server configurations\n"
     ]
    }
   ],
   "source": [
    "# Let's create our configuration files\n",
    "\n",
    "# AWS Configuration\n",
    "aws_config_content = '''\n",
    "\"\"\"AWS Bedrock configuration settings.\"\"\"\n",
    "import os\n",
    "\n",
    "AWS_CONFIG = {\n",
    "    \"region\": \"us-west-2\",\n",
    "    \"model_id\": \"us.mistral.pixtral-large-2502-v1:0\",  # Mistral model on Bedrock\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "# MCP Server Configuration  \n",
    "server_config_content = '''\n",
    "\"\"\"MCP Server configurations for different tools.\"\"\"\n",
    "from mcp import StdioServerParameters\n",
    "import boto3\n",
    "\n",
    "# Get credentials from the execution role\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "# Configuration for different MCP servers\n",
    "SERVER_CONFIGS = [\n",
    "    # Time utilities server\n",
    "    StdioServerParameters(\n",
    "        command=\"npx\",\n",
    "        args=[\"-y\", \"time-mcp\"]\n",
    "    ),\n",
    "    # AWS documents \n",
    "    StdioServerParameters(\n",
    "        command=\"uvx\",\n",
    "        args=[\"awslabs.aws-documentation-mcp-server@latest\"],\n",
    "        env= {\n",
    "        \"FASTMCP_LOG_LEVEL\": \"ERROR\",\n",
    "        \"AWS_DOCUMENTATION_PARTITION\": \"aws\"\n",
    "      },\n",
    "    ),\n",
    "    # Uncomment and configure if you have Google Maps API key\n",
    "    StdioServerParameters(\n",
    "        command=\"npx\",\n",
    "        args=[\"-y\", \"@modelcontextprotocol/server-google-maps\"],\n",
    "        env={\"GOOGLE_MAPS_API_KEY\": \"<Google_API_KEY>\"}\n",
    "    ),\n",
    "]\n",
    "'''\n",
    "\n",
    "# Write configuration files\n",
    "with open('config.py', 'w') as f:\n",
    "    f.write(aws_config_content)\n",
    "    \n",
    "with open('server_configs.py', 'w') as f:\n",
    "    f.write(server_config_content)\n",
    "    \n",
    "print(\"âœ… Configuration files created!\")\n",
    "print(\"ğŸ“ config.py - AWS Bedrock settings\")\n",
    "print(\"ğŸ“ server_configs.py - MCP server configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-explanation",
   "metadata": {},
   "source": [
    "## Step 3: Understanding Model Context Protocol (MCP) and building MCP components \n",
    "\n",
    "**What is MCP?**\n",
    "The Model Context Protocol is a standard protocol that allows AI models to  connect to external tools and data sources. \n",
    "\n",
    "\n",
    "**How it works:**\n",
    "1. **MCP Host**:Maintains conversation, initiates MCP clients and has helper functions such as supporting image processing.\n",
    "2. **MCP Clients**: Communication component within the host that connects to MCP servers.\n",
    "3. **MCP Servers**: Connect to specific data sources (files, APIs, databases) and serve that data back through the protocol, providing specific tools (like time, maps,file access).\n",
    "4. **Bedrock AI Models**: Provide LLM capabilities to understand user questions, decide which MCP tools to use, and generate the final answer.\n",
    "\n",
    "<img src=\"architecture.png\" width=\"800px\" alt=\"Architecture diagram\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24176e3b-4571-4655-a91c-1b32b8e817ab",
   "metadata": {},
   "source": [
    "Let's see what MCP servers and tools are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mcp-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:43:16.775771Z",
     "iopub.status.busy": "2025-06-12T14:43:16.775466Z",
     "iopub.status.idle": "2025-06-12T14:43:17.281272Z",
     "shell.execute_reply": "2025-06-12T14:43:17.280746Z",
     "shell.execute_reply.started": "2025-06-12T14:43:16.775751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AWS Configuration:\n",
      "   region: us-west-2\n",
      "   model_id: us.mistral.pixtral-large-2502-v1:0\n",
      "\n",
      "ğŸ”§ MCP Servers Configured: 3\n",
      "   Server 1: -y time-mcp\n",
      "   Server 2: awslabs.aws-documentation-mcp-server@latest\n",
      "   Server 3: -y @modelcontextprotocol/server-google-maps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from config import AWS_CONFIG\n",
    "from server_configs import SERVER_CONFIGS\n",
    "\n",
    "print(\"ğŸ¤– AWS Configuration:\")\n",
    "for key, value in AWS_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ MCP Servers Configured: {len(SERVER_CONFIGS)}\")\n",
    "for i, server in enumerate(SERVER_CONFIGS, 1):\n",
    "    print(f\"   Server {i}: {' '.join(server.args)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-chat-explanation",
   "metadata": {},
   "source": [
    "## Next let's set up the Bedrock Model, and MCP clients. \n",
    "\n",
    "1. **BedrockModel** - Connects to Amazon Bedrock for AI language model access (e.g. Mistral Models)\n",
    "2. **MCPClient** -Communicates with MCP servers to access external data sources, APIs, and tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "import-libraries",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:43:20.249639Z",
     "iopub.status.busy": "2025-06-12T14:43:20.249142Z",
     "iopub.status.idle": "2025-06-12T14:43:20.389839Z",
     "shell.execute_reply": "2025-06-12T14:43:20.389319Z",
     "shell.execute_reply.started": "2025-06-12T14:43:20.249614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import asyncio\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "from strands.models import BedrockModel\n",
    "from mcp import stdio_client\n",
    "from datetime import datetime\n",
    "from contextlib import ExitStack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bedrock-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:43:22.602540Z",
     "iopub.status.busy": "2025-06-12T14:43:22.602062Z",
     "iopub.status.idle": "2025-06-12T14:43:22.659887Z",
     "shell.execute_reply": "2025-06-12T14:43:22.659381Z",
     "shell.execute_reply.started": "2025-06-12T14:43:22.602517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing Bedrock model: us.mistral.pixtral-large-2502-v1:0\n",
      "âœ… Bedrock model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the Bedrock model\n",
    "\n",
    "model_id = AWS_CONFIG[\"model_id\"]\n",
    "print(f\"ğŸš€ Initializing Bedrock model: {model_id}\")\n",
    "\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=model_id,\n",
    "    streaming=False  # We'll use non-streaming for Mistral models\n",
    ")\n",
    "\n",
    "print(\"âœ… Bedrock model initialized successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-connection",
   "metadata": {},
   "source": [
    "Now let's connect to our MCP tool servers and see what tools they provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mcp-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:43:28.657822Z",
     "iopub.status.busy": "2025-06-12T14:43:28.657323Z",
     "iopub.status.idle": "2025-06-12T14:43:36.280472Z",
     "shell.execute_reply": "2025-06-12T14:43:36.279963Z",
     "shell.execute_reply.started": "2025-06-12T14:43:28.657800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Setting up MCP clients...\n",
      "âœ… Connected to MCP server 1\n",
      "âœ… Connected to MCP server 2\n",
      "âœ… Connected to MCP server 3\n",
      "ğŸ”¨ Loaded 6 tools from server 1\n",
      "ğŸ”¨ Loaded 3 tools from server 2\n",
      "ğŸ”¨ Loaded 7 tools from server 3\n",
      "\n",
      "ğŸ‰ Total tools available: 16\n",
      "\n",
      "ğŸ”§ Available Tools:\n",
      "   â€¢ current_time\n",
      "   â€¢ relative_time\n",
      "   â€¢ days_in_month\n",
      "   â€¢ get_timestamp\n",
      "   â€¢ convert_time\n",
      "   â€¢ get_week_year\n",
      "   â€¢ read_documentation\n",
      "   â€¢ search_documentation\n",
      "   â€¢ recommend\n",
      "   â€¢ maps_geocode\n",
      "   â€¢ maps_reverse_geocode\n",
      "   â€¢ maps_search_places\n",
      "   â€¢ maps_place_details\n",
      "   â€¢ maps_distance_matrix\n",
      "   â€¢ maps_elevation\n",
      "   â€¢ maps_directions\n"
     ]
    }
   ],
   "source": [
    "# Set up MCP clients and tools\n",
    "print(\"ğŸ”§ Setting up MCP clients...\")\n",
    "\n",
    "# Create MCP clients for each server configuration\n",
    "mcp_clients = [\n",
    "    MCPClient(lambda cfg=server_config: stdio_client(cfg))\n",
    "    for server_config in SERVER_CONFIGS\n",
    "]\n",
    "\n",
    "\n",
    "# Use ExitStack to manage the lifecycle of our MCP connections\n",
    "exit_stack = ExitStack()\n",
    "\n",
    "try:\n",
    "    # Connect to all MCP servers\n",
    "    for i, mcp_client in enumerate(mcp_clients):\n",
    "        exit_stack.enter_context(mcp_client)\n",
    "        print(f\"âœ… Connected to MCP server {i+1}\")\n",
    "    \n",
    "    # Collect all available tools\n",
    "    tools = []\n",
    "    for i, mcp_client in enumerate(mcp_clients):\n",
    "        try:\n",
    "            client_tools = mcp_client.list_tools_sync()\n",
    "            tools.extend(client_tools)\n",
    "            print(f\"ğŸ”¨ Loaded {len(client_tools)} tools from server {i+1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error getting tools from server {i+1}: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Total tools available: {len(tools)}\")\n",
    "    \n",
    "    # Display available tools\n",
    "    if tools:\n",
    "        print(\"\\nğŸ”§ Available Tools:\")\n",
    "        for tool in tools:\n",
    "            tool_spec = tool.tool_spec\n",
    "            # print(f\"   â€¢ {tool_spec['name']}: {tool_spec['description']}\")\n",
    "            print(f\"   â€¢ {tool_spec['name']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error setting up MCP tools: {e}\")\n",
    "    tools = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-creation",
   "metadata": {},
   "source": [
    "### Creating the Strands Agent and chat loop \n",
    "\n",
    "Now we combine our Bedrock model with the MCP tools to create an intelligent Strands agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "create-agent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:43:39.143603Z",
     "iopub.status.busy": "2025-06-12T14:43:39.143287Z",
     "iopub.status.idle": "2025-06-12T14:43:39.147792Z",
     "shell.execute_reply": "2025-06-12T14:43:39.147348Z",
     "shell.execute_reply.started": "2025-06-12T14:43:39.143584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AI Agent created successfully!\n",
      "ğŸ§  Agent has access to 16 tools\n"
     ]
    }
   ],
   "source": [
    "# Create the AI agent\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can use tools to help you answer questions and perform tasks.\n",
    "Be friendly, helpful, and make use of your tools when appropriate.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=bedrock_model, \n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "print(\"ğŸ¤– AI Agent created successfully!\")\n",
    "print(f\"ğŸ§  Agent has access to {len(tools)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent",
   "metadata": {},
   "source": [
    "Let's test our agent with a simple chat loop to make sure everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7df4f6b",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-12T14:43:45.124168Z",
     "iopub.status.busy": "2025-06-12T14:43:45.123850Z",
     "iopub.status.idle": "2025-06-12T14:45:39.964278Z",
     "shell.execute_reply": "2025-06-12T14:45:39.963751Z",
     "shell.execute_reply.started": "2025-06-12T14:43:45.124146Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ User:  what's the time in New York now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #1: current_time\n",
      "The time in New York is currently 10:43 AM.\n",
      "\n",
      "ğŸ¤– Agent: The time in New York is currently 10:43 AM.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ User:  suggest top3 stores in london\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #2: maps_search_places\n",
      "Here are the top 3 stores in London:\n",
      "\n",
      "1. **Liberty London**\n",
      "   - Rating: 4.5\n",
      "   - Address: Regent St., Carnaby, London W1B 5AH, United Kingdom\n",
      "   - Types: Department store, Tourist attraction, Hair care, Clothing store, Beauty salon, Spa, Store, Restaurant, Food, Point of interest, Health, Establishment\n",
      "\n",
      "2. **Selfridges**\n",
      "   - Rating: 4.5\n",
      "   - Address: 400 Oxford St, London W1A 1AB, United Kingdom\n",
      "   - Types: Department store, Cafe, Meal takeaway, Grocery or supermarket, Home goods store, Bar, Store, Restaurant, Food, Point of interest, Establishment\n",
      "\n",
      "3. **Harrods**\n",
      "   - Rating: 4.4\n",
      "   - Address: 87-135 Brompton Rd, London SW1X 7XL, United Kingdom\n",
      "   - Types: Department store, Tourist attraction, Jewelry store, Clothing store, Store, Point of interest, Establishment\n",
      "\n",
      "ğŸ¤– Agent: Here are the top 3 stores in London:\n",
      "\n",
      "1. **Liberty London**\n",
      "   - Rating: 4.5\n",
      "   - Address: Regent St., Carnaby, London W1B 5AH, United Kingdom\n",
      "   - Types: Department store, Tourist attraction, Hair care, Clothing store, Beauty salon, Spa, Store, Restaurant, Food, Point of interest, Health, Establishment\n",
      "\n",
      "2. **Selfridges**\n",
      "   - Rating: 4.5\n",
      "   - Address: 400 Oxford St, London W1A 1AB, United Kingdom\n",
      "   - Types: Department store, Cafe, Meal takeaway, Grocery or supermarket, Home goods store, Bar, Store, Restaurant, Food, Point of interest, Establishment\n",
      "\n",
      "3. **Harrods**\n",
      "   - Rating: 4.4\n",
      "   - Address: 87-135 Brompton Rd, London SW1X 7XL, United Kingdom\n",
      "   - Types: Department store, Tourist attraction, Jewelry store, Clothing store, Store, Point of interest, Establishment\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ User:  is harrod open now?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #3: maps_place_details\n",
      "Yes, Harrods is open now. Here are the current opening hours:\n",
      "\n",
      "- Monday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Tuesday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Wednesday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Thursday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Friday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Saturday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Sunday: 11:30â€¯AMâ€‰â€“â€‰6:00â€¯PM\n",
      "\n",
      "ğŸ¤– Agent: Yes, Harrods is open now. Here are the current opening hours:\n",
      "\n",
      "- Monday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Tuesday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Wednesday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Thursday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Friday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Saturday: 10:00â€¯AMâ€‰â€“â€‰9:00â€¯PM\n",
      "- Sunday: 11:30â€¯AMâ€‰â€“â€‰6:00â€¯PM\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ User:  can you give me supported mistral model ID on bedrock \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #4: search_documentation\n",
      "\n",
      "Tool #5: read_documentation\n",
      "\n",
      "Tool #6: read_documentation\n",
      "\n",
      "Tool #7: read_documentation\n",
      "\n",
      "Tool #8: read_documentation\n",
      "\n",
      "Tool #9: read_documentation\n",
      "\n",
      "Tool #10: read_documentation\n",
      "The supported Mistral model IDs on Amazon Bedrock are:\n",
      "\n",
      "1. **Mistral 7B Instruct**\n",
      "   - Model ID: `mistral.mistral-7b-instruct-v0:2`\n",
      "   - Regions supported: `us-east-1, us-west-2, ap-south-1, ap-southeast-2, ca-central-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "2. **Mistral Large (24.02)**\n",
      "   - Model ID: `mistral.mistral-large-2402-v1:0`\n",
      "   - Regions supported: `us-east-1, us-west-2, ap-south-1, ap-southeast-2, ca-central-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "3. **Mistral Large (24.07)**\n",
      "   - Model ID: `mistral.mistral-large-2407-v1:0`\n",
      "   - Regions supported: `us-west-2`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "4. **Mistral Small (24.02)**\n",
      "   - Model ID: `mistral.mistral-small-2402-v1:0`\n",
      "   - Regions supported: `us-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "5. **Mixtral 8x7B Instruct**\n",
      "   - Model ID: `mistral.mixtral-8x7b-instruct-v0:1`\n",
      "   - Regions supported: `us-east-1, us-west-2, ap-south-1, ap-southeast-2, ca-central-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "6. **Pixtral Large (25.02)**\n",
      "   - Model ID: `mistral.pixtral-large-2502-v1:0`\n",
      "   - Regions supported: `us-east-1, us-east-2, us-west-2, eu-central-1, eu-north-1, eu-west-1, eu-west-3`\n",
      "   - Input modalities: `Text, Image`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "ğŸ¤– Agent: The supported Mistral model IDs on Amazon Bedrock are:\n",
      "\n",
      "1. **Mistral 7B Instruct**\n",
      "   - Model ID: `mistral.mistral-7b-instruct-v0:2`\n",
      "   - Regions supported: `us-east-1, us-west-2, ap-south-1, ap-southeast-2, ca-central-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "2. **Mistral Large (24.02)**\n",
      "   - Model ID: `mistral.mistral-large-2402-v1:0`\n",
      "   - Regions supported: `us-east-1, us-west-2, ap-south-1, ap-southeast-2, ca-central-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "3. **Mistral Large (24.07)**\n",
      "   - Model ID: `mistral.mistral-large-2407-v1:0`\n",
      "   - Regions supported: `us-west-2`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "4. **Mistral Small (24.02)**\n",
      "   - Model ID: `mistral.mistral-small-2402-v1:0`\n",
      "   - Regions supported: `us-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "5. **Mixtral 8x7B Instruct**\n",
      "   - Model ID: `mistral.mixtral-8x7b-instruct-v0:1`\n",
      "   - Regions supported: `us-east-1, us-west-2, ap-south-1, ap-southeast-2, ca-central-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1`\n",
      "   - Input modalities: `Text`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "6. **Pixtral Large (25.02)**\n",
      "   - Model ID: `mistral.pixtral-large-2502-v1:0`\n",
      "   - Regions supported: `us-east-1, us-east-2, us-west-2, eu-central-1, eu-north-1, eu-west-1, eu-west-3`\n",
      "   - Input modalities: `Text, Image`\n",
      "   - Output modalities: `Text`\n",
      "   - Streaming supported: `Yes`\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ User:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# print(\"ğŸ’¬ Chat with the assistant. Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"ğŸ‘¤ User: \")\n",
    "    if user_input.strip().lower() == \"quit\":\n",
    "        print(\"ğŸ‘‹ Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Replace this with your own agent/response logic\n",
    "    response = agent(user_input)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print(f\"ğŸ¤– Agent: {response}\")\n",
    "    print(\"-\" * 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d67f2-01ec-4abf-838b-74c3a61f09af",
   "metadata": {},
   "source": [
    "### test questions: \n",
    "- can you give me mistral models model ID on bedrock "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-section",
   "metadata": {},
   "source": [
    "## Step 4: Building a Web Interface with Gradio\n",
    "\n",
    "Now let's take our chat application to the next level by creating a beautiful web interface using Gradio. This will allow users to:\n",
    "\n",
    "- ğŸ’¬ Chat with the AI in a web browser\n",
    "- ğŸ–¼ï¸ Upload and analyze images\n",
    "- ğŸ”§ View available tools and configuration\n",
    "- ğŸ“± Access from any device\n",
    "\n",
    "### Understanding Gradio\n",
    "\n",
    "Gradio is a Python library that makes it easy to create web interfaces for machine learning models and applications. With just a few lines of code, we can create:\n",
    "- Chat interfaces\n",
    "- File upload areas\n",
    "- Information panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gradio-imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:45:48.640877Z",
     "iopub.status.busy": "2025-06-12T14:45:48.640560Z",
     "iopub.status.idle": "2025-06-12T14:45:50.265065Z",
     "shell.execute_reply": "2025-06-12T14:45:50.264582Z",
     "shell.execute_reply.started": "2025-06-12T14:45:48.640855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Gradio and other necessary libraries for the web interface\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-functions",
   "metadata": {},
   "source": [
    "### Helper Functions for the Web Interface\n",
    "\n",
    "Let's create some helper functions for our web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "web-helpers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:45:52.602461Z",
     "iopub.status.busy": "2025-06-12T14:45:52.601755Z",
     "iopub.status.idle": "2025-06-12T14:45:52.611222Z",
     "shell.execute_reply": "2025-06-12T14:45:52.610793Z",
     "shell.execute_reply.started": "2025-06-12T14:45:52.602437Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_available_tools():\n",
    "    \"\"\"Get list of available tools for display\"\"\"\n",
    "    global tools\n",
    "    if not tools:\n",
    "        return \"No tools available yet.\"\n",
    "    \n",
    "    html = \"<h3>Available Tools</h3>\"\n",
    "    for tool in tools:\n",
    "        try:\n",
    "            tool_spec = tool.tool_spec\n",
    "            name = tool_spec.get('name', 'Unknown Tool')\n",
    "            description = tool_spec.get('description', 'No description available')\n",
    "            \n",
    "            html += f\"\"\"\n",
    "            <div style=\"border: 1px solid #ddd; margin-bottom: 10px; padding: 10px; border-radius: 5px;\">\n",
    "                <div style=\"font-weight: bold; color: #2C3E50;\">{name}</div>\n",
    "                <div style=\"margin-top: 5px;\">{description}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error displaying tool: {str(e)}\")\n",
    "            html += f\"\"\"\n",
    "            <div style=\"border: 1px solid #ddd; margin-bottom: 10px; padding: 10px; border-radius: 5px; color: red;\">\n",
    "                <div>Error displaying tool: {str(e)}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "\n",
    "def convert_image_to_bytes(image):\n",
    "    \"\"\"Convert PIL Image to bytes for Bedrock message format\"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Convert PIL Image to bytes\n",
    "        buffered = io.BytesIO()\n",
    "        # Determine format based on image\n",
    "        format_type = image.format if image.format else 'PNG'\n",
    "        if format_type not in ['PNG', 'JPEG', 'JPG']:\n",
    "            format_type = 'PNG'\n",
    "        \n",
    "        image.save(buffered, format=format_type)\n",
    "        image_bytes = buffered.getvalue()\n",
    "        \n",
    "        return {\n",
    "            'bytes': image_bytes,\n",
    "            'format': format_type.lower()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting image to bytes: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_message(message, image=None):\n",
    "    \"\"\"Process a message from the user and get a response from the agent\"\"\"\n",
    "    global agent, tools\n",
    "    \n",
    "    if agent is None:\n",
    "        # First-time initialization\n",
    "        if not initialize_agent():\n",
    "            return \"Error: Failed to initialize the agent. Please check the logs.\"\n",
    "    \n",
    "    try:\n",
    "        # Handle image input by appending message to agent\n",
    "        if image is not None:\n",
    "            # Convert image to bytes\n",
    "            image_data = convert_image_to_bytes(image)\n",
    "            if image_data:\n",
    "                # Create message with image and text content\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"image\": {\n",
    "                                \"format\": image_data['format'],\n",
    "                                \"source\": {\n",
    "                                    \"bytes\": image_data['bytes']\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"text\": message if message.strip() else \"Please analyze the content of the image.\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                # Append the new message to the agent's messages\n",
    "                agent.messages.append(new_message)\n",
    "                \n",
    "                # Get response from agent\n",
    "                response = agent(message)\n",
    "            else:\n",
    "                # Fallback to text-only if image conversion failed\n",
    "                response = agent(message)\n",
    "        else:\n",
    "            # Text-only message\n",
    "            response = agent(message)\n",
    "        \n",
    "        \n",
    "        # Extract the text content from the agent result\n",
    "        if hasattr(response, 'text'):\n",
    "            display_response = response.text\n",
    "        elif hasattr(response, 'content'):\n",
    "            display_response = response.content\n",
    "        else:\n",
    "            # Fallback to string representation\n",
    "            display_response = str(response)\n",
    "        \n",
    "        return display_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing message: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"I encountered an error: {str(e)}\"\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset conversation history\"\"\"\n",
    "    global agent, tool_usage_history\n",
    "    tool_usage_history = []\n",
    "    return [], \"Conversation has been reset.\"\n",
    "\n",
    "def respond(message, chat_history, image=None):\n",
    "    \"\"\"Process the message and update the chat history\"\"\"\n",
    "    if not message.strip() and image is None:\n",
    "        return chat_history, \"\", None\n",
    "    \n",
    "    # Create user message content\n",
    "    user_content = message\n",
    "    if image is not None:\n",
    "        user_content += \" [Image uploaded]\"\n",
    "    \n",
    "    # Add user message to history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    # Process user message with image\n",
    "    bot_response = process_message(message, image)\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "    \n",
    "    return chat_history, \"\", None  # Return empty message and clear image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-interface",
   "metadata": {},
   "source": [
    "### Building the Gradio Interface\n",
    "\n",
    "Now let's create our beautiful web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ae4838b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:45:56.599590Z",
     "iopub.status.busy": "2025-06-12T14:45:56.599201Z",
     "iopub.status.idle": "2025-06-12T14:45:56.767688Z",
     "shell.execute_reply": "2025-06-12T14:45:56.767209Z",
     "shell.execute_reply.started": "2025-06-12T14:45:56.599570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=\"\"\"\n",
    "    .container {\n",
    "        max-width: 1200px;\n",
    "        margin: auto;\n",
    "    }\n",
    "    .tools-panel {\n",
    "        background-color: #f9f9f9;\n",
    "        border-radius: 10px;\n",
    "        padding: 15px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "\"\"\", title=\"MCP Application Demo with Mistral Models on Amazon Bedrock\") as demo:\n",
    "    \n",
    "    gr.HTML(\"\"\"\n",
    "        <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "            <h1>ğŸ¤– MCP Application Demo with Mistral Models on Amazon Bedrock</h1>\n",
    "            <p>Chat with AI powered by Mistral models on Amazon Bedrock and MCP tools</p>\n",
    "        </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            # Chat interface\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_label=False,\n",
    "                container=True,\n",
    "                show_copy_button=True,\n",
    "                type=\"messages\"\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=6):\n",
    "                    msg = gr.Textbox(\n",
    "                        placeholder=\"Type your message here...\",\n",
    "                        show_label=False,\n",
    "                        container=False\n",
    "                    )\n",
    "                with gr.Column(scale=3):\n",
    "                    img_input = gr.Image(\n",
    "                        type=\"pil\",\n",
    "                        label=\"Upload Image\",\n",
    "                        sources=[\"upload\", \"clipboard\"],\n",
    "                    )\n",
    "                submit = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            # Tools panel with tabs\n",
    "            with gr.Tab(\"Available Tools\"):\n",
    "                tools_display = gr.HTML(\n",
    "                    value=get_available_tools,\n",
    "                    show_label=False\n",
    "                )\n",
    "                refresh_tools = gr.Button(\"Refresh Tools\", size=\"sm\")\n",
    "            \n",
    "            with gr.Tab(\"Configuration\"):\n",
    "                gr.Markdown(\"### Current Configuration\")\n",
    "                with gr.Group():\n",
    "                    gr.Textbox(\n",
    "                        label=\"AWS Region\",\n",
    "                        value=AWS_CONFIG[\"region\"],\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    gr.Textbox(\n",
    "                        label=\"Model ID\", \n",
    "                        value=AWS_CONFIG[\"model_id\"],\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    gr.Markdown(\"*To change configuration, edit config.py and restart*\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot, img_input],\n",
    "        outputs=[chatbot, msg, img_input]\n",
    "    ).then(\n",
    "        fn=get_available_tools,\n",
    "        inputs=None,\n",
    "        outputs=tools_display\n",
    "    )\n",
    "    \n",
    "    submit.click(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot, img_input], \n",
    "        outputs=[chatbot, msg, img_input]\n",
    "    ).then(\n",
    "        fn=get_available_tools,\n",
    "        inputs=None,\n",
    "        outputs=tools_display\n",
    "    )\n",
    "    \n",
    "    clear.click(\n",
    "        fn=reset_conversation,\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, gr.Textbox()]\n",
    "    )\n",
    "    \n",
    "    refresh_tools.click(\n",
    "        fn=get_available_tools,\n",
    "        inputs=None,\n",
    "        outputs=tools_display\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch-app",
   "metadata": {},
   "source": [
    "### Launch Your Web Application!\n",
    "\n",
    "Now for the exciting part - let's launch your web application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e712aba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:46:00.456447Z",
     "iopub.status.busy": "2025-06-12T14:46:00.456143Z",
     "iopub.status.idle": "2025-06-12T14:46:01.057842Z",
     "shell.execute_reply": "2025-06-12T14:46:01.057386Z",
     "shell.execute_reply.started": "2025-06-12T14:46:00.456425Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://1e1ead50134c0972d4.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1e1ead50134c0972d4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #11: current_time\n",
      "The time in London is currently 3:47 PM."
     ]
    }
   ],
   "source": [
    "demo.queue()\n",
    "demo.launch(\n",
    "    share=True,\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7861,\n",
    "    show_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workshop-summary",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations! Workshop Complete!\n",
    "\n",
    "You've successfully built a complete AI chat application with:\n",
    "\n",
    "### âœ… What You've Accomplished:\n",
    "1. **Set up** Amazon Bedrock integration\n",
    "2. **Connected** MCP tool servers\n",
    "3. **Created** an intelligent AI agent\n",
    "4. **Built** a command-line chat interface\n",
    "5. **Deployed** a beautiful web application\n",
    "6. **Added** image analysis capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

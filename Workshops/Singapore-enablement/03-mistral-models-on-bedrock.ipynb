{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral Models on Amazon Bedrock\n",
    "\n",
    "This notebook provides a comprehensive guide to using Mistral AI models on Amazon Bedrock.\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Model | Context | Input Price | Output Price | Best For |\n",
    "|-------|---------|-------------|--------------|----------|\n",
    "| Mistral 7B | 32K | $0.00015/1K | $0.0002/1K | Simple tasks |\n",
    "| Mixtral 8x7B | 32K | $0.00045/1K | $0.0007/1K | General purpose |\n",
    "| Mistral Large 2 | 128K | $0.003/1K | $0.009/1K | Complex tasks |\n",
    "| Pixtral Large | 128K | $0.003/1K | $0.009/1K | Vision + text |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "print(\"‚úÖ Bedrock client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Models\n",
    "\n",
    "**‚ö†Ô∏è Run this cell first** to see which Mistral models are available in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "print(\"Checking available Mistral models...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    response = bedrock.list_foundation_models(byProvider='Mistral AI')\n",
    "    \n",
    "    print(f\"\\n{'Model ID':<50} {'Status'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model in response['modelSummaries']:\n",
    "        model_id = model['modelId']\n",
    "        status = model.get('modelLifecycle', {}).get('status', 'ACTIVE')\n",
    "        print(f\"{model_id:<50} {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\n‚úÖ Copy the exact model IDs above to use in the cells below.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nMake sure you have:\")\n",
    "    print(\"1. Enabled model access in Bedrock console\")\n",
    "    print(\"2. Proper IAM permissions\")\n",
    "    print(\"3. Selected the correct region (us-east-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(model_id, prompt, max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Invoke a Mistral model on Bedrock.\"\"\"\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        inferenceConfig={\"maxTokens\": max_tokens, \"temperature\": temperature}\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mistral 7B Instruct\n",
    "\n",
    "### Overview\n",
    "- **Size**: 7 billion parameters\n",
    "- **Context**: 32K tokens\n",
    "- **Model ID**: `mistral.mistral-7b-instruct-v0:2`\n",
    "- **Best for**: Simple classification, high-throughput tasks\n",
    "- **Cost**: $0.25 per 1M input + 500K output tokens\n",
    "\n",
    "### Why Choose Mistral 7B?\n",
    "\n",
    "**Speed & Cost**: Mistral 7B is 30x cheaper than Large models and processes requests in milliseconds. Perfect for:\n",
    "- **High-volume applications**: Processing thousands of requests per minute\n",
    "- **Real-time systems**: Chatbots, live classification, instant responses\n",
    "- **Budget-conscious projects**: When cost per request matters\n",
    "\n",
    "**When NOT to use**: Complex reasoning, long documents, multilingual beyond 5 languages, advanced code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Customer Support Ticket Classification\n",
    "\n",
    "**Scenario**: You receive 10,000 support tickets daily and need to route them to the right team instantly.\n",
    "\n",
    "**Why 7B?**\n",
    "- Fast enough for real-time routing (<500ms)\n",
    "- Simple classification task doesn't need larger models\n",
    "- Cost: $2.50/day vs $75/day with Large models\n",
    "- Accuracy: 95%+ for well-defined categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_7B = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "\n",
    "# Real-world support ticket\n",
    "ticket = \"\"\"\n",
    "Subject: Can't log into my account\n",
    "Message: I've tried resetting my password 3 times but still can't access my account. \n",
    "The reset email arrives but the link says 'expired' even though I click it immediately.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Classify this support ticket into ONE category:\n",
    "- BILLING: Payment, invoices, refunds\n",
    "- TECHNICAL: Login, bugs, errors\n",
    "- ACCOUNT: Profile, settings, access\n",
    "- PRODUCT: Features, how-to questions\n",
    "\n",
    "Ticket: {ticket}\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "result = invoke_model(MODEL_7B, prompt, max_tokens=10, temperature=0.1)\n",
    "print(f\"Classification: {result}\")\n",
    "print(f\"\\nüí° Why 7B works here: Simple, fast classification with clear categories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Product Description Generation at Scale\n",
    "\n",
    "**Scenario**: E-commerce platform needs to generate 50,000 product descriptions from specifications.\n",
    "\n",
    "**Why 7B?**\n",
    "- Generates consistent, template-based content quickly\n",
    "- Cost: $12.50 for 50K descriptions vs $375 with Large models\n",
    "- Quality: Good enough for standard product descriptions\n",
    "- Speed: Can process entire catalog in hours, not days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product specifications\n",
    "product_specs = {\n",
    "    \"name\": \"UltraGrip Wireless Mouse\",\n",
    "    \"features\": [\"Ergonomic design\", \"2400 DPI\", \"6 programmable buttons\", \"30-hour battery\"],\n",
    "    \"price\": \"$29.99\",\n",
    "    \"target\": \"Gamers and professionals\"\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Write a compelling 50-word product description:\n",
    "\n",
    "Product: {product_specs['name']}\n",
    "Features: {', '.join(product_specs['features'])}\n",
    "Price: {product_specs['price']}\n",
    "Target: {product_specs['target']}\n",
    "\n",
    "Description:\n",
    "\"\"\"\n",
    "\n",
    "result = invoke_model(MODEL_7B, prompt, max_tokens=100, temperature=0.7)\n",
    "print(f\"Generated Description:\\n{result}\")\n",
    "print(f\"\\nüí° Why 7B works here: Template-based generation, high volume, cost-effective.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Mixtral 8x7B Instruct\n",
    "\n",
    "### Overview\n",
    "- **Architecture**: Mixture-of-Experts (8 experts √ó 7B)\n",
    "- **Context**: 32K tokens\n",
    "- **Model ID**: `mistral.mixtral-8x7b-instruct-v0:1`\n",
    "- **Best for**: Multilingual, general-purpose tasks\n",
    "- **Cost**: $0.80 per 1M input + 500K output tokens\n",
    "\n",
    "### Why Choose Mixtral 8x7B?\n",
    "\n",
    "The \"Swiss Army Knife\" of models - handles 80% of use cases well:\n",
    "- **Multilingual**: Excellent across 11+ languages (French, Spanish, German, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean)\n",
    "- **Balanced**: Near-large-model quality at 1/10th the cost\n",
    "- **Versatile**: Code, translation, extraction, reasoning all work well\n",
    "- **MoE Architecture**: Only activates 2 of 8 experts per token = efficient\n",
    "\n",
    "**When to use over 7B**: Multilingual needs, moderate complexity, code generation, better reasoning\n",
    "\n",
    "**When to use Large instead**: Very complex reasoning, 100K+ token documents, mission-critical accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Multilingual Customer Communication\n",
    "\n",
    "**Scenario**: Global e-commerce platform needs to respond to customers in their native language across Europe and Asia.\n",
    "\n",
    "**Why Mixtral?**\n",
    "- Native support for 11+ languages (7B only does 5 well)\n",
    "- Maintains context across languages\n",
    "- Cost: $0.80 per 1M tokens vs $7.50 for Large\n",
    "- Quality: Near-native fluency in supported languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MIXTRAL = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "\n",
    "customer_message = \"\"\"\n",
    "Customer (French): Bonjour, je voudrais retourner ma commande car le produit ne correspond pas √† la description.\n",
    "Order ID: #FR-2024-5891\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a customer service agent. Respond to this customer in their language (French).\n",
    "\n",
    "{customer_message}\n",
    "\n",
    "Tasks:\n",
    "1. Acknowledge their concern\n",
    "2. Explain the return process\n",
    "3. Provide next steps\n",
    "4. Be empathetic and professional\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "result = invoke_model(MODEL_MIXTRAL, prompt, max_tokens=300, temperature=0.5)\n",
    "print(f\"Response:\\n{result}\")\n",
    "print(f\"\\nüí° Why Mixtral: Native French support, maintains professional tone, understands context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Code Generation with Explanation\n",
    "\n",
    "**Scenario**: Developer needs a Python function with documentation and error handling.\n",
    "\n",
    "**Why Mixtral?**\n",
    "- Better code quality than 7B\n",
    "- Can explain code logic\n",
    "- Handles multiple programming languages\n",
    "- 10x cheaper than Large for routine code tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Create a Python function that:\n",
    "1. Connects to a PostgreSQL database\n",
    "2. Executes a parameterized query safely (prevent SQL injection)\n",
    "3. Returns results as a list of dictionaries\n",
    "4. Includes proper error handling and logging\n",
    "5. Has comprehensive docstring\n",
    "\n",
    "Include example usage.\n",
    "\"\"\"\n",
    "\n",
    "result = invoke_model(MODEL_MIXTRAL, prompt, max_tokens=800, temperature=0.2)\n",
    "print(f\"Generated Code:\\n{result}\")\n",
    "print(f\"\\nüí° Why Mixtral: Balances code quality with cost. Good enough for most functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Mistral Large 2 (24.07)\n",
    "\n",
    "### Overview\n",
    "- **Size**: 123 billion parameters\n",
    "- **Context**: 128K tokens\n",
    "- **Model ID**: `mistral.mistral-large-2407-v1:0` or `us.mistral.mistral-large-2407-v1:0`\n",
    "- **Best for**: Complex reasoning, long documents\n",
    "- **Cost**: $7.50 per 1M input + 500K output tokens\n",
    "\n",
    "### Why Choose Mistral Large 2?\n",
    "\n",
    "The \"Expert\" model - when accuracy and capability matter most:\n",
    "- **128K context**: Analyze entire codebases, long documents, books\n",
    "- **Advanced reasoning**: Multi-step logic, complex problem-solving\n",
    "- **Best-in-class**: Competes with GPT-4 and Claude on benchmarks\n",
    "- **Function calling**: Native tool use for agent systems\n",
    "- **JSON mode**: Guaranteed valid structured output\n",
    "\n",
    "**When to use**: Complex analysis, long documents, mission-critical tasks, advanced coding, research\n",
    "\n",
    "**Cost justification**: 30x more expensive than 7B, but saves hours of human time on complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Complex Multi-Step Problem Solving\n",
    "\n",
    "**Scenario**: Financial analyst needs to calculate ROI across multiple scenarios with dependencies.\n",
    "\n",
    "**Why Large 2?**\n",
    "- Multi-step reasoning with intermediate calculations\n",
    "- Can handle complex business logic\n",
    "- Explains reasoning process\n",
    "- Accuracy critical for financial decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different model ID formats for Mistral Large 2\n",
    "prompt = \"\"\"\n",
    "A company is evaluating 3 investment options:\n",
    "\n",
    "Option A: Cloud Migration\n",
    "- Upfront cost: $500,000\n",
    "- Annual savings: $150,000\n",
    "- Implementation time: 6 months\n",
    "- Risk: Medium (20% chance of 3-month delay)\n",
    "\n",
    "Option B: AI Automation\n",
    "- Upfront cost: $300,000\n",
    "- Annual savings: $100,000\n",
    "- Additional revenue: $50,000/year\n",
    "- Implementation time: 9 months\n",
    "- Risk: High (30% chance of 50% cost overrun)\n",
    "\n",
    "Option C: Process Optimization\n",
    "- Upfront cost: $150,000\n",
    "- Annual savings: $80,000\n",
    "- Implementation time: 3 months\n",
    "- Risk: Low (5% chance of minor delays)\n",
    "\n",
    "Calculate for each option:\n",
    "1. Break-even point\n",
    "2. 5-year NPV (discount rate: 8%)\n",
    "3. Risk-adjusted ROI\n",
    "4. Recommend best option with reasoning\n",
    "\n",
    "Show all calculations step-by-step.\n",
    "\"\"\"\n",
    "\n",
    "model_ids_to_try = [\n",
    "    \"mistral.mistral-large-2407-v1:0\",\n",
    "    \"us.mistral.mistral-large-2407-v1:0\",\n",
    "    \"mistral.mistral-large-2402-v1:0\",\n",
    "    \"us.mistral.mistral-large-2402-v1:0\"\n",
    "]\n",
    "\n",
    "success = False\n",
    "for model_id in model_ids_to_try:\n",
    "    try:\n",
    "        print(f\"Trying: {model_id}...\")\n",
    "        result = invoke_model(model_id, prompt, max_tokens=1500, temperature=0.2)\n",
    "        print(f\"\\n‚úÖ Success with: {model_id}\\n\")\n",
    "        print(f\"Financial Analysis:\\n{result}\")\n",
    "        print(f\"\\nüí° Why Large 2: Complex calculations, multi-step reasoning, business-critical decision.\")\n",
    "        success = True\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)[:80]}...\\n\")\n",
    "\n",
    "if not success:\n",
    "    print(\"‚ö†Ô∏è  None of the model IDs worked. Please run 'Check Available Models' cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Pixtral Large (25.02)\n",
    "\n",
    "### Overview\n",
    "- **Modality**: Text + Vision\n",
    "- **Context**: 128K tokens\n",
    "- **Inference Profile**: `us.mistral.pixtral-large-2502-v1:0` (required)\n",
    "- **Best for**: Document understanding, image analysis\n",
    "- **Cost**: $7.50 per 1M input + 500K output tokens + images\n",
    "\n",
    "### Why Choose Pixtral Large?\n",
    "\n",
    "The \"Vision Expert\" - when you need to understand images AND text:\n",
    "- **Multimodal**: Processes images, charts, diagrams, documents\n",
    "- **OCR + Understanding**: Not just text extraction, but comprehension\n",
    "- **Document AI**: Invoices, receipts, forms, contracts with layout\n",
    "- **Visual reasoning**: Analyze charts, compare images, UI/UX review\n",
    "- **Same price as Large 2** for text, images counted as tokens\n",
    "\n",
    "**When to use**: Any task involving images, document processing, visual Q&A, chart analysis\n",
    "\n",
    "**vs OCR-only**: Pixtral understands context and relationships, not just text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Invoice Processing with Validation\n",
    "\n",
    "**Scenario**: Accounts payable team processes 1000 invoices/day. Need to extract data AND validate for errors.\n",
    "\n",
    "**Why Pixtral?**\n",
    "- Understands invoice layout and structure\n",
    "- Extracts data from tables accurately\n",
    "- Can spot anomalies (duplicate line items, calculation errors)\n",
    "- Handles poor quality scans\n",
    "- One model for extraction + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixtral Large - requires inference profile\n",
    "prompt = \"\"\"\n",
    "Analyze this invoice and:\n",
    "\n",
    "1. Extract structured data:\n",
    "   - Invoice number, date, due date\n",
    "   - Vendor details\n",
    "   - Line items (description, quantity, unit price, total)\n",
    "   - Subtotal, tax, total\n",
    "\n",
    "2. Validate:\n",
    "   - Do line item calculations match?\n",
    "   - Does subtotal + tax = total?\n",
    "   - Any duplicate line items?\n",
    "   - Any unusual amounts or patterns?\n",
    "\n",
    "3. Flag issues:\n",
    "   - Missing required fields\n",
    "   - Calculation errors\n",
    "   - Potential duplicates\n",
    "   - Amounts over $10,000 (require approval)\n",
    "\n",
    "Return as JSON with validation_status and issues array.\n",
    "\"\"\"\n",
    "\n",
    "model_ids_to_try = [\n",
    "    \"us.mistral.pixtral-large-2502-v1:0\",\n",
    "    \"mistral.pixtral-large-2502-v1:0\"\n",
    "]\n",
    "\n",
    "success = False\n",
    "for model_id in model_ids_to_try:\n",
    "    try:\n",
    "        print(f\"Trying: {model_id}...\")\n",
    "        result = invoke_model(model_id, prompt, max_tokens=800, temperature=0.1)\n",
    "        print(f\"\\n‚úÖ Success with: {model_id}\\n\")\n",
    "        print(f\"Invoice Analysis:\\n{result}\")\n",
    "        print(f\"\\nüí° Why Pixtral: Understands layout, validates logic, spots errors - not just OCR.\")\n",
    "        success = True\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)[:80]}...\\n\")\n",
    "\n",
    "if not success:\n",
    "    print(\"‚ö†Ô∏è  Pixtral Large is not available.\")\n",
    "    print(\"Note: Pixtral requires model access to be enabled in Bedrock console.\")\n",
    "    print(\"Go to: AWS Console ‚Üí Bedrock ‚Üí Model access ‚Üí Request access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Guide\n",
    "\n",
    "### Decision Tree\n",
    "```\n",
    "Need images? ‚Üí Pixtral Large\n",
    "Need long context (>32K)? ‚Üí Mistral Large 2\n",
    "Complex reasoning? ‚Üí Mistral Large 2\n",
    "Multilingual? ‚Üí Mixtral 8x7B\n",
    "Speed/cost critical? ‚Üí Mistral 7B\n",
    "Default ‚Üí Mixtral 8x7B\n",
    "```\n",
    "\n",
    "### Cost Comparison\n",
    "Processing 1M input + 500K output tokens:\n",
    "- Mistral 7B: $0.25\n",
    "- Mixtral 8x7B: $0.80\n",
    "- Mistral Large 2: $7.50\n",
    "- Pixtral Large: $7.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "\n",
    "### Temperature\n",
    "- **0.0-0.3**: Deterministic (classification, extraction)\n",
    "- **0.4-0.7**: Balanced (general tasks)\n",
    "- **0.8-1.0**: Creative (content generation)\n",
    "\n",
    "### Top-P\n",
    "- **0.9**: Recommended default\n",
    "\n",
    "### Max Tokens\n",
    "- Classification: 10-50\n",
    "- Summaries: 200-500\n",
    "- Long-form: 1000-2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Decision Framework\n",
    "\n",
    "### Quick Decision Tree\n",
    "\n",
    "```\n",
    "START: What's your task?\n",
    "\n",
    "‚îú‚îÄ Processing images/documents with layout?\n",
    "‚îÇ  ‚îî‚îÄ YES ‚Üí Pixtral Large ($7.50/1M tokens)\n",
    "‚îÇ\n",
    "‚îú‚îÄ Document over 32K tokens (>24K words)?\n",
    "‚îÇ  ‚îî‚îÄ YES ‚Üí Mistral Large 2 ($7.50/1M tokens)\n",
    "‚îÇ\n",
    "‚îú‚îÄ Complex multi-step reasoning needed?\n",
    "‚îÇ  ‚îî‚îÄ YES ‚Üí Mistral Large 2 ($7.50/1M tokens)\n",
    "‚îÇ\n",
    "‚îú‚îÄ Need multilingual support (>5 languages)?\n",
    "‚îÇ  ‚îî‚îÄ YES ‚Üí Mixtral 8x7B ($0.80/1M tokens)\n",
    "‚îÇ\n",
    "‚îú‚îÄ Code generation (moderate complexity)?\n",
    "‚îÇ  ‚îî‚îÄ YES ‚Üí Mixtral 8x7B ($0.80/1M tokens)\n",
    "‚îÇ\n",
    "‚îú‚îÄ High volume (>10K requests/day)?\n",
    "‚îÇ  ‚îî‚îÄ YES ‚Üí Mistral 7B ($0.25/1M tokens)\n",
    "‚îÇ\n",
    "‚îî‚îÄ DEFAULT ‚Üí Mixtral 8x7B (Best balance)\n",
    "```\n",
    "\n",
    "### Real-World Cost Comparison\n",
    "\n",
    "**Scenario: Customer Support System (10,000 tickets/day)**\n",
    "\n",
    "| Model | Daily Cost | Speed | Accuracy | Best For |\n",
    "|-------|------------|-------|----------|----------|\n",
    "| Mistral 7B | $2.50 | <500ms | 95% | Simple routing |\n",
    "| Mixtral 8x7B | $8.00 | ~1s | 98% | Complex categorization |\n",
    "| Mistral Large 2 | $75.00 | ~2s | 99%+ | High-value customers |\n",
    "\n",
    "**Recommendation**: Use 7B for initial routing, escalate 10% to Mixtral for complex cases. Saves $2,000/month vs using Large for everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Mistral AI Documentation](https://docs.mistral.ai/)\n",
    "- [Mistral on AWS](https://docs.mistral.ai/deployment/cloud/aws)\n",
    "- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

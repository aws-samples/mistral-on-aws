{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Mistral Models on Amazon SageMaker JumpStart\n",
    "\n",
    "This notebook guides you through deploying and using Mistral models on Amazon SageMaker JumpStart.\n",
    "\n",
    "## What is SageMaker JumpStart?\n",
    "\n",
    "SageMaker JumpStart provides pre-trained, open-source models for a wide range of problem types. Benefits:\n",
    "- **One-click deployment**: Deploy models without writing deployment code\n",
    "- **Customizable**: Fine-tune models on your own data\n",
    "- **Cost-effective**: Pay only for the compute you use\n",
    "- **Full control**: Models run in your VPC, data stays in your account\n",
    "\n",
    "## Available Mistral Models on JumpStart\n",
    "\n",
    "As of June 2025, 16+ Mistral models are available. This notebook focuses on:\n",
    "\n",
    "**Mistral-Small-3.2-24B-Instruct-2506** (Latest)\n",
    "- **Size**: 24 billion parameters\n",
    "- **Context**: 32K tokens\n",
    "- **Best for**: Balanced performance and cost\n",
    "- **Advantages**: Latest improvements, efficient inference\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **AWS Account** with SageMaker access\n",
    "2. **IAM Permissions**: SageMaker full access or specific permissions\n",
    "3. **Service Quotas**: Ensure you have quota for ml.g5 instances\n",
    "4. **Region**: This notebook uses us-west-2 for best model availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "First, let's install the SageMaker Python SDK if it's not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing SageMaker Python SDK...\n",
      "\n",
      "‚úÖ SageMaker SDK already installed (version 2.254.1)\n"
     ]
    }
   ],
   "source": [
    "# Install SageMaker SDK\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"üì¶ Installing SageMaker Python SDK...\\n\")\n",
    "\n",
    "try:\n",
    "    import sagemaker\n",
    "    print(f\"‚úÖ SageMaker SDK already installed (version {sagemaker.__version__})\")\n",
    "except ImportError:\n",
    "    print(\"Installing sagemaker package...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sagemaker\"])\n",
    "    print(\"‚úÖ SageMaker SDK installed successfully\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please restart the kernel and run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SageMaker session initialized\n",
      "   Region: us-east-2\n",
      "   Role: arn:aws:iam::314146324612:role/Admin\n",
      "   Session: <sagemaker.session.Session object at 0x13bca9130>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.session import Session\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize SageMaker session with US region\n",
    "region = 'us-east-2'  # Change to 'us-west-1' if preferred\n",
    "boto3_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = Session(boto_session=boto3_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"‚úÖ SageMaker session initialized\")\n",
    "print(f\"   Region: {region}\")\n",
    "print(f\"   Role: {role}\")\n",
    "print(f\"   Session: {sagemaker_session}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Create IAM Role for Workshop Studio (If Needed)\n",
    "\n",
    "For Workshop Studio environments with limited IAM permissions, we need to create a proper SageMaker execution role.\n",
    "\n",
    "This cell will:\n",
    "1. Check if the current role has necessary permissions\n",
    "2. Create a new SageMaker execution role if needed\n",
    "3. Attach required policies for JumpStart model access\n",
    "\n",
    "**Note**: If you're in a Workshop Studio environment, you may need to use this role instead of the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up IAM Role for SageMaker JumpStart\n",
      "\n",
      "================================================================================\n",
      "Account ID: 314146324612\n",
      "Region: us-east-2\n",
      "\n",
      "Creating IAM role: SageMakerJumpStartExecutionRole...\n",
      "‚úÖ Role created: arn:aws:iam::314146324612:role/SageMakerJumpStartExecutionRole\n",
      "\n",
      "Attaching managed policies...\n",
      "‚úÖ Attached AmazonSageMakerFullAccess\n",
      "\n",
      "Creating custom policy: SageMakerJumpStartS3Access...\n",
      "‚úÖ Policy created: arn:aws:iam::314146324612:policy/SageMakerJumpStartS3Access\n",
      "‚úÖ Attached custom JumpStart policy\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ IAM Setup Complete!\n",
      "\n",
      "Using role: arn:aws:iam::314146324612:role/SageMakerJumpStartExecutionRole\n",
      "\n",
      "üí° This role has permissions to:\n",
      "   - Access JumpStart model artifacts in S3\n",
      "   - Pull container images from ECR\n",
      "   - Create and manage SageMaker endpoints\n",
      "   - Write logs to CloudWatch\n",
      "\n",
      "‚è≥ Waiting 10 seconds for IAM changes to propagate...\n",
      "‚úÖ Ready to deploy!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "print(\"üîß Setting up IAM Role for SageMaker JumpStart\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get account ID\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\\n\")\n",
    "\n",
    "# Define role name\n",
    "sagemaker_role_name = 'SageMakerJumpStartExecutionRole'\n",
    "\n",
    "# Trust policy for SageMaker\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Custom policy for JumpStart S3 access\n",
    "jumpstart_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::jumpstart-cache-prod-{region}\",\n",
    "                f\"arn:aws:s3:::jumpstart-cache-prod-{region}/*\",\n",
    "                \"arn:aws:s3:::jumpstart-cache-prod-*\",\n",
    "                \"arn:aws:s3:::jumpstart-cache-prod-*/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:GetAuthorizationToken\",\n",
    "                \"ecr:BatchCheckLayerAvailability\",\n",
    "                \"ecr:GetDownloadUrlForLayer\",\n",
    "                \"ecr:BatchGetImage\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the role\n",
    "    print(f\"Creating IAM role: {sagemaker_role_name}...\")\n",
    "    \n",
    "    try:\n",
    "        create_role_response = iam_client.create_role(\n",
    "            RoleName=sagemaker_role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description='SageMaker execution role for JumpStart models',\n",
    "            MaxSessionDuration=3600\n",
    "        )\n",
    "        print(f\"‚úÖ Role created: {create_role_response['Role']['Arn']}\")\n",
    "        new_role_arn = create_role_response['Role']['Arn']\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "            print(f\"‚ÑπÔ∏è  Role already exists, retrieving...\")\n",
    "            get_role_response = iam_client.get_role(RoleName=sagemaker_role_name)\n",
    "            new_role_arn = get_role_response['Role']['Arn']\n",
    "            print(f\"‚úÖ Using existing role: {new_role_arn}\")\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Attach AWS managed policy for SageMaker\n",
    "    print(\"\\nAttaching managed policies...\")\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=sagemaker_role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    "        )\n",
    "        print(\"‚úÖ Attached AmazonSageMakerFullAccess\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] != 'EntityAlreadyExists':\n",
    "            print(f\"‚ö†Ô∏è  Could not attach AmazonSageMakerFullAccess: {e}\")\n",
    "    \n",
    "    # Create and attach custom JumpStart policy\n",
    "    jumpstart_policy_name = 'SageMakerJumpStartS3Access'\n",
    "    jumpstart_policy_arn = f\"arn:aws:iam::{account_id}:policy/{jumpstart_policy_name}\"\n",
    "    \n",
    "    print(f\"\\nCreating custom policy: {jumpstart_policy_name}...\")\n",
    "    try:\n",
    "        create_policy_response = iam_client.create_policy(\n",
    "            PolicyName=jumpstart_policy_name,\n",
    "            PolicyDocument=json.dumps(jumpstart_policy),\n",
    "            Description='Custom policy for SageMaker JumpStart S3 access'\n",
    "        )\n",
    "        print(f\"‚úÖ Policy created: {create_policy_response['Policy']['Arn']}\")\n",
    "        jumpstart_policy_arn = create_policy_response['Policy']['Arn']\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "            print(f\"‚ÑπÔ∏è  Policy already exists\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Could not create policy: {e}\")\n",
    "    \n",
    "    # Attach custom policy to role\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=sagemaker_role_name,\n",
    "            PolicyArn=jumpstart_policy_arn\n",
    "        )\n",
    "        print(f\"‚úÖ Attached custom JumpStart policy\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] != 'EntityAlreadyExists':\n",
    "            print(f\"‚ö†Ô∏è  Could not attach custom policy: {e}\")\n",
    "    \n",
    "    # Update the role variable to use the new role\n",
    "    role = new_role_arn\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\n‚úÖ IAM Setup Complete!\")\n",
    "    print(f\"\\nUsing role: {role}\")\n",
    "    print(f\"\\nüí° This role has permissions to:\")\n",
    "    print(f\"   - Access JumpStart model artifacts in S3\")\n",
    "    print(f\"   - Pull container images from ECR\")\n",
    "    print(f\"   - Create and manage SageMaker endpoints\")\n",
    "    print(f\"   - Write logs to CloudWatch\")\n",
    "    \n",
    "    # Wait a few seconds for IAM to propagate\n",
    "    print(f\"\\n‚è≥ Waiting 10 seconds for IAM changes to propagate...\")\n",
    "    import time\n",
    "    time.sleep(10)\n",
    "    print(f\"‚úÖ Ready to deploy!\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    \n",
    "    if error_code == 'AccessDenied':\n",
    "        print(f\"\\n‚ùå Access Denied: Your current IAM user/role doesn't have permission to create IAM roles.\")\n",
    "        print(f\"\\nüìã Required IAM Permissions:\")\n",
    "        print(f\"   - iam:CreateRole\")\n",
    "        print(f\"   - iam:AttachRolePolicy\")\n",
    "        print(f\"   - iam:CreatePolicy\")\n",
    "        print(f\"   - iam:GetRole\")\n",
    "        print(f\"\\nüîß Solutions:\")\n",
    "        print(f\"   1. Ask your AWS administrator to create the role with the policies shown above\")\n",
    "        print(f\"   2. Use the existing role if it has the necessary permissions\")\n",
    "        print(f\"   3. Try using Amazon Bedrock instead (no IAM setup needed) - see notebook 03\")\n",
    "        print(f\"\\nüìù Current role: {role}\")\n",
    "        print(f\"   You can try to proceed with this role, but deployment may fail if it lacks S3 permissions.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        print(f\"\\nüìù Current role: {role}\")\n",
    "        print(f\"   Proceeding with existing role...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "    print(f\"\\nüìù Current role: {role}\")\n",
    "    print(f\"   Proceeding with existing role...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Select and Configure the Model\n",
    "\n",
    "We'll use **Mistral-7B-Instruct**, a reliable and widely available model in SageMaker JumpStart.\n",
    "\n",
    "### Why Mistral 7B Instruct?\n",
    "\n",
    "- **Widely Available**: Supported in all major AWS regions\n",
    "- **Cost-Effective**: Smaller model means lower inference costs\n",
    "- **Fast**: Quick inference times with good performance\n",
    "- **Versatile**: Handles most common use cases well\n",
    "- **Proven**: Battle-tested model with strong community support\n",
    "\n",
    "### Instance Selection\n",
    "\n",
    "Recommended instances for 7B model:\n",
    "- **ml.g5.2xlarge**: 1 GPU, good for testing ($1.21/hour)\n",
    "- **ml.g5.xlarge**: 1 GPU, more economical ($1.01/hour)\n",
    "- **ml.g5.4xlarge**: 1 GPU, better performance ($1.94/hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  Model ID: huggingface-llm-mistral-7b-instruct\n",
      "  Version: *\n",
      "  Instance: ml.g5.2xlarge\n",
      "  Region: us-east-2\n",
      "\n",
      "üí° Tip: You can change instance_type to ml.g5.4xlarge or ml.g5.12xlarge for better performance.\n",
      "\n",
      "üìù Note: Using Mistral 7B Instruct - a reliable and widely available model.\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "# Using Mistral 7B Instruct - widely available in SageMaker JumpStart\n",
    "model_id = \"huggingface-llm-mistral-7b-instruct\"\n",
    "model_version = \"*\"  # Use latest version\n",
    "\n",
    "# Instance configuration\n",
    "instance_type = \"ml.g5.2xlarge\"  # Start with smaller instance for testing\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  Model ID: {model_id}\")\n",
    "print(f\"  Version: {model_version}\")\n",
    "print(f\"  Instance: {instance_type}\")\n",
    "print(f\"  Region: {region}\")\n",
    "print(f\"\\nüí° Tip: You can change instance_type to ml.g5.4xlarge or ml.g5.12xlarge for better performance.\")\n",
    "print(f\"\\nüìù Note: Using Mistral 7B Instruct - a reliable and widely available model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Verify IAM Permissions (Optional)\n",
    "\n",
    "Before deploying, let's verify your IAM role has the necessary permissions to access JumpStart models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking IAM Role Configuration...\n",
      "\n",
      "================================================================================\n",
      "Current Identity: arn:aws:sts::314146324612:assumed-role/Admin/joshtam-Isengard\n",
      "Account: 314146324612\n",
      "\n",
      "SageMaker Role: SageMakerJumpStartExecutionRole\n",
      "‚úÖ Role exists\n",
      "\n",
      "Attached Policies: 2\n",
      "  - SageMakerJumpStartS3Access\n",
      "  - AmazonSageMakerFullAccess\n",
      "\n",
      "üí° If deployment fails with S3 access errors, you may need to add:\n",
      "   - AmazonSageMakerFullAccess policy\n",
      "   - Or custom policy with s3:GetObject on jumpstart-cache-prod-* buckets\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check IAM role permissions\n",
    "import boto3\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "print(\"üîç Checking IAM Role Configuration...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Get current identity\n",
    "    identity = sts_client.get_caller_identity()\n",
    "    print(f\"Current Identity: {identity['Arn']}\")\n",
    "    print(f\"Account: {identity['Account']}\")\n",
    "    \n",
    "    # Extract role name from ARN\n",
    "    role_name = role.split('/')[-1]\n",
    "    print(f\"\\nSageMaker Role: {role_name}\")\n",
    "    \n",
    "    # Check if role exists and has SageMaker trust policy\n",
    "    try:\n",
    "        role_info = iam_client.get_role(RoleName=role_name)\n",
    "        print(\"‚úÖ Role exists\")\n",
    "        \n",
    "        # Check attached policies\n",
    "        attached_policies = iam_client.list_attached_role_policies(RoleName=role_name)\n",
    "        print(f\"\\nAttached Policies: {len(attached_policies['AttachedPolicies'])}\")\n",
    "        for policy in attached_policies['AttachedPolicies'][:5]:\n",
    "            print(f\"  - {policy['PolicyName']}\")\n",
    "        \n",
    "        print(\"\\nüí° If deployment fails with S3 access errors, you may need to add:\")\n",
    "        print(\"   - AmazonSageMakerFullAccess policy\")\n",
    "        print(\"   - Or custom policy with s3:GetObject on jumpstart-cache-prod-* buckets\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not retrieve role details: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error checking permissions: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the Model\n",
    "\n",
    "This will:\n",
    "1. Create a SageMaker endpoint configuration\n",
    "2. Launch the specified instance\n",
    "3. Load the model onto the instance\n",
    "4. Create an endpoint for inference\n",
    "\n",
    "**‚è±Ô∏è Deployment time**: 5-10 minutes\n",
    "\n",
    "**üí∞ Cost**: You'll be charged for the instance from deployment until deletion\n",
    "\n",
    "### Troubleshooting IAM Permissions\n",
    "\n",
    "If you get S3 access errors, your IAM role needs these permissions:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::jumpstart-cache-prod-*\",\n",
    "                \"arn:aws:s3:::jumpstart-cache-prod-*/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Alternative**: Use Amazon Bedrock for Mistral models (no IAM setup needed) - see notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model deployment...\n",
      "\n",
      "This will take 5-10 minutes. Please wait...\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-llm-mistral-7b-instruct' with wildcard version identifier '*'. You can pin to version '3.22.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------*"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please check the troubleshooting guide for common errors: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html#sagemaker-python-sdk-troubleshooting-create-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Deployment failed: Error hosting endpoint hf-llm-mistral-7b-instruct-2025-11-22-04-29-10-139: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html\n",
      "\n",
      "Common issues:\n",
      "1. Insufficient service quota for the instance type\n",
      "2. Model not available in your region\n",
      "3. IAM role lacks necessary permissions\n",
      "\n",
      "Try:\n",
      "- Request quota increase in Service Quotas console\n",
      "- Use a different instance type (ml.g5.xlarge)\n",
      "- Check IAM role has SageMaker permissions\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting model deployment...\\n\")\n",
    "print(\"This will take 5-10 minutes. Please wait...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Create JumpStart model\n",
    "    model = JumpStartModel(\n",
    "        model_id=model_id,\n",
    "        model_version=model_version,\n",
    "        instance_type=instance_type,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    # Deploy the model\n",
    "    start_time = datetime.now()\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        wait=True  # Wait for deployment to complete\n",
    "    )\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    deployment_time = (end_time - start_time).total_seconds() / 60\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\n‚úÖ Model deployed successfully!\")\n",
    "    print(f\"   Endpoint name: {predictor.endpoint_name}\")\n",
    "    print(f\"   Deployment time: {deployment_time:.1f} minutes\")\n",
    "    print(f\"   Instance: {instance_type}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Remember: You're now being charged for this instance!\")\n",
    "    print(f\"   Delete the endpoint when done to stop charges.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Deployment failed: {e}\")\n",
    "    print(\"\\nCommon issues:\")\n",
    "    print(\"1. Insufficient service quota for the instance type\")\n",
    "    print(\"2. Model not available in your region\")\n",
    "    print(\"3. IAM role lacks necessary permissions\")\n",
    "    print(\"\\nTry:\")\n",
    "    print(\"- Request quota increase in Service Quotas console\")\n",
    "    print(\"- Use a different instance type (ml.g5.xlarge)\")\n",
    "    print(\"- Check IAM role has SageMaker permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Invoke the Model - Basic Usage\n",
    "\n",
    "Now that the model is deployed, let's learn how to invoke it. Mistral models use a specific message format with roles and content.\n",
    "\n",
    "### Message Format\n",
    "\n",
    "Mistral models expect messages in this format:\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Your prompt here\"}\n",
    "    ],\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- **messages**: List of conversation messages with roles (user/assistant/system)\n",
    "- **max_tokens**: Maximum tokens to generate (default: 512)\n",
    "- **temperature**: Randomness (0.0 = deterministic, 1.0 = creative)\n",
    "- **top_p**: Nucleus sampling (0.0-1.0, default: 1.0)\n",
    "- **top_k**: Top-k sampling (default: 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to invoke the model\n",
    "def invoke_model(prompt, max_tokens=512, temperature=0.7, top_p=1.0):\n",
    "    \"\"\"\n",
    "    Invoke the Mistral model with a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User prompt string\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0-1.0)\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = predictor.predict(payload)\n",
    "        \n",
    "        # Extract the generated text\n",
    "        if isinstance(response, dict) and 'choices' in response:\n",
    "            return response['choices'][0]['message']['content']\n",
    "        elif isinstance(response, list) and len(response) > 0:\n",
    "            return response[0]['generated_text']\n",
    "        else:\n",
    "            return str(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error invoking model: {e}\"\n",
    "\n",
    "# Test basic invocation\n",
    "print(\"üß™ Testing basic model invocation...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_prompt = \"What is Amazon SageMaker? Explain in 2-3 sentences.\"\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "\n",
    "response = invoke_model(test_prompt, max_tokens=200, temperature=0.7)\n",
    "print(f\"Response:\\n{response}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Use Case 1 - Text Summarization\n",
    "\n",
    "Mistral Small excels at summarizing long documents into concise summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Summarize a technical document\n",
    "long_text = \"\"\"\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists \n",
    "and developers can quickly and easily build and train machine learning models, and then directly \n",
    "deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring \n",
    "notebook instance for easy access to your data sources for exploration and analysis, so you don't \n",
    "have to manage servers. It also provides common machine learning algorithms that are optimized to \n",
    "run efficiently against extremely large data in a distributed environment. With native support for \n",
    "bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options \n",
    "that adjust to your specific workflows. Deploy a model into a secure and scalable environment by \n",
    "launching it with a single click from the SageMaker console.\n",
    "\"\"\"\n",
    "\n",
    "summarization_prompt = f\"\"\"\n",
    "Summarize the following text in 2-3 bullet points:\n",
    "\n",
    "{long_text}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Use Case: Text Summarization\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original text length: {len(long_text)} characters\\n\")\n",
    "\n",
    "summary = invoke_model(summarization_prompt, max_tokens=300, temperature=0.5)\n",
    "print(f\"Summary:\\n{summary}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Use Case 2 - Code Generation\n",
    "\n",
    "Generate code snippets in various programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate Python code\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function that:\n",
    "1. Takes a list of numbers as input\n",
    "2. Removes duplicates\n",
    "3. Sorts the list in descending order\n",
    "4. Returns the top 5 numbers\n",
    "\n",
    "Include docstring and type hints.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üíª Use Case: Code Generation\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Request: {code_prompt.strip()}\\n\")\n",
    "\n",
    "code_response = invoke_model(code_prompt, max_tokens=500, temperature=0.3)\n",
    "print(f\"Generated Code:\\n{code_response}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Use Case 3 - Question Answering with Context\n",
    "\n",
    "Answer questions based on provided context (RAG-style use case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Q&A with context\n",
    "context = \"\"\"\n",
    "Singapore is a sovereign city-state and island country in Southeast Asia. It lies off the southern \n",
    "tip of the Malay Peninsula and is separated from Malaysia by the Straits of Johor to the north. \n",
    "The country is highly urbanized with very little primary rainforest remaining. Singapore's territory \n",
    "consists of one main island along with 62 other islets. Since independence, extensive land reclamation \n",
    "has increased its total size by 25%. The country has a tropical rainforest climate with no distinctive \n",
    "seasons. Its GDP per capita is one of the highest in the world.\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is Singapore's climate like and how has its land area changed?\"\n",
    "\n",
    "qa_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question based only on the context provided above. Be concise.\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ùì Use Case: Question Answering with Context\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "answer = invoke_model(qa_prompt, max_tokens=300, temperature=0.3)\n",
    "print(f\"Answer:\\n{answer}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Use Case 4 - Sentiment Analysis\n",
    "\n",
    "Analyze sentiment and extract insights from customer feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze customer reviews\n",
    "reviews = [\n",
    "    \"The product arrived quickly and works perfectly. Very satisfied!\",\n",
    "    \"Disappointed with the quality. Not worth the price.\",\n",
    "    \"Good product but customer service could be better.\"\n",
    "]\n",
    "\n",
    "sentiment_prompt = f\"\"\"\n",
    "Analyze the sentiment of these customer reviews and provide:\n",
    "1. Overall sentiment (Positive/Negative/Mixed)\n",
    "2. Key themes\n",
    "3. Actionable insights\n",
    "\n",
    "Reviews:\n",
    "{chr(10).join([f'{i+1}. {r}' for i, r in enumerate(reviews)])}\n",
    "\n",
    "Analysis:\n",
    "\"\"\"\n",
    "\n",
    "print(\"üòä Use Case: Sentiment Analysis\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Analyzing customer reviews...\\n\")\n",
    "\n",
    "sentiment_analysis = invoke_model(sentiment_prompt, max_tokens=400, temperature=0.5)\n",
    "print(f\"Analysis:\\n{sentiment_analysis}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Use Case 5 - Multi-turn Conversation\n",
    "\n",
    "Maintain context across multiple conversation turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-turn conversation\n",
    "def multi_turn_conversation(conversation_history):\n",
    "    \"\"\"\n",
    "    Handle multi-turn conversations with context.\n",
    "    \n",
    "    Args:\n",
    "        conversation_history: List of message dicts with 'role' and 'content'\n",
    "    \n",
    "    Returns:\n",
    "        Assistant's response\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"messages\": conversation_history,\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = predictor.predict(payload)\n",
    "        if isinstance(response, dict) and 'choices' in response:\n",
    "            return response['choices'][0]['message']['content']\n",
    "        return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Simulate a conversation\n",
    "print(\"üí¨ Use Case: Multi-turn Conversation\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What are the main AWS compute services?\"},\n",
    "]\n",
    "\n",
    "print(\"User: What are the main AWS compute services?\")\n",
    "response1 = multi_turn_conversation(conversation)\n",
    "print(f\"\\nAssistant: {response1}\\n\")\n",
    "\n",
    "# Add to conversation history\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response1})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Which one is best for machine learning workloads?\"})\n",
    "\n",
    "print(\"\\nUser: Which one is best for machine learning workloads?\")\n",
    "response2 = multi_turn_conversation(conversation)\n",
    "print(f\"\\nAssistant: {response2}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Use Case 6 - Structured Data Extraction\n",
    "\n",
    "Extract structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract structured data\n",
    "unstructured_text = \"\"\"\n",
    "John Smith works as a Senior Data Scientist at TechCorp in Singapore. \n",
    "He can be reached at john.smith@techcorp.com or +65 9123 4567. \n",
    "He specializes in machine learning and has 8 years of experience.\n",
    "\"\"\"\n",
    "\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract the following information from the text and format as JSON:\n",
    "- name\n",
    "- job_title\n",
    "- company\n",
    "- location\n",
    "- email\n",
    "- phone\n",
    "- specialization\n",
    "- years_of_experience\n",
    "\n",
    "Text:\n",
    "{unstructured_text}\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä Use Case: Structured Data Extraction\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Input text:\\n{unstructured_text}\\n\")\n",
    "\n",
    "extracted_data = invoke_model(extraction_prompt, max_tokens=300, temperature=0.1)\n",
    "print(f\"Extracted JSON:\\n{extracted_data}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Advanced - Batch Processing\n",
    "\n",
    "Process multiple prompts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch processing\n",
    "import time\n",
    "\n",
    "def batch_process(prompts, max_tokens=200, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Process multiple prompts in batch.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompt strings\n",
    "        max_tokens: Maximum tokens per response\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        List of responses\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"Processing {i}/{len(prompts)}...\", end=\" \")\n",
    "        start = time.time()\n",
    "        \n",
    "        response = invoke_model(prompt, max_tokens, temperature)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Done ({elapsed:.2f}s)\")\n",
    "        \n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"time\": elapsed\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test batch processing\n",
    "print(\"‚ö° Advanced: Batch Processing\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "batch_prompts = [\n",
    "    \"What is machine learning in one sentence?\",\n",
    "    \"What is deep learning in one sentence?\",\n",
    "    \"What is natural language processing in one sentence?\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(batch_prompts)} prompts...\\n\")\n",
    "batch_results = batch_process(batch_prompts, max_tokens=100, temperature=0.5)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    print(f\"\\n{i}. {result['prompt']}\")\n",
    "    print(f\"   Response: {result['response']}\")\n",
    "    print(f\"   Time: {result['time']:.2f}s\")\n",
    "\n",
    "avg_time = sum(r['time'] for r in batch_results) / len(batch_results)\n",
    "print(f\"\\nAverage response time: {avg_time:.2f}s\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Performance Monitoring\n",
    "\n",
    "Monitor endpoint performance and costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get endpoint information\n",
    "import boto3\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(\"üìä Endpoint Performance Monitoring\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Get endpoint details\n",
    "    endpoint_desc = sagemaker_client.describe_endpoint(\n",
    "        EndpointName=predictor.endpoint_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Endpoint Name: {endpoint_desc['EndpointName']}\")\n",
    "    print(f\"Status: {endpoint_desc['EndpointStatus']}\")\n",
    "    print(f\"Creation Time: {endpoint_desc['CreationTime']}\")\n",
    "    print(f\"Last Modified: {endpoint_desc['LastModifiedTime']}\")\n",
    "    \n",
    "    # Get endpoint config\n",
    "    config_desc = sagemaker_client.describe_endpoint_config(\n",
    "        EndpointConfigName=endpoint_desc['EndpointConfigName']\n",
    "    )\n",
    "    \n",
    "    variant = config_desc['ProductionVariants'][0]\n",
    "    print(f\"\\nInstance Type: {variant['InstanceType']}\")\n",
    "    print(f\"Instance Count: {variant['InitialInstanceCount']}\")\n",
    "    \n",
    "    # Calculate estimated cost\n",
    "    instance_costs = {\n",
    "        'ml.g5.2xlarge': 1.21,\n",
    "        'ml.g5.4xlarge': 1.94,\n",
    "        'ml.g5.12xlarge': 7.09\n",
    "    }\n",
    "    \n",
    "    hourly_cost = instance_costs.get(variant['InstanceType'], 0)\n",
    "    daily_cost = hourly_cost * 24\n",
    "    monthly_cost = daily_cost * 30\n",
    "    \n",
    "    print(f\"\\nEstimated Costs (USD):\")\n",
    "    print(f\"  Hourly: ${hourly_cost:.2f}\")\n",
    "    print(f\"  Daily: ${daily_cost:.2f}\")\n",
    "    print(f\"  Monthly: ${monthly_cost:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüí° Tip: Delete the endpoint when not in use to save costs!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting endpoint info: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Best Practices and Tips\n",
    "\n",
    "Key recommendations for production use:\n",
    "\n",
    "### 1. Temperature Settings\n",
    "- **0.0-0.3**: Factual, deterministic tasks (Q&A, extraction)\n",
    "- **0.4-0.7**: Balanced creativity (summarization, general chat)\n",
    "- **0.8-1.0**: Creative tasks (brainstorming, storytelling)\n",
    "\n",
    "### 2. Token Management\n",
    "- Set appropriate `max_tokens` to control costs\n",
    "- Monitor token usage for billing\n",
    "- Use shorter prompts when possible\n",
    "\n",
    "### 3. Error Handling\n",
    "- Always wrap predictions in try-except blocks\n",
    "- Implement retry logic for transient failures\n",
    "- Log errors for debugging\n",
    "\n",
    "### 4. Cost Optimization\n",
    "- Delete endpoints when not in use\n",
    "- Use auto-scaling for variable workloads\n",
    "- Consider SageMaker Serverless Inference for sporadic traffic\n",
    "\n",
    "### 5. Performance\n",
    "- Batch similar requests together\n",
    "- Use appropriate instance types for your workload\n",
    "- Monitor CloudWatch metrics\n",
    "\n",
    "### 6. Security\n",
    "- Use VPC endpoints for private connectivity\n",
    "- Enable encryption at rest and in transit\n",
    "- Implement IAM policies for access control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Cleanup - Delete the Endpoint\n",
    "\n",
    "**IMPORTANT**: Always delete your endpoint when finished to avoid ongoing charges.\n",
    "\n",
    "This will:\n",
    "1. Delete the endpoint\n",
    "2. Delete the endpoint configuration\n",
    "3. Optionally delete the model\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This action cannot be undone. You'll need to redeploy to use the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup function\n",
    "def cleanup_endpoint(delete_model=False):\n",
    "    \"\"\"\n",
    "    Delete the SageMaker endpoint and associated resources.\n",
    "    \n",
    "    Args:\n",
    "        delete_model: If True, also delete the model\n",
    "    \"\"\"\n",
    "    print(\"üßπ Cleaning up resources...\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Delete endpoint\n",
    "        print(f\"Deleting endpoint: {predictor.endpoint_name}\")\n",
    "        predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "        print(\"‚úÖ Endpoint deleted successfully\")\n",
    "        \n",
    "        # Optionally delete model\n",
    "        if delete_model:\n",
    "            print(f\"\\nDeleting model...\")\n",
    "            predictor.delete_model()\n",
    "            print(\"‚úÖ Model deleted successfully\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"\\n‚úÖ Cleanup complete! You are no longer being charged.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during cleanup: {e}\")\n",
    "        print(\"\\nYou may need to manually delete resources from the SageMaker console.\")\n",
    "\n",
    "# Uncomment the line below to delete the endpoint\n",
    "# cleanup_endpoint(delete_model=True)\n",
    "\n",
    "print(\"‚ö†Ô∏è  Endpoint is still running!\")\n",
    "print(\"\\nTo delete the endpoint and stop charges, uncomment and run:\")\n",
    "print(\"cleanup_endpoint(delete_model=True)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

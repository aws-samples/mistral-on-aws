{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral OCR Workshop\n",
    "\n",
    "This notebook demonstrates Optical Character Recognition using Mistral models on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Mistral OCR\n",
    "\n",
    "**Mistral OCR** is a specialized optical character recognition model designed for extracting text, images, tables, and mathematical expressions from documents. Unlike traditional OCR models that only extract text, Mistral OCR comprehends each element of documents and returns ordered interleaved text and images in markdown format, making it ideal for multimodal document understanding and RAG systems.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **State-of-the-Art Performance**: Achieves 94.89% overall accuracy on benchmarks, outperforming Google Document AI (83.42%), Azure OCR (89.52%), Gemini models (88-90%), and GPT-4o (89.77%)\n",
    "- **Complex Document Understanding**: Excels at processing interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting\n",
    "- **Natively Multilingual**: Parses, understands, and transcribes thousands of scripts, fonts, and languages across all continents with 99%+ accuracy\n",
    "- **Multi-format Support**: Process images (JPG, PNG, WebP, etc.) and multi-page PDF documents\n",
    "- **Doc-as-Prompt & Structured Output**: Use documents as prompts and extract information into structured formats like JSON for downstream function calls and agent building\n",
    "- **RAG-Ready**: Ideal model for use in Retrieval-Augmented Generation (RAG) systems with multimodal documents such as slides or complex PDFs\n",
    "- **Production-Ready**: Deploy on Amazon SageMaker, la Plateforme, or self-host for organizations with stringent data privacy requirements\n",
    "\n",
    "### Performance Highlights\n",
    "\n",
    "Mistral OCR excels across multiple dimensions:\n",
    "\n",
    "| Category | Mistral OCR 2503 | Best Competitor |\n",
    "|----------|------------------|-----------------|\n",
    "| Overall Accuracy | 94.89% | 90.23% (Gemini-1.5-Flash) |\n",
    "| Mathematical Expressions | 94.29% | 89.11% (Gemini-1.5-Flash) |\n",
    "| Scanned Documents | 98.96% | 96.15% (Gemini-1.5-Pro) |\n",
    "| Tables | 96.12% | 91.70% (GPT-4o) |\n",
    "| Multilingual | 99.02% | 97.31% (Azure OCR) |\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **Document Digitization**: Convert scanned documents, receipts, forms, and historical archives into searchable, AI-ready text\n",
    "2. **Scientific Research**: Extract text, equations, tables, charts, and figures from scientific papers and journals\n",
    "3. **Handwriting Recognition**: Process handwritten notes, whiteboard images, and forms\n",
    "4. **Multimodal RAG Systems**: Build intelligent document understanding pipelines by combining Mistral OCR with LLMs like Mistral Small for analysis and summarization\n",
    "5. **Multi-language Processing**: Handle documents in diverse linguistic backgrounds, from global organizations to hyperlocal businesses\n",
    "6. **Structured Data Extraction**: Extract specific information from documents and format it into JSON for automated workflows and agent systems\n",
    "7. **Cultural Heritage Preservation**: Digitize historical documents and artifacts for preservation and accessibility\n",
    "\n",
    "This workshop will demonstrate how to deploy and use Mistral OCR on Amazon SageMaker for various document processing tasks, leveraging its industry-leading accuracy, speed, and versatility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Mistral OCR Response Format\n",
    "\n",
    "Mistral OCR returns document content as **Markdown with interleaved text and images**, preserving the document structure and layout hierarchy. This format is optimized for downstream processing by LLMs and provides precise positioning information through bounding boxes.\n",
    "\n",
    "### Supported Document Elements\n",
    "\n",
    "Mistral OCR can extract and recognize a wide variety of document elements:\n",
    "\n",
    "- **Standard typed text** - Regular printed text in any font\n",
    "- **Multilingual text** - Mixed scripts (e.g., Asian and Roman characters)\n",
    "- **Mathematical expressions** - Formulas, equations, and LaTeX notation\n",
    "- **Handwriting** - Handwritten notes and annotations\n",
    "- **Strikethrough text** - Text with strikethrough formatting\n",
    "- **Diverse layout formats** - Complex page layouts and structures\n",
    "- **Multi-column tables** - Tables with multiple columns and complex structures\n",
    "- **Text with specific bounding boxes** - Precise spatial positioning\n",
    "- **Text on colored backgrounds** - Text overlaid on colored regions\n",
    "- **Form elements** - Checkboxes and circle selection fields\n",
    "\n",
    "### Response Structure\n",
    "\n",
    "Responses are returned in **Markdown format** with:\n",
    "- Structural elements like pipes (`|`), LaTeX, and tables\n",
    "- Layout cues that help LLMs understand document hierarchy\n",
    "- Image placeholders embedded in the text\n",
    "\n",
    "#### Image Representation\n",
    "\n",
    "Images within documents are represented as Markdown image syntax:\n",
    "```markdown\n",
    "![img-0.jpeg](img-0.jpeg)\n",
    "```\n",
    "\n",
    "The image ID (e.g., `img-0.jpeg`) maps to the `pages[n].images` array, which contains:\n",
    "- **Bounding box coordinates** (`top_left_x`, `top_left_y`, `bottom_right_x`, `bottom_right_y`)\n",
    "- **Base64-encoded payload** (optional)\n",
    "- **Image annotations** (if applicable)\n",
    "\n",
    "#### Example Response Structure\n",
    "\n",
    "```python\n",
    "{\n",
    "  'index': 13,\n",
    "  'markdown': \"![img-13.jpeg](img-13.jpeg)\\n\\nFigure 11: Examples of model responses...\",\n",
    "  'images': [\n",
    "    {\n",
    "      'id': 'img-13.jpeg',\n",
    "      'top_left_x': 294,\n",
    "      'top_left_y': 512,\n",
    "      'bottom_right_x': 1404,\n",
    "      'bottom_right_y': 1568,\n",
    "      'image_base64': None,\n",
    "      'image_annotation': None\n",
    "    }\n",
    "  ],\n",
    "  'dimensions': {\n",
    "    'dpi': 200,\n",
    "    'height': 2200,\n",
    "    'width': 1700\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This structured format enables precise extraction, spatial understanding, and seamless integration with downstream AI pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries for OCR processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Setup\n",
    "\n",
    "This workshop demonstrates Mistral OCR using an endpoint deployed in your AWS account.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You should have:\n",
    "1. **Deployed the Mistral OCR model** to a SageMaker endpoint in your AWS account\n",
    "2. **Appropriate IAM permissions** to invoke the SageMaker endpoint\n",
    "3. **AWS credentials configured** in your environment\n",
    "\n",
    "### Endpoint Information\n",
    "\n",
    "The endpoint ARN for this demo is:\n",
    "```\n",
    "arn:aws:sagemaker:us-west-2:314146324612:endpoint/mistral-ocr-endpoint\n",
    "```\n",
    "\n",
    "Let's configure the connection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Configuration\n",
    "\n",
    "Configure the endpoint details for your deployed Mistral OCR model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint Configuration\n",
    "# OCR Endpoint: us-west-2 (your deployed SageMaker endpoint)\n",
    "# Bedrock (Pixtral Large): us-west-2 (configured separately in Bedrock cells)\n",
    "\n",
    "ENDPOINT_NAME = \"mistral-ocr-endpoint\"  # Your deployed endpoint name\n",
    "REGION = \"us-west-2\"  # AWS region where OCR endpoint is deployed\n",
    "\n",
    "print(\"Endpoint Configuration:\")\n",
    "print(f\"  OCR Endpoint: {ENDPOINT_NAME}\")\n",
    "print(f\"  OCR Region: {REGION}\")\n",
    "print(f\"  Bedrock Region: us-west-2 (for Pixtral Large)\")\n",
    "print(\"\\nâœ… Configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create SageMaker Runtime Client\n",
    "\n",
    "Now we'll create a SageMaker Runtime client to invoke your OCR endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker Runtime client using your AWS credentials\n",
    "# This will use the credentials configured in your environment\n",
    "sagemaker_client = boto3.client('sagemaker-runtime', region_name=REGION)\n",
    "\n",
    "print(\"âœ… SageMaker Runtime client created successfully!\")\n",
    "print(f\"   Region: {REGION}\")\n",
    "print(f\"   Ready to invoke endpoint: {ENDPOINT_NAME}\")\n",
    "\n",
    "# Store endpoint name for easy access throughout the notebook\n",
    "MISTRAL_OCR_ENDPOINT_NAME = ENDPOINT_NAME\n",
    "\n",
    "print(\"\\nðŸŽ‰ Setup complete! You're ready to use Mistral OCR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "\n",
    "Let's verify the client is properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the client configuration\n",
    "print(\"Client Configuration:\")\n",
    "print(f\"  Service: sagemaker-runtime\")\n",
    "print(f\"  Region: {sagemaker_client.meta.region_name}\")\n",
    "print(f\"  Endpoint: {MISTRAL_OCR_ENDPOINT_NAME}\")\n",
    "print(\"\\nâœ… Client verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These helper functions support image processing, model invocation, and post-response processing for the Mistral OCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_local_file_base64(file_path: str, file_type: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Encode a local file (image or PDF) to base64 string.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the local file\n",
    "        file_type: Type of file ('image' or 'pdf'). If None, inferred from extension.\n",
    "    \n",
    "    Returns:\n",
    "        Base64 encoded string of the file\n",
    "    \"\"\"\n",
    "    if file_type is None:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            file_type = \"pdf\"\n",
    "        elif ext in (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\"):\n",
    "            file_type = \"image\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type from extension: {ext}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            encoded_data = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "            return encoded_data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to encode {file_type} at {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_inference(client, endpoint_name: str, payload: dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke the SageMaker endpoint for OCR inference.\n",
    "    \n",
    "    Args:\n",
    "        client: SageMaker runtime client\n",
    "        endpoint_name: Name of the deployed endpoint\n",
    "        payload: JSON payload containing the image data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing parsed OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inference_out = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        inference_resp_str = inference_out[\"Body\"].read().decode(\"utf-8\")\n",
    "        return json.loads(inference_resp_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Replace image placeholders in markdown with base64-encoded images.\n",
    "    \n",
    "    Args:\n",
    "        markdown_str: Markdown string with image placeholders\n",
    "        images_dict: Dictionary mapping image names to base64 strings\n",
    "    \n",
    "    Returns:\n",
    "        Markdown string with embedded base64 images\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(\n",
    "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
    "        )\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: dict) -> str:\n",
    "    \"\"\"\n",
    "    Combine OCR text and images into a single markdown document.\n",
    "    \n",
    "    Args:\n",
    "        ocr_response: Response dictionary from OCR model\n",
    "    \n",
    "    Returns:\n",
    "        Combined markdown string with embedded images\n",
    "    \"\"\"\n",
    "    markdowns = []\n",
    "    for page in ocr_response[\"pages\"]:\n",
    "        image_data = {img[\"id\"]: img[\"image_base64\"] for img in page.get(\"images\", [])}\n",
    "        markdown_with_images = replace_images_in_markdown(page[\"markdown\"], image_data)\n",
    "        markdowns.append(markdown_with_images)\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Mistral OCR\n",
    "\n",
    "Now let's use the Mistral OCR model to extract text from an image document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Access and Deployment  \n",
    "\n",
    "**Important Notice:**                                                                                                                                                              \n",
    "The Mistral OCR model is available from AWS Marketplace via private offer. If you want to trial this model, please contact your account manager and provide your AWS account ID to whitelist for this model access.                                                                                                                                                   Then, please follow this link to deploy the Mistral OCR model on a SageMaker endpoint:                                                                                            [Mistral OCR SageMaker Deployment Guide](https://github.com/aws-samples/mistral-on-aws/blob/main/Mistral%20OCR/Mistral-OCR-SageMaker-Deployment-example.ipynb)     \n",
    "\n",
    "\n",
    "**For This Demo:**                                                                                                                                                     \n",
    "This notebook uses the Mistral OCR endpoint deployed in your AWS account (us-west-2 region). The endpoint is configured in the setup cells above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Document Processing Examples\n",
    "\n",
    "The following sections demonstrate Mistral OCR's capabilities across various document types. Each example showcases how the model handles different challenges in document understanding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Whiteboard OCR\n",
    "\n",
    "### Use Case\n",
    "Extract text from whiteboard images captured during meetings, brainstorming sessions, or classroom lectures.\n",
    "\n",
    "### Why This Matters\n",
    "Whiteboards often contain:\n",
    "- Handwritten text in various styles\n",
    "- Diagrams and annotations\n",
    "- Mixed content (text, arrows, shapes)\n",
    "- Challenging lighting and perspective angles\n",
    "\n",
    "### What This Code Does\n",
    "1. **Encodes** the whiteboard image to base64 format\n",
    "2. **Sends** the image to Mistral OCR endpoint with proper MIME type\n",
    "3. **Extracts** all text content while preserving structure\n",
    "4. **Displays** the results in readable markdown format\n",
    "\n",
    "**Reference**: [Mistral OCR Documentation - Handwriting Recognition](https://docs.mistral.ai/capabilities/vision/#handwriting-recognition)\n",
    "\n",
    "\n",
    "<img src=\"images/whiteboarding.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process whiteboard image\n",
    "whiteboard_b64 = encode_local_file_base64(file_path=\"images/whiteboarding.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "whiteboard_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/png;base64,{whiteboard_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "whiteboard_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=whiteboard_payload\n",
    ")\n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Whiteboard Content:\")\n",
    "display(Markdown(get_combined_markdown(whiteboard_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Low-Resolution Handwriting OCR\n",
    "\n",
    "### Use Case\n",
    "Process handwritten notes from mobile phone photos, scanned documents, or low-quality images.\n",
    "\n",
    "### Why This Matters\n",
    "Real-world handwriting scenarios often involve:\n",
    "- Low image resolution from quick phone captures\n",
    "- Varying handwriting styles and legibility\n",
    "- Compression artifacts from image processing\n",
    "- Poor lighting conditions\n",
    "\n",
    "### What This Code Does\n",
    "1. **Loads** a low-resolution handwritten image\n",
    "2. **Processes** the image despite quality limitations\n",
    "3. **Extracts** handwritten text with high accuracy\n",
    "4. **Returns** structured markdown output\n",
    "\n",
    "This demonstrates Mistral OCR's robustness in handling challenging image quality while maintaining accuracy.\n",
    "\n",
    "**Reference**: [Mistral OCR Documentation - Document Digitization](https://docs.mistral.ai/capabilities/vision/#document-digitization)\n",
    "\n",
    "<img src=\"images/handwriting_low_res_2_resize.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process low-resolution handwriting image\n",
    "handwriting_b64 = encode_local_file_base64(file_path=\"images/handwriting_low_res_2_resize.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "handwriting_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{handwriting_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "handwriting_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=handwriting_payload\n",
    ")\n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Low-Resolution Handwriting Content:\")\n",
    "display(Markdown(get_combined_markdown(handwriting_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Invoice Processing\n",
    "\n",
    "### Use Case\n",
    "Automate invoice data extraction for accounts payable, expense management, and financial auditing.\n",
    "\n",
    "### Why This Matters\n",
    "Invoice processing is critical for:\n",
    "- **Accounts Payable Automation**: Reduce manual data entry by 90%+\n",
    "- **Compliance & Auditing**: Maintain accurate financial records\n",
    "- **Fraud Detection**: Identify duplicate or suspicious invoices\n",
    "- **Payment Processing**: Extract vendor details, amounts, and due dates\n",
    "\n",
    "### What This Code Does\n",
    "1. **Encodes** the invoice image to base64\n",
    "2. **Extracts** structured data including:\n",
    "   - Vendor name and contact information\n",
    "   - Invoice number and date\n",
    "   - Line items with descriptions and amounts\n",
    "   - Subtotals, taxes, and total amounts\n",
    "3. **Preserves** table structures and formatting\n",
    "4. **Outputs** in markdown format for easy downstream processing\n",
    "\n",
    "The extracted data can be fed into accounting systems, RAG pipelines, or LLMs for further analysis.\n",
    "\n",
    "**Reference**: [Mistral OCR Documentation - Structured Data Extraction](https://docs.mistral.ai/capabilities/vision/#structured-data-extraction)\n",
    "\n",
    "<img src=\"images/invoice_1.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single invoice\n",
    "invoice_b64 = encode_local_file_base64(file_path=\"images/invoice_1.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "invoice_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{invoice_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "invoice_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=invoice_payload\n",
    ")\n",
    "\n",
    "# Display the extracted invoice content\n",
    "print(\"Extracted Invoice Content:\")\n",
    "display(Markdown(get_combined_markdown(invoice_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Official Document Processing\n",
    "\n",
    "### Use Case\n",
    "Extract information from official forms, legal documents, and government reports for insurance claims, legal proceedings, and compliance.\n",
    "\n",
    "### Why This Matters\n",
    "Official documents require:\n",
    "- **High Accuracy**: Legal and compliance implications\n",
    "- **Form Field Recognition**: Checkboxes, signatures, stamps\n",
    "- **Mixed Content Handling**: Typed text, handwritten notes, and printed forms\n",
    "- **Structured Extraction**: Preserve relationships between fields\n",
    "\n",
    "### What This Code Does\n",
    "1. **Processes** a police report with complex layout\n",
    "2. **Extracts** both printed and handwritten content\n",
    "3. **Identifies** form fields, checkboxes, and structured data\n",
    "4. **Maintains** document hierarchy and relationships\n",
    "\n",
    "### Real-World Applications\n",
    "- **Insurance Claims**: Automatically process accident reports\n",
    "- **Legal Discovery**: Extract evidence from official documents\n",
    "- **Compliance Auditing**: Verify document completeness\n",
    "- **Case Management**: Digitize paper-based records\n",
    "\n",
    "**Reference**: [Mistral OCR Documentation - Form Processing](https://docs.mistral.ai/capabilities/vision/#form-elements)\n",
    "\n",
    "<img src=\"images/national_youth_service_corps.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process national service certificate image\n",
    "national_service_cert_b64 = encode_local_file_base64(file_path=\"images/national_youth_service_corps.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "national_service_cert_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{national_service_cert_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "national_service_cert_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=national_service_cert_payload\n",
    ")\n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted National Service Cert Content:\")\n",
    "display(Markdown(get_combined_markdown(national_service_cert_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Mathematical Expression Recognition\n",
    "\n",
    "### Use Case\n",
    "Extract mathematical equations, formulas, and scientific notation from research papers, textbooks, and technical documents.\n",
    "\n",
    "### Why This Matters\n",
    "Mathematical content presents unique challenges:\n",
    "- **Complex Notation**: Fractions, integrals, summations, matrices\n",
    "- **LaTeX Formatting**: Convert visual math to machine-readable LaTeX\n",
    "- **Subscripts & Superscripts**: Preserve mathematical meaning\n",
    "- **Special Symbols**: Greek letters, operators, and mathematical symbols\n",
    "\n",
    "### What This Code Does\n",
    "1. **Recognizes** mathematical expressions in images\n",
    "2. **Converts** visual formulas to LaTeX notation\n",
    "3. **Preserves** mathematical structure and relationships\n",
    "4. **Outputs** in markdown with embedded LaTeX for rendering\n",
    "\n",
    "### Real-World Applications\n",
    "- **Academic Research**: Digitize equations from scanned papers\n",
    "- **Educational Content**: Convert textbook formulas to digital format\n",
    "- **Scientific Publishing**: Extract equations for citation and reuse\n",
    "- **STEM Documentation**: Build searchable mathematical knowledge bases\n",
    "\n",
    "Mistral OCR achieves **94.29% accuracy** on mathematical expressions, outperforming competitors by 5%+.\n",
    "\n",
    "**Reference**: [Mistral OCR Documentation - Mathematical Expressions](https://docs.mistral.ai/capabilities/vision/#mathematical-expressions)\n",
    "\n",
    "<img src=\"images/math_formula.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process mathematical formula image\n",
    "math_formula_b64 = encode_local_file_base64(file_path=\"images/math_formula.png\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "math_formula_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/png;base64,{math_formula_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "math_formula_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=math_formula_payload\n",
    ")\n",
    "\n",
    "# Display the extracted mathematical content with LaTeX\n",
    "print(\"Extracted Mathematical Formula (LaTeX):\")\n",
    "display(Markdown(get_combined_markdown(math_formula_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Part 2: Multimodal RAG: Combining OCR with LLMs\n",
    "\n",
    "Now that we've seen how Mistral OCR extracts text from various document types, let's build an **intelligent document understanding pipeline** by combining OCR with Large Language Models (LLMs).\n",
    "\n",
    "---\n",
    "\n",
    "## What is Multimodal RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** enhances LLM responses by providing relevant context. In a multimodal RAG pipeline:\n",
    "\n",
    "1. **OCR Model** (Mistral OCR) extracts structured text from documents\n",
    "2. **LLM** (Pixtral Large) analyzes the extracted text to answer questions, summarize, or extract insights\n",
    "\n",
    "This two-stage approach provides:\n",
    "- **Higher Accuracy**: OCR specializes in text extraction, LLM specializes in understanding\n",
    "- **Better Performance**: Smaller context windows for LLM processing\n",
    "- **Cost Efficiency**: Process images once with OCR, reuse text for multiple queries\n",
    "- **Flexibility**: Swap models independently based on requirements\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "Document Image â†’ Mistral OCR (us-west-2) â†’ Structured Text â†’ Pixtral Large (us-east-1) â†’ Insights\n",
    "```\n",
    "\n",
    "**Important Note on Regions:**\n",
    "- **OCR Endpoint**: us-west-2 (your deployed SageMaker endpoint)\n",
    "- **Bedrock (Pixtral Large)**: us-east-1 (only region where Pixtral Large is available)\n",
    "- These services can work together across regions without any issues\n",
    "\n",
    "Let's set up the helper functions for invoking Bedrock models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for Bedrock Runtime\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "def bedrock_converse(system_prompt: str, messages: list, endpoint_arn: str, display_usage=False):\n",
    "    \"\"\"Invoke model using Converse API\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "\n",
    "    output_content = ''.join(\n",
    "        content['text'] for content in response['output']['message']['content']\n",
    "    )\n",
    "\n",
    "    if display_usage:\n",
    "        token_usage = response['usage']\n",
    "        print(f\"\\tLatency: {response['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content\n",
    "\n",
    "def bedrock_converse_stream(system_prompt: str, messages: list, endpoint_arn: str):\n",
    "    \"\"\"Invoke model with streaming\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse_stream(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "    \n",
    "    stream = response.get('stream')\n",
    "    output_content = ''\n",
    "    \n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "            \n",
    "            if 'contentBlockDelta' in event:\n",
    "                text_chunk = event['contentBlockDelta']['delta']['text']\n",
    "                print(text_chunk, end=\"\")\n",
    "                output_content += text_chunk\n",
    "            \n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "            \n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Understanding Pipeline\n",
    "\n",
    "This function orchestrates the complete document understanding workflow:\n",
    "\n",
    "### Pipeline Steps\n",
    "\n",
    "1. **Image Encoding**: Convert local document image to base64\n",
    "2. **OCR Extraction**: Send to Mistral OCR endpoint to extract structured text\n",
    "3. **Text Processing**: Convert OCR output to clean markdown format\n",
    "4. **LLM Analysis**: Send extracted text + user query to Pixtral Large on Bedrock\n",
    "5. **Stream Response**: Display insights in real-time\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "- **Separation of Concerns**: OCR handles extraction, LLM handles reasoning\n",
    "- **Reusability**: Extract once, query multiple times\n",
    "- **Transparency**: See both raw OCR output and LLM analysis\n",
    "- **Flexibility**: Easily swap models or add processing steps\n",
    "\n",
    "Let's define the pipeline function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_understanding_pipeline(\n",
    "    image_path: str,\n",
    "    user_prompt: str,\n",
    "    ocr_endpoint: str=None,\n",
    "    llm_endpoint: str=None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Full pipeline for document understanding from image input.\n",
    "\n",
    "    Args:\n",
    "        image_path: Local path to the document image\n",
    "        user_prompt: What insights the user wants to extract\n",
    "        ocr_endpoint: SageMaker endpoint for OCR model  \n",
    "        llm_endpoint: Bedrock inference profile for Pixtral Large (e.g., \"us.mistral.pixtral-large-2502-v1:0\")\n",
    "\n",
    "    Returns:\n",
    "        Model-generated response with document insights\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Encode local file using helper\n",
    "    encoded_image = encode_local_file_base64(image_path)\n",
    "\n",
    "    # Prepare OCR payload\n",
    "    api_payload = {\n",
    "        \"model\": \"mistral-ocr-2505\",\n",
    "        \"document\": {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{encoded_image}\",  # Fixed: was image_b64\n",
    "        },\n",
    "        \"include_image_base64\": False\n",
    "    }\n",
    "\n",
    "    # Step 2: Run OCR model\n",
    "    print(\"Step 1/2: Running Mistral OCR...\")\n",
    "    ocr_result = run_inference(client=sagemaker_client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=api_payload)\n",
    "\n",
    "    # Step 3: Convert OCR output to Markdown\n",
    "    print(\"Extracting text from document...\")\n",
    "    markdown_doc = get_combined_markdown(ocr_result)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“„ EXTRACTED TEXT FROM OCR\")\n",
    "    print(\"=\"*80)\n",
    "    display(Markdown(markdown_doc))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 4: Prepare LLM messages for Bedrock\n",
    "    system_prompt = (\n",
    "        \"You are a document understanding assistant powered by Mistral AI. \"\n",
    "        \"The user will provide structured OCR content from a scanned document. \"\n",
    "        \"Use that information to generate clear, factual insights that answer the user's request. \"\n",
    "        \"Be thorough and reference specific details from the document.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": f\"{user_prompt}\\n\\n--- Document Content ---\\n{markdown_doc}\"}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Step 5: Call Bedrock Pixtral Large with streaming\n",
    "    print(\"Step 2/2: Running Pixtral Large on Bedrock for analysis...\")\n",
    "    print(\"-\"*80)\n",
    "    insights = bedrock_converse_stream(system_prompt, messages, llm_endpoint)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… Analysis complete!\")\n",
    "    \n",
    "    return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Document Summarization\n",
    "\n",
    "Let's use the pipeline to extract and summarize a document.\n",
    "\n",
    "<img src=\"images/french.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"images/french.png\"\n",
    "user_prompt = \"can you summarise this document\"\n",
    "\n",
    "# Using Pixtral Large 2502 - multimodal model with vision capabilities\n",
    "llm_endpoint = \"us.mistral.pixtral-large-2502-v1:0\"\n",
    "\n",
    "# Alternative options:\n",
    "# llm_endpoint = \"mistral.mistral-large-2407-v1:0\"  # Mistral Large 2 (text-only)\n",
    "# llm_endpoint = \"mistral.mistral-small-2402-v1:0\"  # Mistral Small (faster, cheaper)\n",
    "\n",
    "document_understanding_pipeline(image_path=image_path, user_prompt=user_prompt, llm_endpoint=llm_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 3: Advanced: Multi-Agent Document Intelligence with OCR + Strands Agents\n",
    "\n",
    "In this section, we'll go beyond simple OCR extraction to build a **sophisticated multi-agent document processing system** using **Strands Agents**. This demonstrates how to orchestrate multiple specialized AI agents that work together to analyze documents, enforce compliance rules, and generate actionable insights.\n",
    "\n",
    "---\n",
    "\n",
    "### What is Strands Agents?\n",
    "\n",
    "**Strands Agents** is a Python framework for building agentic AI applications with tool use capabilities. It provides:\n",
    "- **Agent Orchestration**: Coordinate multiple specialized agents in workflows\n",
    "- **Tool Functions**: Decorate Python functions with `@tool` to make them callable by agents\n",
    "- **Model Integration**: Works seamlessly with Amazon Bedrock models like Pixtral Large\n",
    "- **State Management**: Track execution across multi-step workflows\n",
    "\n",
    "Learn more: [https://github.com/strands-agents/strands-agents](https://github.com/strands-agents/strands-agents)\n",
    "\n",
    "### What We'll Build: Invoice & Document Compliance Workflow\n",
    "\n",
    "We'll create an **intelligent document processing pipeline** that:\n",
    "1. **Extracts** text from invoices and official documents using Mistral OCR\n",
    "2. **Classifies** document types automatically (invoice, report, form, etc.)\n",
    "3. **Analyzes** documents with domain-specific agents\n",
    "4. **Validates** against compliance rules (duplicate detection, anomaly detection)\n",
    "5. **Synthesizes** findings into an executive summary with flagged issues\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "This pattern is applicable to:\n",
    "- **Accounts Payable Automation**: Process invoices, detect duplicates, flag anomalies\n",
    "- **Insurance Claims Processing**: Analyze claim documents and police reports\n",
    "- **Legal Document Review**: Extract key information from contracts and forms\n",
    "- **Compliance Auditing**: Validate documents against regulatory requirements\n",
    "- **Research Paper Analysis**: Extract equations, citations, and key findings\n",
    "\n",
    "### Multi-Agent Architecture\n",
    "\n",
    "Our workflow uses 5 specialized agents:\n",
    "\n",
    "1. **Document Triage Agent**: Classifies document type (invoice, police report, form)\n",
    "2. **Invoice Analysis Agent**: Extracts vendor, amounts, line items, checks for issues\n",
    "3. **Report Analysis Agent**: Analyzes official documents like police reports\n",
    "4. **Compliance Agent**: Validates documents against business rules\n",
    "5. **Synthesis Agent**: Generates executive summary with prioritized actions\n",
    "\n",
    "### Why Multi-Agent vs Single-Agent?\n",
    "\n",
    "| Single Agent | Multi-Agent Workflow |\n",
    "|-------------|---------------------|\n",
    "| Generic prompts | Specialized expertise per domain |\n",
    "| Limited context window | Distributed processing |\n",
    "| Hard to maintain | Modular and scalable |\n",
    "| One-size-fits-all | Tailored analysis per document type |\n",
    "\n",
    "### Workflow Steps\n",
    "\n",
    "```\n",
    "1. OCR Extraction â†’ 2. Document Triage â†’ 3. Specialized Analysis â†’ 4. Compliance Check â†’ 5. Synthesis\n",
    "```\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Install and Import Strands Agents\n",
    "\n",
    "First, let's install Strands Agents and import the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Strands Agents (if not already installed)\n",
    "# !pip install strands-agents>=0.1.6\n",
    "\n",
    "# Import Strands components\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from strands.tools import tool\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… Strands Agents imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Document Inventory\n",
    "\n",
    "First, we'll set up our document inventory and caching system.\n",
    "\n",
    "#### What This Code Does\n",
    "\n",
    "- **Document Registry**: Creates a dictionary mapping document IDs to file paths\n",
    "- **Lazy Loading**: Documents are only processed when needed by agents\n",
    "- **OCR Caching**: Stores OCR results to avoid redundant API calls\n",
    "- **Type Classification**: Initially marked as \"unknown\" - will be classified by the triage agent\n",
    "\n",
    "#### Why Caching Matters\n",
    "\n",
    "In a multi-agent workflow, multiple agents may need to access the same document. Caching ensures:\n",
    "- **Cost Efficiency**: OCR is called only once per document\n",
    "- **Performance**: Subsequent accesses are instant\n",
    "- **Consistency**: All agents work with identical extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document metadata (file paths for processing)\n",
    "# The OCR text will be extracted dynamically when needed\n",
    "\n",
    "DOCUMENTS_TO_PROCESS = {\n",
    "    \"doc_001\": {\n",
    "        \"doc_id\": \"doc_001\",\n",
    "        \"file_path\": \"images/invoice_1.jpg\",\n",
    "        \"doc_type\": \"unknown\"  # Will be classified by triage agent\n",
    "    },\n",
    "    \"doc_002\": {\n",
    "        \"doc_id\": \"doc_002\", \n",
    "        \"file_path\": \"images/invoice_2.jpg\",\n",
    "        \"doc_type\": \"unknown\"\n",
    "    },\n",
    "    \"doc_003\": {\n",
    "        \"doc_id\": \"doc_003\",\n",
    "        \"file_path\": \"images/Invoice_3.jpg\",\n",
    "        \"doc_type\": \"unknown\"\n",
    "    },\n",
    "    \"doc_004\": {\n",
    "        \"doc_id\": \"doc_004\",\n",
    "        \"file_path\": \"images/national_youth_service_corps.jpg\",\n",
    "        \"doc_type\": \"unknown\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cache for extracted OCR results (to avoid re-processing same images)\n",
    "OCR_CACHE = {}\n",
    "\n",
    "print(f\"âœ… Document metadata created for {len(DOCUMENTS_TO_PROCESS)} documents\")\n",
    "print(\"\\nDocuments to process:\")\n",
    "for doc_id, doc in DOCUMENTS_TO_PROCESS.items():\n",
    "    print(f\"  - {doc_id}: {doc['file_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Agent Tools\n",
    "\n",
    "Now we'll define the **tools** that agents can use to interact with documents and perform analysis.\n",
    "\n",
    "#### What are Agent Tools?\n",
    "\n",
    "In Strands Agents, tools are Python functions decorated with `@tool` that agents can call during their reasoning process. Think of them as the \"hands\" of the agent - they enable agents to:\n",
    "- Access external data (OCR extraction)\n",
    "- Query databases (document listing)\n",
    "- Perform computations (duplicate detection)\n",
    "\n",
    "#### Tools We're Creating\n",
    "\n",
    "1. **`extract_document_text(doc_id)`**\n",
    "   - Calls Mistral OCR to extract text from a document\n",
    "   - Implements caching to avoid redundant API calls\n",
    "   - Returns structured JSON with document metadata and OCR text\n",
    "\n",
    "2. **`list_documents(doc_type)`**\n",
    "   - Lists all available documents, optionally filtered by type\n",
    "   - Helps agents discover what documents need processing\n",
    "\n",
    "3. **`check_duplicate_invoice(doc_id, vendor, amount, date)`**\n",
    "   - Compares invoice details against previously processed documents\n",
    "   - Detects potential duplicate submissions\n",
    "   - Returns match results with confidence indicators\n",
    "\n",
    "#### How Agents Use Tools\n",
    "\n",
    "When an agent needs information, it:\n",
    "1. Decides which tool to call based on its task\n",
    "2. Generates the appropriate parameters\n",
    "3. Receives the tool output\n",
    "4. Incorporates the results into its reasoning\n",
    "\n",
    "This is similar to how ChatGPT uses function calling, but orchestrated within a multi-agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_document_text(doc_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a document using Mistral OCR.\n",
    "    Caches results to avoid re-processing.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Document ID to extract text from\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with document details and extracted OCR text\n",
    "    \"\"\"\n",
    "    if doc_id not in DOCUMENTS_TO_PROCESS:\n",
    "        return json.dumps({\"error\": f\"Document {doc_id} not found\"})\n",
    "    \n",
    "    # Check cache first\n",
    "    if doc_id in OCR_CACHE:\n",
    "        print(f\"  [Using cached OCR for {doc_id}]\")\n",
    "        return json.dumps(OCR_CACHE[doc_id], indent=2)\n",
    "    \n",
    "    # Extract from document\n",
    "    doc_info = DOCUMENTS_TO_PROCESS[doc_id]\n",
    "    file_path = doc_info['file_path']\n",
    "    \n",
    "    print(f\"  [Running OCR on {file_path}...]\")\n",
    "    \n",
    "    try:\n",
    "        # Encode image\n",
    "        image_b64 = encode_local_file_base64(file_path=file_path)\n",
    "        \n",
    "        # Prepare OCR payload\n",
    "        ocr_payload = {\n",
    "            \"model\": \"mistral-ocr-2505\",\n",
    "            \"document\": {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": f\"data:image/jpeg;base64,{image_b64}\"\n",
    "            },\n",
    "            \"include_image_base64\": False\n",
    "        }\n",
    "        \n",
    "        # Call OCR endpoint\n",
    "        ocr_result = run_inference(client=sagemaker_client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=ocr_payload)\n",
    "        \n",
    "        # Extract markdown text\n",
    "        ocr_text = get_combined_markdown(ocr_result)\n",
    "        \n",
    "        # Cache result\n",
    "        result = {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"file_path\": file_path,\n",
    "            \"ocr_text\": ocr_text,\n",
    "            \"doc_type\": doc_info['doc_type']\n",
    "        }\n",
    "        OCR_CACHE[doc_id] = result\n",
    "        \n",
    "        print(f\"  [OCR complete: {len(ocr_text)} characters extracted]\")\n",
    "        return json.dumps(result, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"OCR extraction failed: {str(e)}\"})\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_documents(doc_type: str = None) -> str:\n",
    "    \"\"\"\n",
    "    List all documents available for processing.\n",
    "    \n",
    "    Args:\n",
    "        doc_type: Filter by document type (optional)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with list of documents\n",
    "    \"\"\"\n",
    "    docs = list(DOCUMENTS_TO_PROCESS.values())\n",
    "    \n",
    "    if doc_type and doc_type != \"unknown\":\n",
    "        docs = [d for d in docs if d['doc_type'] == doc_type]\n",
    "    \n",
    "    result = [{\"doc_id\": d['doc_id'], \"file_path\": d['file_path'], \"doc_type\": d['doc_type']}\n",
    "              for d in docs]\n",
    "    \n",
    "    return json.dumps(result, indent=2)\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_duplicate_invoice(doc_id: str, vendor: str, amount: float, date: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if an invoice might be a duplicate by comparing with other processed invoices.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Current document ID (to exclude from comparison)\n",
    "        vendor: Vendor name\n",
    "        amount: Invoice amount\n",
    "        date: Invoice date\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with duplicate check results\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    # Check all cached documents\n",
    "    for cached_doc_id, cached_doc in OCR_CACHE.items():\n",
    "        if cached_doc_id == doc_id:\n",
    "            continue  # Skip self\n",
    "            \n",
    "        # Simple heuristic: check if vendor and amount appear in OCR text\n",
    "        ocr_text = cached_doc.get('ocr_text', '').lower()\n",
    "        \n",
    "        if (vendor.lower() in ocr_text and \n",
    "            str(amount) in ocr_text and\n",
    "            date in ocr_text):\n",
    "            matches.append({\n",
    "                \"doc_id\": cached_doc_id,\n",
    "                \"file_path\": cached_doc.get('file_path'),\n",
    "                \"match_reason\": \"Vendor, amount, and date found in document\"\n",
    "            })\n",
    "    \n",
    "    result = {\n",
    "        \"is_duplicate\": len(matches) > 0,\n",
    "        \"match_count\": len(matches),\n",
    "        \"matches\": matches,\n",
    "        \"note\": \"This is a heuristic check based on OCR text content\"\n",
    "    }\n",
    "    \n",
    "    return json.dumps(result, indent=2)\n",
    "\n",
    "\n",
    "print(\"âœ… Tool functions defined:\")\n",
    "print(\"  - extract_document_text(doc_id) - Calls OCR API dynamically\")\n",
    "print(\"  - list_documents(doc_type)\")\n",
    "print(\"  - check_duplicate_invoice(doc_id, vendor, amount, date)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Specialized Agents\n",
    "\n",
    "Now we'll create our team of specialized AI agents, each with domain expertise and specific tools.\n",
    "\n",
    "#### Agent Architecture\n",
    "\n",
    "Each agent is configured with:\n",
    "- **System Prompt**: Defines the agent's role, expertise, and instructions\n",
    "- **Tools**: Specific functions the agent can call\n",
    "- **Model**: Pixtral Large on Bedrock for reasoning and tool orchestration\n",
    "\n",
    "#### Our Agent Team\n",
    "\n",
    "**1. Triage Agent** ðŸ·ï¸\n",
    "- **Role**: Document classification specialist\n",
    "- **Tools**: `extract_document_text`\n",
    "- **Task**: Classify documents as invoice, police_report, contract, form, or other\n",
    "\n",
    "**2. Invoice Analyst Agent** ðŸ’°\n",
    "- **Role**: Invoice processing expert\n",
    "- **Tools**: `extract_document_text`, `check_duplicate_invoice`\n",
    "- **Task**: Extract vendor details, amounts, dates; validate structure; detect anomalies and duplicates\n",
    "\n",
    "**3. Report Analyst Agent** ðŸ“‹\n",
    "- **Role**: Official document specialist\n",
    "- **Tools**: `extract_document_text`\n",
    "- **Task**: Extract incident details, parties involved, financial impacts from reports\n",
    "\n",
    "**4. Compliance Agent** ðŸ›¡ï¸\n",
    "- **Role**: Compliance validation specialist\n",
    "- **Tools**: `list_documents`, `extract_document_text`, `check_duplicate_invoice`\n",
    "- **Task**: Check for duplicates, missing fields, threshold violations; assign risk levels\n",
    "\n",
    "**5. Synthesis Agent** ðŸ“Š\n",
    "- **Role**: Executive summary specialist\n",
    "- **Tools**: None (works with aggregated data from other agents)\n",
    "- **Task**: Create executive summary with prioritized findings and recommended actions\n",
    "\n",
    "#### Why Specialized Agents?\n",
    "\n",
    "- **Domain Expertise**: Each agent has tailored prompts for specific document types\n",
    "- **Tool Access Control**: Agents only have tools relevant to their role\n",
    "- **Scalability**: Easy to add new agent types (e.g., contract analyst, receipt processor)\n",
    "- **Maintainability**: Update one agent without affecting others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock model for agents\n",
    "# CRITICAL: us.* model IDs are ONLY available in US regions (us-east-1, us-west-2)\n",
    "# Your AWS profile is set to ap-southeast-1, so we need to explicitly override it\n",
    "BEDROCK_REGION = \"us-west-2\"  # Must be US region for us.* models\n",
    "\n",
    "# Create explicit boto3 session to override your default AWS profile region\n",
    "import boto3\n",
    "bedrock_boto_session = boto3.Session(region_name=BEDROCK_REGION)\n",
    "\n",
    "# Initialize BedrockModel with explicit session to force correct region\n",
    "bedrock_model_agents = BedrockModel(\n",
    "    model_id=\"us.mistral.pixtral-large-2502-v1:0\",\n",
    "    region=BEDROCK_REGION,  # Explicitly set to us-west-2\n",
    "    streaming=False,\n",
    "    boto_session=bedrock_boto_session  # This overrides your AWS profile!\n",
    ")\n",
    "\n",
    "print(f\"âœ… Bedrock model configured for agents:\")\n",
    "print(f\"   Model: us.mistral.pixtral-large-2502-v1:0\")\n",
    "print(f\"   Bedrock Region: {BEDROCK_REGION}\")\n",
    "print(f\"   Boto Session Region: {bedrock_boto_session.region_name}\")\n",
    "print(f\"   OCR Endpoint Region: {REGION} (us-west-2)\")\n",
    "print(f\"   âœ“ Overriding your AWS profile default (ap-southeast-1)\\n\")\n",
    "\n",
    "def create_triage_agent():\n",
    "    \"\"\"Agent that classifies document types\"\"\"\n",
    "    system_prompt = \"\"\"You are a document classification specialist. \n",
    "Analyze document content and classify it as: invoice, police_report, contract, form, or other.\n",
    "Provide brief reasoning for your classification.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[extract_document_text], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_invoice_analyst_agent():\n",
    "    \"\"\"Agent specialized in invoice analysis\"\"\"\n",
    "    system_prompt = \"\"\"You are an invoice analysis specialist. Your role:\n",
    "1. Extract key information (vendor, invoice number, amounts, dates)\n",
    "2. Validate invoice structure and completeness\n",
    "3. Check for anomalies (unusual amounts, missing fields, calculation errors)\n",
    "4. Use check_duplicate_invoice to detect potential duplicates\n",
    "5. Flag potential issues for review\n",
    "\n",
    "Be thorough and flag anything suspicious.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, \n",
    "                 tools=[extract_document_text, check_duplicate_invoice], \n",
    "                 system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_report_analyst_agent():\n",
    "    \"\"\"Agent specialized in official report analysis\"\"\"\n",
    "    system_prompt = \"\"\"You are an official document analyst specializing in reports.\n",
    "Extract key information such as:\n",
    "- Report numbers and dates\n",
    "- Incident types and locations  \n",
    "- Key parties involved\n",
    "- Financial impacts\n",
    "- Status and follow-up actions\n",
    "\n",
    "Provide structured summaries.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[extract_document_text], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_compliance_agent():\n",
    "    \"\"\"Agent that validates compliance rules\"\"\"\n",
    "    system_prompt = \"\"\"You are a compliance validation specialist. Check documents for:\n",
    "1. Duplicate submissions (use check_duplicate_invoice tool)\n",
    "2. Missing required fields\n",
    "3. Amounts exceeding thresholds ($5,000+)\n",
    "4. Date inconsistencies\n",
    "5. Policy violations\n",
    "\n",
    "Assign risk levels: LOW, MEDIUM, HIGH, CRITICAL.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, \n",
    "                 tools=[list_documents, extract_document_text, check_duplicate_invoice], \n",
    "                 system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_synthesis_agent():\n",
    "    \"\"\"Agent that synthesizes findings\"\"\"\n",
    "    system_prompt = \"\"\"You are an executive synthesis specialist. Create:\n",
    "1. Executive Summary\n",
    "2. Key Findings (prioritized by risk)\n",
    "3. Flagged Issues requiring immediate attention\n",
    "4. Recommended Actions\n",
    "\n",
    "Be concise and actionable.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "print(\"âœ… Specialized agents created:\")\n",
    "print(\"  - Triage Agent\")\n",
    "print(\"  - Invoice Analyst Agent\")\n",
    "print(\"  - Report Analyst Agent\")\n",
    "print(\"  - Compliance Agent\")\n",
    "print(\"  - Synthesis Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow State Management\n",
    "\n",
    "Define the state object to track the multi-agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentWorkflowState:\n",
    "    \"\"\"State object for document processing workflow\"\"\"\n",
    "    document_ids: List[str]\n",
    "    \n",
    "    # Classification results\n",
    "    document_classifications: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    # Analysis results\n",
    "    invoice_analysis: Dict[str, Any] = field(default_factory=dict)\n",
    "    report_analysis: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    # Compliance results\n",
    "    compliance_findings: List[Dict] = field(default_factory=list)\n",
    "    flagged_issues: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    # Final output\n",
    "    executive_summary: str = \"\"\n",
    "    \n",
    "    # Tracking\n",
    "    agents_invoked: List[str] = field(default_factory=list)\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "print(\"âœ… Workflow state management ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Orchestrate the Multi-Agent Workflow\n",
    "\n",
    "Now we'll create the orchestrator that coordinates all agents in a sequential workflow.\n",
    "\n",
    "#### Workflow Execution Flow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 1: TRIAGE & CLASSIFICATION                                â”‚\n",
    "â”‚  â€¢ Triage Agent classifies each document                        â”‚\n",
    "â”‚  â€¢ Calls extract_document_text for each doc                     â”‚\n",
    "â”‚  â€¢ Updates document_classifications in state                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 2: SPECIALIZED ANALYSIS                                   â”‚\n",
    "â”‚  â€¢ Route invoices â†’ Invoice Analyst Agent                       â”‚\n",
    "â”‚  â€¢ Route reports â†’ Report Analyst Agent                         â”‚\n",
    "â”‚  â€¢ Each agent performs domain-specific extraction               â”‚\n",
    "â”‚  â€¢ Results stored in state.invoice_analysis / report_analysis   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 3: COMPLIANCE VALIDATION                                  â”‚\n",
    "â”‚  â€¢ Compliance Agent reviews all documents                       â”‚\n",
    "â”‚  â€¢ Checks for duplicates, anomalies, threshold violations       â”‚\n",
    "â”‚  â€¢ Assigns risk levels (LOW, MEDIUM, HIGH, CRITICAL)            â”‚\n",
    "â”‚  â€¢ Results stored in state.compliance_findings                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 4: EXECUTIVE SYNTHESIS                                    â”‚\n",
    "â”‚  â€¢ Synthesis Agent aggregates all findings                      â”‚\n",
    "â”‚  â€¢ Creates executive summary with prioritized actions           â”‚\n",
    "â”‚  â€¢ Generates final report                                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### What This Orchestrator Does\n",
    "\n",
    "1. **State Management**: Tracks all intermediate results in `DocumentWorkflowState`\n",
    "2. **Sequential Execution**: Each step builds on previous results\n",
    "3. **Agent Coordination**: Routes documents to appropriate specialist agents\n",
    "4. **Progress Tracking**: Displays real-time progress with emojis and status updates\n",
    "5. **Performance Metrics**: Measures execution time and agent invocations\n",
    "\n",
    "#### Real-World Benefits\n",
    "\n",
    "- **Auditability**: Complete trace of which agents processed which documents\n",
    "- **Debugging**: State object shows intermediate results at each step\n",
    "- **Scalability**: Easy to add new workflow steps or parallel processing\n",
    "- **Flexibility**: Modify agent behavior without changing orchestration logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_document_workflow(document_ids: List[str]) -> DocumentWorkflowState:\n",
    "    \"\"\"\n",
    "    Orchestrate multi-agent document processing workflow.\n",
    "    \n",
    "    Workflow Steps:\n",
    "    1. Triage - Classify each document (calls OCR)\n",
    "    2. Specialized Analysis - Route to appropriate analyst\n",
    "    3. Compliance Check - Validate against rules\n",
    "    4. Synthesis - Generate executive summary\n",
    "    \n",
    "    Args:\n",
    "        document_ids: List of document IDs to process\n",
    "        \n",
    "    Returns:\n",
    "        DocumentWorkflowState with complete analysis\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    state = DocumentWorkflowState(document_ids=document_ids)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“„ DOCUMENT PROCESSING WORKFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Processing {len(document_ids)} documents\\n\")\n",
    "    \n",
    "    # STEP 1: Triage and Classification\n",
    "    print(\"\\nðŸ“‹ STEP 1: Document Triage & Classification\")\n",
    "    print(\"-\"*80)\n",
    "    triage_agent = create_triage_agent()\n",
    "    state.agents_invoked.append(\"triage\")\n",
    "    \n",
    "    for doc_id in document_ids:\n",
    "        query = f\"Classify the document type for document {doc_id}. Use extract_document_text to retrieve and analyze it.\"\n",
    "        response = triage_agent(query)\n",
    "        response_str = str(response)\n",
    "        \n",
    "        # Determine classification\n",
    "        if \"invoice\" in response_str.lower():\n",
    "            doc_type = \"invoice\"\n",
    "        elif \"police\" in response_str.lower() or \"report\" in response_str.lower():\n",
    "            doc_type = \"police_report\"\n",
    "        else:\n",
    "            doc_type = \"other\"\n",
    "        \n",
    "        state.document_classifications[doc_id] = doc_type\n",
    "        print(f\"  âœ“ {doc_id}: {doc_type}\")\n",
    "    \n",
    "    # STEP 2: Specialized Analysis\n",
    "    print(\"\\n\\nðŸ” STEP 2: Specialized Analysis\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    invoice_agent = create_invoice_analyst_agent()\n",
    "    report_agent = create_report_analyst_agent()\n",
    "    \n",
    "    for doc_id, doc_type in state.document_classifications.items():\n",
    "        if doc_type == \"invoice\":\n",
    "            print(f\"\\n  Analyzing invoice {doc_id}...\")\n",
    "            query = f\"Analyze invoice {doc_id}. Extract all details, check for anomalies, and use check_duplicate_invoice to detect duplicates.\"\n",
    "            response = invoice_agent(query)\n",
    "            state.invoice_analysis[doc_id] = str(response)\n",
    "            state.agents_invoked.append(f\"invoice_analyst_{doc_id}\")\n",
    "            print(f\"  âœ“ Invoice analysis complete\")\n",
    "            \n",
    "        elif doc_type == \"police_report\":\n",
    "            print(f\"\\n  Analyzing police report {doc_id}...\")\n",
    "            query = f\"Analyze police report {doc_id}. Extract key incident details.\"\n",
    "            response = report_agent(query)\n",
    "            state.report_analysis[doc_id] = str(response)\n",
    "            state.agents_invoked.append(f\"report_analyst_{doc_id}\")\n",
    "            print(f\"  âœ“ Report analysis complete\")\n",
    "    \n",
    "    # STEP 3: Compliance Check\n",
    "    print(\"\\n\\nðŸ›¡ï¸  STEP 3: Compliance Validation\")\n",
    "    print(\"-\"*80)\n",
    "    compliance_agent = create_compliance_agent()\n",
    "    state.agents_invoked.append(\"compliance\")\n",
    "    \n",
    "    query = \"Review all processed invoices for compliance issues: duplicates, missing fields, amounts >$5000, anomalies. List all documents first.\"\n",
    "    response = compliance_agent(query)\n",
    "    state.compliance_findings = [str(response)]\n",
    "    print(f\"  âœ“ Compliance check complete\")\n",
    "    \n",
    "    # STEP 4: Synthesis\n",
    "    print(\"\\n\\nðŸ’¡ STEP 4: Executive Synthesis\")\n",
    "    print(\"-\"*80)\n",
    "    synthesis_agent = create_synthesis_agent()\n",
    "    state.agents_invoked.append(\"synthesis\")\n",
    "    \n",
    "    synthesis_context = f\"\"\"Based on document processing results:\n",
    "\n",
    "DOCUMENTS PROCESSED: {len(document_ids)}\n",
    "CLASSIFICATIONS: {state.document_classifications}\n",
    "\n",
    "INVOICE ANALYSIS:\n",
    "{state.invoice_analysis}\n",
    "\n",
    "REPORT ANALYSIS:\n",
    "{state.report_analysis}\n",
    "\n",
    "COMPLIANCE FINDINGS:\n",
    "{state.compliance_findings}\n",
    "\n",
    "Create an executive summary with key findings and recommended actions.\"\"\"\n",
    "    \n",
    "    response = synthesis_agent(synthesis_context)\n",
    "    state.executive_summary = str(response)\n",
    "    \n",
    "    state.execution_time = time.time() - start_time\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“ EXECUTIVE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(state.executive_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š WORKFLOW SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Documents Processed: {len(document_ids)}\")\n",
    "    print(f\"Agents Invoked: {len(state.agents_invoked)}\")\n",
    "    print(f\"Total Execution Time: {state.execution_time:.2f}s\")\n",
    "    print(f\"OCR Cache Entries: {len(OCR_CACHE)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ… Workflow orchestrator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Execute the Workflow\n",
    "\n",
    "Now let's run the complete multi-agent workflow on our document batch.\n",
    "\n",
    "#### What Will Happen\n",
    "\n",
    "When you run this cell, you'll see:\n",
    "\n",
    "1. **Real-time Progress Updates** ðŸ“Š\n",
    "   - Each workflow step displays with visual separators\n",
    "   - OCR extraction progress for each document\n",
    "   - Agent invocations and classifications\n",
    "\n",
    "2. **Document Processing** ðŸ”„\n",
    "   - 3 invoices will be routed to the Invoice Analyst Agent\n",
    "   - 1 police report will be routed to the Report Analyst Agent\n",
    "   - Each document is processed only once (cached)\n",
    "\n",
    "3. **Compliance Analysis** ðŸ›¡ï¸\n",
    "   - Duplicate detection across all invoices\n",
    "   - Threshold checks (amounts > $5,000)\n",
    "   - Risk level assignments\n",
    "\n",
    "4. **Executive Summary** ðŸ“‹\n",
    "   - Consolidated findings from all agents\n",
    "   - Prioritized action items\n",
    "   - Risk assessment\n",
    "\n",
    "#### Expected Output\n",
    "\n",
    "- **Processing Time**: ~30-60 seconds (depending on document complexity)\n",
    "- **Agent Invocations**: 9-10 total (1 triage + 3-4 analysts + 1 compliance + 1 synthesis)\n",
    "- **OCR Calls**: 4 (one per document, with caching)\n",
    "\n",
    "#### Try It Yourself\n",
    "\n",
    "After running, you can:\n",
    "- Inspect `result.document_classifications` to see how documents were classified\n",
    "- Review `result.invoice_analysis` for detailed invoice breakdowns\n",
    "- Check `result.compliance_findings` for flagged issues\n",
    "- Read `result.executive_summary` for the final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow on all documents (3 invoices + 1 police report)\n",
    "document_ids = [\"doc_001\", \"doc_002\", \"doc_003\", \"doc_004\"]\n",
    "\n",
    "result = run_document_workflow(document_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workshop Conclusion\n",
    "\n",
    "Congratulations! You've completed the Mistral OCR workshop. ðŸŽ‰\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Mistral OCR Capabilities**\n",
    "   - Handwriting and whiteboard recognition\n",
    "   - Invoice and form processing\n",
    "   - Mathematical expression extraction\n",
    "   - Official document handling\n",
    "\n",
    "2. **Multimodal RAG Pipeline**\n",
    "   - Combining OCR with LLMs for document understanding\n",
    "   - Building two-stage processing workflows\n",
    "   - Leveraging Bedrock's Pixtral Large for analysis\n",
    "\n",
    "3. **Multi-Agent Systems**\n",
    "   - Orchestrating specialized agents with Strands\n",
    "   - Tool-based agent interactions\n",
    "   - State management in complex workflows\n",
    "   - Compliance automation and duplicate detection\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Extend This Workshop:**\n",
    "- Add new document types (contracts, receipts, medical records)\n",
    "- Create custom compliance rules for your industry\n",
    "- Integrate with your document management system\n",
    "- Build a web interface for document upload and processing\n",
    "\n",
    "**Production Deployment:**\n",
    "- Set up auto-scaling for SageMaker endpoints\n",
    "- Implement batch processing for large document volumes\n",
    "- Add monitoring and alerting with CloudWatch\n",
    "- Store results in a database for audit trails\n",
    "\n",
    "**Learn More:**\n",
    "- [Mistral OCR Documentation](https://docs.mistral.ai/capabilities/vision/)\n",
    "- [Strands Agents GitHub](https://github.com/strands-agents/strands-agents)\n",
    "- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [SageMaker Deployment Best Practices](https://docs.aws.amazon.com/sagemaker/)\n",
    "\n",
    "### Questions?\n",
    "\n",
    "Reach out to your workshop facilitator or AWS account team for:\n",
    "- Production deployment guidance\n",
    "- Custom use case discussions\n",
    "- Architecture reviews\n",
    "- Cost optimization strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

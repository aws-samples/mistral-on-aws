{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral OCR Workshop\n",
    "\n",
    "This notebook demonstrates Optical Character Recognition using Mistral models on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Mistral OCR\n",
    "\n",
    "**Mistral OCR** is a specialized optical character recognition model designed for extracting text, images, tables, and mathematical expressions from documents. Unlike traditional OCR models that only extract text, Mistral OCR comprehends each element of documents and returns ordered interleaved text and images in markdown format, making it ideal for multimodal document understanding and RAG systems.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **State-of-the-Art Performance**: Achieves 94.89% overall accuracy on benchmarks, outperforming Google Document AI (83.42%), Azure OCR (89.52%), Gemini models (88-90%), and GPT-4o (89.77%)\n",
    "- **Complex Document Understanding**: Excels at processing interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting\n",
    "- **Natively Multilingual**: Parses, understands, and transcribes thousands of scripts, fonts, and languages across all continents with 99%+ accuracy\n",
    "- **Multi-format Support**: Process images (JPG, PNG, WebP, etc.) and multi-page PDF documents\n",
    "- **Doc-as-Prompt & Structured Output**: Use documents as prompts and extract information into structured formats like JSON for downstream function calls and agent building\n",
    "- **RAG-Ready**: Ideal model for use in Retrieval-Augmented Generation (RAG) systems with multimodal documents such as slides or complex PDFs\n",
    "- **Production-Ready**: Deploy on Amazon SageMaker, la Plateforme, or self-host for organizations with stringent data privacy requirements\n",
    "\n",
    "### Performance Highlights\n",
    "\n",
    "Mistral OCR excels across multiple dimensions:\n",
    "\n",
    "| Category | Mistral OCR 2503 | Best Competitor |\n",
    "|----------|------------------|-----------------|\n",
    "| Overall Accuracy | 94.89% | 90.23% (Gemini-1.5-Flash) |\n",
    "| Mathematical Expressions | 94.29% | 89.11% (Gemini-1.5-Flash) |\n",
    "| Scanned Documents | 98.96% | 96.15% (Gemini-1.5-Pro) |\n",
    "| Tables | 96.12% | 91.70% (GPT-4o) |\n",
    "| Multilingual | 99.02% | 97.31% (Azure OCR) |\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **Document Digitization**: Convert scanned documents, receipts, forms, and historical archives into searchable, AI-ready text\n",
    "2. **Scientific Research**: Extract text, equations, tables, charts, and figures from scientific papers and journals\n",
    "3. **Handwriting Recognition**: Process handwritten notes, whiteboard images, and forms\n",
    "4. **Multimodal RAG Systems**: Build intelligent document understanding pipelines by combining Mistral OCR with LLMs like Mistral Small for analysis and summarization\n",
    "5. **Multi-language Processing**: Handle documents in diverse linguistic backgrounds, from global organizations to hyperlocal businesses\n",
    "6. **Structured Data Extraction**: Extract specific information from documents and format it into JSON for automated workflows and agent systems\n",
    "7. **Cultural Heritage Preservation**: Digitize historical documents and artifacts for preservation and accessibility\n",
    "\n",
    "This workshop will demonstrate how to deploy and use Mistral OCR on Amazon SageMaker for various document processing tasks, leveraging its industry-leading accuracy, speed, and versatility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Mistral OCR Response Format\n",
    "\n",
    "Mistral OCR returns document content as **Markdown with interleaved text and images**, preserving the document structure and layout hierarchy. This format is optimized for downstream processing by LLMs and provides precise positioning information through bounding boxes.\n",
    "\n",
    "### Supported Document Elements\n",
    "\n",
    "Mistral OCR can extract and recognize a wide variety of document elements:\n",
    "\n",
    "- **Standard typed text** - Regular printed text in any font\n",
    "- **Multilingual text** - Mixed scripts (e.g., Asian and Roman characters)\n",
    "- **Mathematical expressions** - Formulas, equations, and LaTeX notation\n",
    "- **Handwriting** - Handwritten notes and annotations\n",
    "- **Strikethrough text** - Text with strikethrough formatting\n",
    "- **Diverse layout formats** - Complex page layouts and structures\n",
    "- **Multi-column tables** - Tables with multiple columns and complex structures\n",
    "- **Text with specific bounding boxes** - Precise spatial positioning\n",
    "- **Text on colored backgrounds** - Text overlaid on colored regions\n",
    "- **Form elements** - Checkboxes and circle selection fields\n",
    "\n",
    "### Response Structure\n",
    "\n",
    "Responses are returned in **Markdown format** with:\n",
    "- Structural elements like pipes (`|`), LaTeX, and tables\n",
    "- Layout cues that help LLMs understand document hierarchy\n",
    "- Image placeholders embedded in the text\n",
    "\n",
    "#### Image Representation\n",
    "\n",
    "Images within documents are represented as Markdown image syntax:\n",
    "```markdown\n",
    "![img-0.jpeg](img-0.jpeg)\n",
    "```\n",
    "\n",
    "The image ID (e.g., `img-0.jpeg`) maps to the `pages[n].images` array, which contains:\n",
    "- **Bounding box coordinates** (`top_left_x`, `top_left_y`, `bottom_right_x`, `bottom_right_y`)\n",
    "- **Base64-encoded payload** (optional)\n",
    "- **Image annotations** (if applicable)\n",
    "\n",
    "#### Example Response Structure\n",
    "\n",
    "```python\n",
    "{\n",
    "  'index': 13,\n",
    "  'markdown': \"![img-13.jpeg](img-13.jpeg)\\n\\nFigure 11: Examples of model responses...\",\n",
    "  'images': [\n",
    "    {\n",
    "      'id': 'img-13.jpeg',\n",
    "      'top_left_x': 294,\n",
    "      'top_left_y': 512,\n",
    "      'bottom_right_x': 1404,\n",
    "      'bottom_right_y': 1568,\n",
    "      'image_base64': None,\n",
    "      'image_annotation': None\n",
    "    }\n",
    "  ],\n",
    "  'dimensions': {\n",
    "    'dpi': 200,\n",
    "    'height': 2200,\n",
    "    'width': 1700\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This structured format enables precise extraction, spatial understanding, and seamless integration with downstream AI pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries for OCR processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These helper functions support image processing, model invocation, and post-response processing for the Mistral OCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_local_file_base64(file_path: str, file_type: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Encode a local file (image or PDF) to base64 string.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the local file\n",
    "        file_type: Type of file ('image' or 'pdf'). If None, inferred from extension.\n",
    "    \n",
    "    Returns:\n",
    "        Base64 encoded string of the file\n",
    "    \"\"\"\n",
    "    if file_type is None:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            file_type = \"pdf\"\n",
    "        elif ext in (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\"):\n",
    "            file_type = \"image\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type from extension: {ext}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            encoded_data = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "            return encoded_data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to encode {file_type} at {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_inference(client, endpoint_name: str, payload: dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke the SageMaker endpoint for OCR inference.\n",
    "    \n",
    "    Args:\n",
    "        client: SageMaker runtime client\n",
    "        endpoint_name: Name of the deployed endpoint\n",
    "        payload: JSON payload containing the image data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing parsed OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inference_out = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        inference_resp_str = inference_out[\"Body\"].read().decode(\"utf-8\")\n",
    "        return json.loads(inference_resp_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Replace image placeholders in markdown with base64-encoded images.\n",
    "    \n",
    "    Args:\n",
    "        markdown_str: Markdown string with image placeholders\n",
    "        images_dict: Dictionary mapping image names to base64 strings\n",
    "    \n",
    "    Returns:\n",
    "        Markdown string with embedded base64 images\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(\n",
    "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
    "        )\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: dict) -> str:\n",
    "    \"\"\"\n",
    "    Combine OCR text and images into a single markdown document.\n",
    "    \n",
    "    Args:\n",
    "        ocr_response: Response dictionary from OCR model\n",
    "    \n",
    "    Returns:\n",
    "        Combined markdown string with embedded images\n",
    "    \"\"\"\n",
    "    markdowns = []\n",
    "    for page in ocr_response[\"pages\"]:\n",
    "        image_data = {img[\"id\"]: img[\"image_base64\"] for img in page.get(\"images\", [])}\n",
    "        markdown_with_images = replace_images_in_markdown(page[\"markdown\"], image_data)\n",
    "        markdowns.append(markdown_with_images)\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the workshop, use pre-built Mistral OCR endpoint for inference\n",
    "\n",
    "def run_inference_with_api(api_endpoint: str, payload: dict[str, Any], timeout: int = 300) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke the API Gateway endpoint for OCR inference using HTTP requests.\n",
    "    \n",
    "    Args:\n",
    "      api_endpoint: URL of the API Gateway endpoint\n",
    "      payload: JSON payload containing the image data\n",
    "      timeout: Request timeout in seconds (default: 300)\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary containing parsed OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "      response = requests.post(\n",
    "          api_endpoint,\n",
    "          json=payload,\n",
    "          headers={\"Content-Type\": \"application/json\"},\n",
    "          timeout=timeout\n",
    "      )\n",
    "\n",
    "      # Raise an exception for bad status codes\n",
    "      response.raise_for_status()\n",
    "\n",
    "      # Parse and return the JSON response\n",
    "      return response.json()\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timeout after {timeout} seconds\")\n",
    "        raise\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response status: {e.response.status_code}\")\n",
    "            print(f\"Response body: {e.response.text}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Mistral OCR\n",
    "\n",
    "Now let's use the Mistral OCR model to extract text from an image document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Access and Deployment  \n",
    "\n",
    "**Important Notice:**                                                                                                                                                              \n",
    "The Mistral OCR model is available from AWS Marketplace via private offer. If you want to trial this model, please contact your account manager and provide your AWS account ID to whitelist for this model access.                                                                                                                                                   Then, please follow this link to deploy the Mistral OCR model on a SageMaker endpoint:                                                                                            [Mistral OCR SageMaker Deployment Guide](https://github.com/aws-samples/mistral-on-aws/blob/main/Mistral%20OCR/Mistral-OCR-SageMaker-Deployment-example.ipynb)     \n",
    "\n",
    "\n",
    "**For Workshop Participants:**                                                                                                                                                     During the workshop, we will use a pre-created API link to access the Mistral OCR model. The link will be provided during the workshop session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your AWS account is whitelisted to the model access, and also Mistral OCR model is deployed as SageMaker endpoint, please unncomment and use below code: \n",
    "\n",
    "# MISTRAL_OCR_ENDPOINT_NAME = \"mistral-ocr-endpoint\"\n",
    "# image_b64 = encode_local_file_base64(file_path=\"images/french.png\")\n",
    "\n",
    "# # Prepare the payload for Mistral OCR model\n",
    "# payload = {\n",
    "#     \"model\": \"mistral-ocr-2505\",\n",
    "#     \"document\": {\n",
    "#         \"type\": \"image_url\",\n",
    "#         \"image_url\": f\"data:image/jpeg;base64,{image_b64}\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Create a client and invoke the endpoint\n",
    "# sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "# result_parsed = run_inference(client=sagemaker_client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=payload)\n",
    "\n",
    "# # Display final markdown content with embedded images\n",
    "# display(Markdown(get_combined_markdown(result_parsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Workshop, plase use below code to access the pre-built Mistral OCR endpoint\n",
    "import requests\n",
    "\n",
    "# API Gateway endpoint URL\n",
    "API_ENDPOINT = \"<will provide in workshop>\"\n",
    "\n",
    "# Encode the image (reusing the same encoded image from above)\n",
    "image_b64 = encode_local_file_base64(file_path=\"images/french.png\")\n",
    "\n",
    "# Prepare the payload for API request\n",
    "api_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image_b64}\",\n",
    "      \n",
    "  },\n",
    "    \"include_image_base64\": True  # Add this parameter to include embeded images in the output\n",
    "}\n",
    "\n",
    "# Make POST request to API Gateway\n",
    "result_parsed =  run_inference_with_api(API_ENDPOINT, api_payload) \n",
    "\n",
    "# Display final markdown content with embedded images\n",
    "print(\"OCR Result from API Gateway:\")\n",
    "display(Markdown(get_combined_markdown(result_parsed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Understanding Pipeline with OCR and LLM\n",
    "\n",
    "Combine Mistral OCR with Mistral Small 3.0 to build an intelligent document understanding pipeline that can extract text and answer questions about the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for Bedrock Runtime\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "def bedrock_converse(system_prompt: str, messages: list, endpoint_arn: str, display_usage=False):\n",
    "    \"\"\"Invoke model using Converse API\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "\n",
    "    output_content = ''.join(\n",
    "        content['text'] for content in response['output']['message']['content']\n",
    "    )\n",
    "\n",
    "    if display_usage:\n",
    "        token_usage = response['usage']\n",
    "        print(f\"\\tLatency: {response['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content\n",
    "\n",
    "def bedrock_converse_stream(system_prompt: str, messages: list, endpoint_arn: str):\n",
    "    \"\"\"Invoke model with streaming\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse_stream(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "    \n",
    "    stream = response.get('stream')\n",
    "    output_content = ''\n",
    "    \n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "            \n",
    "            if 'contentBlockDelta' in event:\n",
    "                text_chunk = event['contentBlockDelta']['delta']['text']\n",
    "                print(text_chunk, end=\"\")\n",
    "                output_content += text_chunk\n",
    "            \n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "            \n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_understanding_pipeline(\n",
    "    image_path: str,\n",
    "    user_prompt: str,\n",
    "    ocr_endpoint: str=None,\n",
    "    llm_endpoint: str=None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Full pipeline for document understanding from image input.\n",
    "\n",
    "    Args:\n",
    "        image_path: Local path to the document image\n",
    "        user_prompt: What insights the user wants to extract\n",
    "        ocr_endpoint: SageMaker endpoint for OCR model\n",
    "        llm_endpoint: Bedrock endpoint for document understanding LLM\n",
    "\n",
    "    Returns:\n",
    "        Model-generated response with document insights\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Encode local file using helper\n",
    "    encoded_image = encode_local_file_base64(image_path)\n",
    "\n",
    "    # payload = {\n",
    "    #     \"model\": \"mistral-ocr-2505\",\n",
    "    #     \"document\": {\n",
    "    #         \"type\": \"image_url\",\n",
    "    #         \"image_url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    "    api_payload = {\n",
    "        \"model\": \"mistral-ocr-2505\",\n",
    "        \"document\": {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{image_b64}\",\n",
    "          \n",
    "      },\n",
    "        \"include_image_base64\": False  # Add this parameter to include embeded images in the output\n",
    "    }\n",
    "\n",
    "\n",
    "    # Step 2: Run OCR model\n",
    "    print(\"Running OCR model...\")\n",
    "    # ocr_result = run_inference(client=sagemaker_client, endpoint_name=ocr_endpoint, payload=payload) # SageMaker endpoint \n",
    "    ocr_result =  run_inference_with_api(API_ENDPOINT, api_payload) # Workshop pre-built API endpoint\n",
    "\n",
    "\n",
    "    # Step 3: Convert OCR output to Markdown\n",
    "    print(\"Formatting OCR output...\")\n",
    "    markdown_doc = get_combined_markdown(ocr_result)\n",
    "\n",
    "    print(\"----- OCR Text  -----\")\n",
    "    display(Markdown(markdown_doc))\n",
    "\n",
    "    # Step 4: Prepare LLM messages\n",
    "    system_prompt = (\n",
    "        \"You are a document understanding assistant. The user will provide structured OCR content \"\n",
    "        \"from a scanned document. Use that information to generate clear, factual insights that \"\n",
    "        \"answer the user's request.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": f\"{user_prompt}\\n\\n--- Document Content ---\\n{markdown_doc}\"}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Step 5: Call Bedrock LLM with streaming\n",
    "    print(\"Running LLM for document insights...\")\n",
    "    insights = bedrock_converse_stream(system_prompt, messages, llm_endpoint)\n",
    "\n",
    "    return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Document Summarization\n",
    "\n",
    "Let's use the pipeline to extract and summarize a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"images/french.png\"\n",
    "user_prompt = \"can you summarise this document\"\n",
    "# ocr_endpoint = MISTRAL_OCR_ENDPOINT_NAME\n",
    "# llm_endpoint = \"<ENDPOINT_ARN>\"  # Replace with your Mistral Small 3.0 endpoint ARN\n",
    "llm_endpoint = \"us.mistral.pixtral-large-2502-v1:0\" # or use Pixtral model from Bedrock\n",
    "\n",
    "# document_understanding_pipeline(image_path, user_prompt, ocr_endpoint, llm_endpoint)\n",
    "document_understanding_pipeline(image_path=image_path, user_prompt=user_prompt, llm_endpoint=llm_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Whiteboard/Handwriting OCR\n",
    "\n",
    "Extract text from whiteboard images or handwritten notes using Mistral OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process whiteboard image\n",
    "whiteboard_b64 = encode_local_file_base64(file_path=\"images/whiteboard.png\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "whiteboard_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/png;base64,{whiteboard_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# whiteboard_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=whiteboard_payload\n",
    "# )\n",
    "\n",
    "# In Workshop, invoke the pre-built API endpoint \n",
    "whiteboard_result =  run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    whiteboard_payload) \n",
    "\n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Whiteboard Content:\")\n",
    "display(Markdown(get_combined_markdown(whiteboard_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-Resolution Handwriting Recognition\n",
    "\n",
    "Now let's test Mistral OCR's ability to handle challenging, low-quality handwritten content. This demonstrates the model's robustness even when dealing with compressed or poor-quality images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process low-resolution handwriting image\n",
    "handwriting_b64 = encode_local_file_base64(file_path=\"images/handwriting_low_res_2_resize.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "handwriting_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{handwriting_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# handwriting_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=handwriting_payload\n",
    "# )\n",
    "\n",
    "# In Workshop, invoke the pre-built API endpoint \n",
    "handwriting_result =  run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    handwriting_payload) \n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Low-Resolution Handwriting Content:\")\n",
    "display(Markdown(get_combined_markdown(handwriting_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Invoice Processing\n",
    "\n",
    "Process invoice images to extract structured data such as vendor details, line items, totals, and dates. This is a common use case for automating accounts payable workflows and financial document processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Invoice Processing\n",
    "\n",
    "First, let's process a single invoice to extract all its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single invoice\n",
    "invoice_b64 = encode_local_file_base64(file_path=\"images/invoice_1.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "invoice_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{invoice_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# invoice_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=invoice_result\n",
    "# )\n",
    "\n",
    "# In Workshop, invoke the pre-built API endpoint \n",
    "invoice_result =  run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    invoice_payload) \n",
    "\n",
    "# Display the extracted invoice content\n",
    "print(\"Extracted Invoice Content:\")\n",
    "display(Markdown(get_combined_markdown(invoice_result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

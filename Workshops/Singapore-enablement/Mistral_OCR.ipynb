{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral OCR Workshop\n",
    "\n",
    "This notebook demonstrates Optical Character Recognition using Mistral models on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Mistral OCR\n",
    "\n",
    "**Mistral OCR (mistral-ocr-latest)** is a specialized optical character recognition model designed for extracting text, images, tables, and mathematical expressions from documents. Unlike traditional OCR models that only extract text, Mistral OCR comprehends each element of documents and returns ordered interleaved text and images in markdown format, making it ideal for multimodal document understanding and RAG systems.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **State-of-the-Art Performance**: Achieves 94.89% overall accuracy on benchmarks, outperforming Google Document AI (83.42%), Azure OCR (89.52%), Gemini models (88-90%), and GPT-4o (89.77%)\n",
    "- **Complex Document Understanding**: Excels at processing interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting\n",
    "- **Natively Multilingual**: Parses, understands, and transcribes thousands of scripts, fonts, and languages across all continents with 99%+ accuracy\n",
    "- **Multi-format Support**: Process images (JPG, PNG, WebP, etc.) and multi-page PDF documents\n",
    "- **Blazingly Fast**: Processes up to 2000 pages per minute on a single nodeâ€”fastest in its category\n",
    "- **Doc-as-Prompt & Structured Output**: Use documents as prompts and extract information into structured formats like JSON for downstream function calls and agent building\n",
    "- **RAG-Ready**: Ideal model for use in Retrieval-Augmented Generation (RAG) systems with multimodal documents such as slides or complex PDFs\n",
    "- **Production-Ready**: Deploy on Amazon SageMaker, la Plateforme, or self-host for organizations with stringent data privacy requirements\n",
    "- **Cost-Effective**: Processes 1000 pages per dollar (approximately double with batch inference)\n",
    "\n",
    "### Performance Highlights\n",
    "\n",
    "Mistral OCR excels across multiple dimensions:\n",
    "\n",
    "| Category | Mistral OCR 2503 | Best Competitor |\n",
    "|----------|------------------|-----------------|\n",
    "| Overall Accuracy | 94.89% | 90.23% (Gemini-1.5-Flash) |\n",
    "| Mathematical Expressions | 94.29% | 89.11% (Gemini-1.5-Flash) |\n",
    "| Scanned Documents | 98.96% | 96.15% (Gemini-1.5-Pro) |\n",
    "| Tables | 96.12% | 91.70% (GPT-4o) |\n",
    "| Multilingual | 99.02% | 97.31% (Azure OCR) |\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **Document Digitization**: Convert scanned documents, receipts, forms, and historical archives into searchable, AI-ready text\n",
    "2. **Scientific Research**: Extract text, equations, tables, charts, and figures from scientific papers and journals\n",
    "3. **Handwriting Recognition**: Process handwritten notes, whiteboard images, and forms\n",
    "4. **Multimodal RAG Systems**: Build intelligent document understanding pipelines by combining Mistral OCR with LLMs like Mistral Small for analysis and summarization\n",
    "5. **Multi-language Processing**: Handle documents in diverse linguistic backgrounds, from global organizations to hyperlocal businesses\n",
    "6. **Structured Data Extraction**: Extract specific information from documents and format it into JSON for automated workflows and agent systems\n",
    "7. **Cultural Heritage Preservation**: Digitize historical documents and artifacts for preservation and accessibility\n",
    "\n",
    "This workshop will demonstrate how to deploy and use Mistral OCR on Amazon SageMaker for various document processing tasks, leveraging its industry-leading accuracy, speed, and versatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries for OCR processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These helper functions support image processing, model invocation, and post-response processing for the Mistral OCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_local_file_base64(file_path: str, file_type: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Encode a local file (image or PDF) to base64 string.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the local file\n",
    "        file_type: Type of file ('image' or 'pdf'). If None, inferred from extension.\n",
    "    \n",
    "    Returns:\n",
    "        Base64 encoded string of the file\n",
    "    \"\"\"\n",
    "    if file_type is None:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            file_type = \"pdf\"\n",
    "        elif ext in (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\"):\n",
    "            file_type = \"image\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type from extension: {ext}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            encoded_data = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "            return encoded_data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to encode {file_type} at {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_inference(client, endpoint_name: str, payload: dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke the SageMaker endpoint for OCR inference.\n",
    "    \n",
    "    Args:\n",
    "        client: SageMaker runtime client\n",
    "        endpoint_name: Name of the deployed endpoint\n",
    "        payload: JSON payload containing the image data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing parsed OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inference_out = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        inference_resp_str = inference_out[\"Body\"].read().decode(\"utf-8\")\n",
    "        return json.loads(inference_resp_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Replace image placeholders in markdown with base64-encoded images.\n",
    "    \n",
    "    Args:\n",
    "        markdown_str: Markdown string with image placeholders\n",
    "        images_dict: Dictionary mapping image names to base64 strings\n",
    "    \n",
    "    Returns:\n",
    "        Markdown string with embedded base64 images\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(\n",
    "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
    "        )\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: dict) -> str:\n",
    "    \"\"\"\n",
    "    Combine OCR text and images into a single markdown document.\n",
    "    \n",
    "    Args:\n",
    "        ocr_response: Response dictionary from OCR model\n",
    "    \n",
    "    Returns:\n",
    "        Combined markdown string with embedded images\n",
    "    \"\"\"\n",
    "    markdowns = []\n",
    "    for page in ocr_response[\"pages\"]:\n",
    "        image_data = {img[\"id\"]: img[\"image_base64\"] for img in page.get(\"images\", [])}\n",
    "        markdown_with_images = replace_images_in_markdown(page[\"markdown\"], image_data)\n",
    "        markdowns.append(markdown_with_images)\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Mistral OCR\n",
    "\n",
    "Now let's use the Mistral OCR model to extract text from an image document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_OCR_ENDPOINT_NAME = \"mistral-ocr-endpoint\"\n",
    "image_b64 = encode_local_file_base64(file_path=\"images/french.png\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a client and invoke the endpoint\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "result_parsed = run_inference(client=sagemaker_client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=payload)\n",
    "\n",
    "# Display final markdown content with embedded images\n",
    "display(Markdown(get_combined_markdown(result_parsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Understanding Pipeline with OCR and LLM\n",
    "\n",
    "Combine Mistral OCR with Mistral Small 3.0 to build an intelligent document understanding pipeline that can extract text and answer questions about the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for Bedrock Runtime\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "def bedrock_converse(system_prompt: str, messages: list, endpoint_arn: str, display_usage=False):\n",
    "    \"\"\"Invoke model using Converse API\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "\n",
    "    output_content = ''.join(\n",
    "        content['text'] for content in response['output']['message']['content']\n",
    "    )\n",
    "\n",
    "    if display_usage:\n",
    "        token_usage = response['usage']\n",
    "        print(f\"\\tLatency: {response['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content\n",
    "\n",
    "def bedrock_converse_stream(system_prompt: str, messages: list, endpoint_arn: str):\n",
    "    \"\"\"Invoke model with streaming\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse_stream(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "    \n",
    "    stream = response.get('stream')\n",
    "    output_content = ''\n",
    "    \n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "            \n",
    "            if 'contentBlockDelta' in event:\n",
    "                text_chunk = event['contentBlockDelta']['delta']['text']\n",
    "                print(text_chunk, end=\"\")\n",
    "                output_content += text_chunk\n",
    "            \n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "            \n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_understanding_pipeline(\n",
    "    image_path: str,\n",
    "    user_prompt: str,\n",
    "    ocr_endpoint: str,\n",
    "    llm_endpoint: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Full pipeline for document understanding from image input.\n",
    "\n",
    "    Args:\n",
    "        image_path: Local path to the document image\n",
    "        user_prompt: What insights the user wants to extract\n",
    "        ocr_endpoint: SageMaker endpoint for OCR model\n",
    "        llm_endpoint: Bedrock endpoint for document understanding LLM\n",
    "\n",
    "    Returns:\n",
    "        Model-generated response with document insights\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Encode local file using helper\n",
    "    encoded_image = encode_local_file_base64(image_path)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"mistral-ocr-2505\",\n",
    "        \"document\": {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Step 2: Run OCR model\n",
    "    print(\"Running OCR model...\")\n",
    "    ocr_result = run_inference(client=sagemaker_client, endpoint_name=ocr_endpoint, payload=payload)\n",
    "\n",
    "    # Step 3: Convert OCR output to Markdown\n",
    "    print(\"Formatting OCR output...\")\n",
    "    markdown_doc = get_combined_markdown(ocr_result)\n",
    "\n",
    "    print(\"----- OCR Text  -----\")\n",
    "    display(Markdown(markdown_doc))\n",
    "\n",
    "    # Step 4: Prepare LLM messages\n",
    "    system_prompt = (\n",
    "        \"You are a document understanding assistant. The user will provide structured OCR content \"\n",
    "        \"from a scanned document. Use that information to generate clear, factual insights that \"\n",
    "        \"answer the user's request.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": f\"{user_prompt}\\n\\n--- Document Content ---\\n{markdown_doc}\"}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Step 5: Call Bedrock LLM with streaming\n",
    "    print(\"Running LLM for document insights...\")\n",
    "    insights = bedrock_converse_stream(system_prompt, messages, llm_endpoint)\n",
    "\n",
    "    return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Document Summarization\n",
    "\n",
    "Let's use the pipeline to extract and summarize a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"images/french.png\"\n",
    "user_prompt = \"can you summarise this document\"\n",
    "ocr_endpoint = MISTRAL_OCR_ENDPOINT_NAME\n",
    "llm_endpoint = \"<ENDPOINT_ARN>\"  # Replace with your Mistral Small 3.0 endpoint ARN\n",
    "\n",
    "document_understanding_pipeline(image_path, user_prompt, ocr_endpoint, llm_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Whiteboard/Handwriting OCR\n",
    "\n",
    "Extract text from whiteboard images or handwritten notes using Mistral OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process whiteboard image\n",
    "whiteboard_b64 = encode_local_file_base64(file_path=\"images/whiteboard.png\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "whiteboard_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/png;base64,{whiteboard_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "whiteboard_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=whiteboard_payload\n",
    ")\n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Whiteboard Content:\")\n",
    "display(Markdown(get_combined_markdown(whiteboard_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Low-Resolution Handwriting Recognition\n\nNow let's test Mistral OCR's ability to handle challenging, low-quality handwritten content. This demonstrates the model's robustness even when dealing with compressed or poor-quality images.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Process low-resolution handwriting image\nhandwriting_b64 = encode_local_file_base64(file_path=\"images/handwriting_low_res_2_resize.jpg\")\n\n# Prepare the payload for Mistral OCR model\nhandwriting_payload = {\n    \"model\": \"mistral-ocr-2505\",\n    \"document\": {\n        \"type\": \"image_url\",\n        \"image_url\": f\"data:image/jpeg;base64,{handwriting_b64}\"\n    }\n}\n\n# Invoke the endpoint\nhandwriting_result = run_inference(\n    client=sagemaker_client, \n    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n    payload=handwriting_payload\n)\n\n# Display the extracted text\nprint(\"Extracted Low-Resolution Handwriting Content:\")\ndisplay(Markdown(get_combined_markdown(handwriting_result)))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Invoice Processing\n",
    "\n",
    "Process invoice images to extract structured data such as vendor details, line items, totals, and dates. This is a common use case for automating accounts payable workflows and financial document processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Invoice Processing\n",
    "\n",
    "First, let's process a single invoice to extract all its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single invoice\n",
    "invoice_b64 = encode_local_file_base64(file_path=\"images/invoice_1.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "invoice_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{invoice_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "invoice_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=invoice_payload\n",
    ")\n",
    "\n",
    "# Display the extracted invoice content\n",
    "print(\"Extracted Invoice Content:\")\n",
    "display(Markdown(get_combined_markdown(invoice_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Invoice Processing\n",
    "\n",
    "Now let's process all three invoices in a single request. The Mistral OCR model can handle multiple images by treating them as separate pages in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all three invoices together\n",
    "invoice_files = [\"images/invoice_1.jpg\", \"images/invoice_2.jpg\", \"images/Invoice_3.jpg\"]\n",
    "\n",
    "# Encode all invoices\n",
    "print(f\"Processing {len(invoice_files)} invoices...\")\n",
    "encoded_invoices = [encode_local_file_base64(file_path=invoice_file) for invoice_file in invoice_files]\n",
    "\n",
    "# Create a combined payload with all invoices as multiple images\n",
    "# Note: For multiple images, we can create a multi-page document by concatenating them\n",
    "batch_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"documents\": [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{encoded_invoice}\"\n",
    "        }\n",
    "        for encoded_invoice in encoded_invoices\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Invoke the endpoint with all invoices\n",
    "batch_result = run_inference(\n",
    "    client=sagemaker_client, \n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "    payload=batch_payload\n",
    ")\n",
    "\n",
    "# Display results for each invoice\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BATCH PROCESSING RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for idx, page in enumerate(batch_result.get(\"pages\", []), 1):\n",
    "    print(f\"\\n--- Invoice {idx} ---\")\n",
    "    invoice_markdown = page.get(\"markdown\", \"\")\n",
    "    # Handle embedded images\n",
    "    if \"images\" in page:\n",
    "        image_data = {img[\"id\"]: img[\"image_base64\"] for img in page[\"images\"]}\n",
    "        invoice_markdown = replace_images_in_markdown(invoice_markdown, image_data)\n",
    "    display(Markdown(invoice_markdown))\n",
    "    print(f\"\\n{'='*80}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
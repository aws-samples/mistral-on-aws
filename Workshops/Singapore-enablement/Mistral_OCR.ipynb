{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral OCR Workshop\n",
    "\n",
    "This notebook demonstrates Optical Character Recognition using Mistral models on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Mistral OCR\n",
    "\n",
    "**Mistral OCR** is a specialized optical character recognition model designed for extracting text, images, tables, and mathematical expressions from documents. Unlike traditional OCR models that only extract text, Mistral OCR comprehends each element of documents and returns ordered interleaved text and images in markdown format, making it ideal for multimodal document understanding and RAG systems.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **State-of-the-Art Performance**: Achieves 94.89% overall accuracy on benchmarks, outperforming Google Document AI (83.42%), Azure OCR (89.52%), Gemini models (88-90%), and GPT-4o (89.77%)\n",
    "- **Complex Document Understanding**: Excels at processing interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting\n",
    "- **Natively Multilingual**: Parses, understands, and transcribes thousands of scripts, fonts, and languages across all continents with 99%+ accuracy\n",
    "- **Multi-format Support**: Process images (JPG, PNG, WebP, etc.) and multi-page PDF documents\n",
    "- **Doc-as-Prompt & Structured Output**: Use documents as prompts and extract information into structured formats like JSON for downstream function calls and agent building\n",
    "- **RAG-Ready**: Ideal model for use in Retrieval-Augmented Generation (RAG) systems with multimodal documents such as slides or complex PDFs\n",
    "- **Production-Ready**: Deploy on Amazon SageMaker, la Plateforme, or self-host for organizations with stringent data privacy requirements\n",
    "\n",
    "### Performance Highlights\n",
    "\n",
    "Mistral OCR excels across multiple dimensions:\n",
    "\n",
    "| Category | Mistral OCR 2503 | Best Competitor |\n",
    "|----------|------------------|-----------------|\n",
    "| Overall Accuracy | 94.89% | 90.23% (Gemini-1.5-Flash) |\n",
    "| Mathematical Expressions | 94.29% | 89.11% (Gemini-1.5-Flash) |\n",
    "| Scanned Documents | 98.96% | 96.15% (Gemini-1.5-Pro) |\n",
    "| Tables | 96.12% | 91.70% (GPT-4o) |\n",
    "| Multilingual | 99.02% | 97.31% (Azure OCR) |\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **Document Digitization**: Convert scanned documents, receipts, forms, and historical archives into searchable, AI-ready text\n",
    "2. **Scientific Research**: Extract text, equations, tables, charts, and figures from scientific papers and journals\n",
    "3. **Handwriting Recognition**: Process handwritten notes, whiteboard images, and forms\n",
    "4. **Multimodal RAG Systems**: Build intelligent document understanding pipelines by combining Mistral OCR with LLMs like Mistral Small for analysis and summarization\n",
    "5. **Multi-language Processing**: Handle documents in diverse linguistic backgrounds, from global organizations to hyperlocal businesses\n",
    "6. **Structured Data Extraction**: Extract specific information from documents and format it into JSON for automated workflows and agent systems\n",
    "7. **Cultural Heritage Preservation**: Digitize historical documents and artifacts for preservation and accessibility\n",
    "\n",
    "This workshop will demonstrate how to deploy and use Mistral OCR on Amazon SageMaker for various document processing tasks, leveraging its industry-leading accuracy, speed, and versatility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Mistral OCR Response Format\n",
    "\n",
    "Mistral OCR returns document content as **Markdown with interleaved text and images**, preserving the document structure and layout hierarchy. This format is optimized for downstream processing by LLMs and provides precise positioning information through bounding boxes.\n",
    "\n",
    "### Supported Document Elements\n",
    "\n",
    "Mistral OCR can extract and recognize a wide variety of document elements:\n",
    "\n",
    "- **Standard typed text** - Regular printed text in any font\n",
    "- **Multilingual text** - Mixed scripts (e.g., Asian and Roman characters)\n",
    "- **Mathematical expressions** - Formulas, equations, and LaTeX notation\n",
    "- **Handwriting** - Handwritten notes and annotations\n",
    "- **Strikethrough text** - Text with strikethrough formatting\n",
    "- **Diverse layout formats** - Complex page layouts and structures\n",
    "- **Multi-column tables** - Tables with multiple columns and complex structures\n",
    "- **Text with specific bounding boxes** - Precise spatial positioning\n",
    "- **Text on colored backgrounds** - Text overlaid on colored regions\n",
    "- **Form elements** - Checkboxes and circle selection fields\n",
    "\n",
    "### Response Structure\n",
    "\n",
    "Responses are returned in **Markdown format** with:\n",
    "- Structural elements like pipes (`|`), LaTeX, and tables\n",
    "- Layout cues that help LLMs understand document hierarchy\n",
    "- Image placeholders embedded in the text\n",
    "\n",
    "#### Image Representation\n",
    "\n",
    "Images within documents are represented as Markdown image syntax:\n",
    "```markdown\n",
    "![img-0.jpeg](img-0.jpeg)\n",
    "```\n",
    "\n",
    "The image ID (e.g., `img-0.jpeg`) maps to the `pages[n].images` array, which contains:\n",
    "- **Bounding box coordinates** (`top_left_x`, `top_left_y`, `bottom_right_x`, `bottom_right_y`)\n",
    "- **Base64-encoded payload** (optional)\n",
    "- **Image annotations** (if applicable)\n",
    "\n",
    "#### Example Response Structure\n",
    "\n",
    "```python\n",
    "{\n",
    "  'index': 13,\n",
    "  'markdown': \"![img-13.jpeg](img-13.jpeg)\\n\\nFigure 11: Examples of model responses...\",\n",
    "  'images': [\n",
    "    {\n",
    "      'id': 'img-13.jpeg',\n",
    "      'top_left_x': 294,\n",
    "      'top_left_y': 512,\n",
    "      'bottom_right_x': 1404,\n",
    "      'bottom_right_y': 1568,\n",
    "      'image_base64': None,\n",
    "      'image_annotation': None\n",
    "    }\n",
    "  ],\n",
    "  'dimensions': {\n",
    "    'dpi': 200,\n",
    "    'height': 2200,\n",
    "    'width': 1700\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This structured format enables precise extraction, spatial understanding, and seamless integration with downstream AI pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries for OCR processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These helper functions support image processing, model invocation, and post-response processing for the Mistral OCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_local_file_base64(file_path: str, file_type: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Encode a local file (image or PDF) to base64 string.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the local file\n",
    "        file_type: Type of file ('image' or 'pdf'). If None, inferred from extension.\n",
    "    \n",
    "    Returns:\n",
    "        Base64 encoded string of the file\n",
    "    \"\"\"\n",
    "    if file_type is None:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            file_type = \"pdf\"\n",
    "        elif ext in (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\"):\n",
    "            file_type = \"image\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type from extension: {ext}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            encoded_data = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "            return encoded_data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to encode {file_type} at {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_inference(client, endpoint_name: str, payload: dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke the SageMaker endpoint for OCR inference.\n",
    "    \n",
    "    Args:\n",
    "        client: SageMaker runtime client\n",
    "        endpoint_name: Name of the deployed endpoint\n",
    "        payload: JSON payload containing the image data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing parsed OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inference_out = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        inference_resp_str = inference_out[\"Body\"].read().decode(\"utf-8\")\n",
    "        return json.loads(inference_resp_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Replace image placeholders in markdown with base64-encoded images.\n",
    "    \n",
    "    Args:\n",
    "        markdown_str: Markdown string with image placeholders\n",
    "        images_dict: Dictionary mapping image names to base64 strings\n",
    "    \n",
    "    Returns:\n",
    "        Markdown string with embedded base64 images\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(\n",
    "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
    "        )\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: dict) -> str:\n",
    "    \"\"\"\n",
    "    Combine OCR text and images into a single markdown document.\n",
    "    \n",
    "    Args:\n",
    "        ocr_response: Response dictionary from OCR model\n",
    "    \n",
    "    Returns:\n",
    "        Combined markdown string with embedded images\n",
    "    \"\"\"\n",
    "    markdowns = []\n",
    "    for page in ocr_response[\"pages\"]:\n",
    "        image_data = {img[\"id\"]: img[\"image_base64\"] for img in page.get(\"images\", [])}\n",
    "        markdown_with_images = replace_images_in_markdown(page[\"markdown\"], image_data)\n",
    "        markdowns.append(markdown_with_images)\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the workshop, use pre-built Mistral OCR endpoint for inference\n",
    "\n",
    "def run_inference_with_api(api_endpoint: str, payload: dict[str, Any], timeout: int = 300) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke the API Gateway endpoint for OCR inference using HTTP requests.\n",
    "    \n",
    "    Args:\n",
    "      api_endpoint: URL of the API Gateway endpoint\n",
    "      payload: JSON payload containing the image data\n",
    "      timeout: Request timeout in seconds (default: 300)\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary containing parsed OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "      response = requests.post(\n",
    "          api_endpoint,\n",
    "          json=payload,\n",
    "          headers={\"Content-Type\": \"application/json\"},\n",
    "          timeout=timeout\n",
    "      )\n",
    "\n",
    "      # Raise an exception for bad status codes\n",
    "      response.raise_for_status()\n",
    "\n",
    "      # Parse and return the JSON response\n",
    "      return response.json()\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timeout after {timeout} seconds\")\n",
    "        raise\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response status: {e.response.status_code}\")\n",
    "            print(f\"Response body: {e.response.text}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Mistral OCR\n",
    "\n",
    "Now let's use the Mistral OCR model to extract text from an image document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Access and Deployment  \n",
    "\n",
    "**Important Notice:**                                                                                                                                                              \n",
    "The Mistral OCR model is available from AWS Marketplace via private offer. If you want to trial this model, please contact your account manager and provide your AWS account ID to whitelist for this model access.                                                                                                                                                   Then, please follow this link to deploy the Mistral OCR model on a SageMaker endpoint:                                                                                            [Mistral OCR SageMaker Deployment Guide](https://github.com/aws-samples/mistral-on-aws/blob/main/Mistral%20OCR/Mistral-OCR-SageMaker-Deployment-example.ipynb)     \n",
    "\n",
    "\n",
    "**For Workshop Participants:**                                                                                                                                                     During the workshop, we will use a pre-created API link to access the Mistral OCR model. The link will be provided during the workshop session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your AWS account is whitelisted to the model access, and also Mistral OCR model is deployed as SageMaker endpoint, please unncomment and use below code: \n",
    "\n",
    "# MISTRAL_OCR_ENDPOINT_NAME = \"mistral-ocr-endpoint\"\n",
    "# image_b64 = encode_local_file_base64(file_path=\"images/french.png\")\n",
    "\n",
    "# # Prepare the payload for Mistral OCR model\n",
    "# payload = {\n",
    "#     \"model\": \"mistral-ocr-2505\",\n",
    "#     \"document\": {\n",
    "#         \"type\": \"image_url\",\n",
    "#         \"image_url\": f\"data:image/jpeg;base64,{image_b64}\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Create a client and invoke the endpoint\n",
    "# sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "# result_parsed = run_inference(client=sagemaker_client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=payload)\n",
    "\n",
    "# # Display final markdown content with embedded images\n",
    "# display(Markdown(get_combined_markdown(result_parsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Whiteboard/Handwriting OCR\n",
    "\n",
    "Extract text from whiteboard images or handwritten notes using Mistral OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process whiteboard image\n",
    "whiteboard_b64 = encode_local_file_base64(file_path=\"images/whiteboard.png\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "whiteboard_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/png;base64,{whiteboard_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# whiteboard_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=whiteboard_payload\n",
    "# )\n",
    "\n",
    "#\u00a0In Workshop, invoke the pre-built API endpoint \n",
    "whiteboard_result =  run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    whiteboard_payload) \n",
    "\n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Whiteboard Content:\")\n",
    "display(Markdown(get_combined_markdown(whiteboard_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Low-Resolution Handwriting Recognition\n",
    "\n",
    "Now let's test Mistral OCR's ability to handle challenging, low-quality handwritten content. This demonstrates the model's robustness even when dealing with compressed or poor-quality images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process low-resolution handwriting image\n",
    "handwriting_b64 = encode_local_file_base64(file_path=\"images/handwriting_low_res_2_resize.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "handwriting_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{handwriting_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# handwriting_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=handwriting_payload\n",
    "# )\n",
    "\n",
    "#\u00a0In Workshop, invoke the pre-built API endpoint \n",
    "handwriting_result =  run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    handwriting_payload) \n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Low-Resolution Handwriting Content:\")\n",
    "display(Markdown(get_combined_markdown(handwriting_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Invoice Processing\n",
    "\n",
    "Process invoice images to extract structured data such as vendor details, line items, totals, and dates. This is a common use case for automating accounts payable workflows and financial document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single invoice\n",
    "invoice_b64 = encode_local_file_base64(file_path=\"images/invoice_1.jpg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "invoice_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{invoice_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# invoice_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=invoice_result\n",
    "# )\n",
    "\n",
    "#\u00a0In Workshop, invoke the pre-built API endpoint \n",
    "invoice_result =  run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    invoice_payload) \n",
    "\n",
    "# Display the extracted invoice content\n",
    "print(\"Extracted Invoice Content:\")\n",
    "display(Markdown(get_combined_markdown(invoice_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Police Report OCR\n",
    "\n",
    "Extract text and structured information from police report documents. This demonstrates Mistral OCR's ability to handle official documents with mixed layouts, stamps, and form fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process police report image\n",
    "police_report_b64 = encode_local_file_base64(file_path=\"images/Sample-Police-Report.jpeg\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "police_report_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{police_report_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# police_report_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=police_report_payload\n",
    "# )\n",
    "\n",
    "# In Workshop, invoke the pre-built API endpoint \n",
    "police_report_result = run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    police_report_payload\n",
    ") \n",
    "\n",
    "# Display the extracted text\n",
    "print(\"Extracted Police Report Content:\")\n",
    "display(Markdown(get_combined_markdown(police_report_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Mathematical Formula OCR\n",
    "\n",
    "Extract mathematical expressions and equations from images and convert them to accurate LaTeX notation. Mistral OCR excels at recognizing complex mathematical formulas, including integrals, summations, fractions, Greek letters, and special symbols, and automatically converts them into properly formatted LaTeX code. This is particularly valuable for digitizing scientific papers, educational materials, and technical documentation where mathematical precision is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process mathematical formula image\n",
    "math_formula_b64 = encode_local_file_base64(file_path=\"images/math_formula.png\")\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "math_formula_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/png;base64,{math_formula_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Invoke the endpoint\n",
    "# math_formula_result = run_inference(\n",
    "#     client=sagemaker_client, \n",
    "#     endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, \n",
    "#     payload=math_formula_payload\n",
    "# )\n",
    "\n",
    "# In Workshop, invoke the pre-built API endpoint \n",
    "math_formula_result = run_inference_with_api(\n",
    "    API_ENDPOINT, \n",
    "    math_formula_payload\n",
    ") \n",
    "\n",
    "# Display the extracted mathematical content with LaTeX\n",
    "print(\"Extracted Mathematical Formula (LaTeX):\")\n",
    "display(Markdown(get_combined_markdown(math_formula_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\u00a0in Workshop, plase use below code to access the pre-built Mistral OCR endpoint\n",
    "import requests\n",
    "\n",
    "# API Gateway endpoint URL\n",
    "API_ENDPOINT = \"<will provide in workshop>\"\n",
    "\n",
    "# Encode the image (reusing the same encoded image from above)\n",
    "image_b64 = encode_local_file_base64(file_path=\"images/french.png\")\n",
    "\n",
    "# Prepare the payload for API request\n",
    "api_payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image_b64}\",\n",
    "      \n",
    "  },\n",
    "    \"include_image_base64\": True  # Add this parameter to include embeded images in the output\n",
    "}\n",
    "\n",
    "# Make POST request to API Gateway\n",
    "result_parsed =  run_inference_with_api(API_ENDPOINT, api_payload) \n",
    "\n",
    "# Display final markdown content with embedded images\n",
    "print(\"OCR Result from API Gateway:\")\n",
    "display(Markdown(get_combined_markdown(result_parsed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Understanding Pipeline with OCR and LLM\n",
    "\n",
    "Combine Mistral OCR with Pixtral Large to build an intelligent document understanding pipeline that can extract text and answer questions about the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for Bedrock Runtime\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "def bedrock_converse(system_prompt: str, messages: list, endpoint_arn: str, display_usage=False):\n",
    "    \"\"\"Invoke model using Converse API\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "\n",
    "    output_content = ''.join(\n",
    "        content['text'] for content in response['output']['message']['content']\n",
    "    )\n",
    "\n",
    "    if display_usage:\n",
    "        token_usage = response['usage']\n",
    "        print(f\"\\tLatency: {response['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content\n",
    "\n",
    "def bedrock_converse_stream(system_prompt: str, messages: list, endpoint_arn: str):\n",
    "    \"\"\"Invoke model with streaming\"\"\"\n",
    "    system = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_runtime.converse_stream(\n",
    "        modelId=endpoint_arn,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        additionalModelRequestFields={\"max_tokens\": 2000, \"temperature\": 0.3}\n",
    "    )\n",
    "    \n",
    "    stream = response.get('stream')\n",
    "    output_content = ''\n",
    "    \n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "            \n",
    "            if 'contentBlockDelta' in event:\n",
    "                text_chunk = event['contentBlockDelta']['delta']['text']\n",
    "                print(text_chunk, end=\"\")\n",
    "                output_content += text_chunk\n",
    "            \n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "            \n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']}ms\")\n",
    "    \n",
    "    return output_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_understanding_pipeline(\n",
    "    image_path: str,\n",
    "    user_prompt: str,\n",
    "    ocr_endpoint: str=None,\n",
    "    llm_endpoint: str=None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Full pipeline for document understanding from image input.\n",
    "\n",
    "    Args:\n",
    "        image_path: Local path to the document image\n",
    "        user_prompt: What insights the user wants to extract\n",
    "        ocr_endpoint: SageMaker endpoint for OCR model\n",
    "        llm_endpoint: Bedrock endpoint for document understanding LLM\n",
    "\n",
    "    Returns:\n",
    "        Model-generated response with document insights\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Encode local file using helper\n",
    "    encoded_image = encode_local_file_base64(image_path)\n",
    "\n",
    "    # payload = {\n",
    "    #     \"model\": \"mistral-ocr-2505\",\n",
    "    #     \"document\": {\n",
    "    #         \"type\": \"image_url\",\n",
    "    #         \"image_url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    "    api_payload = {\n",
    "        \"model\": \"mistral-ocr-2505\",\n",
    "        \"document\": {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{image_b64}\",\n",
    "          \n",
    "      },\n",
    "        \"include_image_base64\": False  # Add this parameter to include embeded images in the output\n",
    "    }\n",
    "\n",
    "\n",
    "    # Step 2: Run OCR model\n",
    "    print(\"Running OCR model...\")\n",
    "    # ocr_result = run_inference(client=sagemaker_client, endpoint_name=ocr_endpoint, payload=payload) #\u00a0SageMaker endpoint \n",
    "    ocr_result =  run_inference_with_api(API_ENDPOINT, api_payload) # Workshop pre-built API endpoint\n",
    "\n",
    "\n",
    "    # Step 3: Convert OCR output to Markdown\n",
    "    print(\"Formatting OCR output...\")\n",
    "    markdown_doc = get_combined_markdown(ocr_result)\n",
    "\n",
    "    print(\"----- OCR Text  -----\")\n",
    "    display(Markdown(markdown_doc))\n",
    "\n",
    "    # Step 4: Prepare LLM messages\n",
    "    system_prompt = (\n",
    "        \"You are a document understanding assistant. The user will provide structured OCR content \"\n",
    "        \"from a scanned document. Use that information to generate clear, factual insights that \"\n",
    "        \"answer the user's request.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": f\"{user_prompt}\\n\\n--- Document Content ---\\n{markdown_doc}\"}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Step 5: Call Bedrock LLM with streaming\n",
    "    print(\"Running LLM for document insights...\")\n",
    "    insights = bedrock_converse_stream(system_prompt, messages, llm_endpoint)\n",
    "\n",
    "    return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Document Summarization\n",
    "\n",
    "Let's use the pipeline to extract and summarize a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"images/french.png\"\n",
    "user_prompt = \"can you summarise this document\"\n",
    "# ocr_endpoint = MISTRAL_OCR_ENDPOINT_NAME\n",
    "llm_endpoint = \"us.mistral.pixtral-large-2502-v1:0\" # Use Pixtral Large model from Bedrock\n",
    "\n",
    "# document_understanding_pipeline(image_path, user_prompt, ocr_endpoint, llm_endpoint)\n",
    "document_understanding_pipeline(image_path=image_path, user_prompt=user_prompt, llm_endpoint=llm_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Advanced: Multi-Agent Document Intelligence with OCR + Strands Agents\n\nIn this section, we'll go beyond simple OCR extraction to build a **sophisticated multi-agent document processing system** using **Strands Agents**. This demonstrates how to orchestrate multiple specialized AI agents that work together to analyze documents, enforce compliance rules, and generate actionable insights.\n\n### What is Strands Agents?\n\n**Strands Agents** is a Python framework for building agentic AI applications with tool use capabilities. It provides:\n- **Agent Orchestration**: Coordinate multiple specialized agents in workflows\n- **Tool Functions**: Decorate Python functions with `@tool` to make them callable by agents\n- **Model Integration**: Works seamlessly with Amazon Bedrock models like Pixtral Large\n- **State Management**: Track execution across multi-step workflows\n\nLearn more: [https://github.com/strands-agents/strands-agents](https://github.com/strands-agents/strands-agents)\n\n### What We'll Build: Invoice & Document Compliance Workflow\n\nWe'll create an **intelligent document processing pipeline** that:\n1. **Extracts** text from invoices and official documents using Mistral OCR\n2. **Classifies** document types automatically (invoice, report, form, etc.)\n3. **Analyzes** documents with domain-specific agents\n4. **Validates** against compliance rules (duplicate detection, anomaly detection)\n5. **Synthesizes** findings into an executive summary with flagged issues\n\n### Real-World Use Cases\n\nThis pattern is applicable to:\n- **Accounts Payable Automation**: Process invoices, detect duplicates, flag anomalies\n- **Insurance Claims Processing**: Analyze claim documents and police reports\n- **Legal Document Review**: Extract key information from contracts and forms\n- **Compliance Auditing**: Validate documents against regulatory requirements\n- **Research Paper Analysis**: Extract equations, citations, and key findings\n\n### Multi-Agent Architecture\n\nOur workflow uses 5 specialized agents:\n\n1. **Document Triage Agent**: Classifies document type (invoice, police report, form)\n2. **Invoice Analysis Agent**: Extracts vendor, amounts, line items, checks for issues\n3. **Report Analysis Agent**: Analyzes official documents like police reports\n4. **Compliance Agent**: Validates documents against business rules\n5. **Synthesis Agent**: Generates executive summary with prioritized actions\n\n### Why Multi-Agent vs Single-Agent?\n\n| Single Agent | Multi-Agent Workflow |\n|-------------|---------------------|\n| Generic prompts | Specialized expertise per domain |\n| Limited context window | Distributed processing |\n| Hard to maintain | Modular and scalable |\n| One-size-fits-all | Tailored analysis per document type |\n\n### Workflow Steps\n\n```\n1. OCR Extraction \u2192 2. Document Triage \u2192 3. Specialized Analysis \u2192 4. Compliance Check \u2192 5. Synthesis\n```\n\nLet's build it!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Setup: Install and Import Strands Agents\n\nFirst, let's install Strands Agents and import the necessary components.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install Strands Agents (if not already installed)\n# !pip install strands-agents>=0.1.6\n\n# Import Strands components\nfrom strands import Agent\nfrom strands.models import BedrockModel\nfrom strands.tools import tool\nfrom dataclasses import dataclass, field\nfrom typing import List\nfrom datetime import datetime\n\nprint(\"\u2705 Strands Agents imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Document metadata (file paths for processing)\n",
    "# The OCR text will be extracted dynamically when needed\n",
    "\n",
    "DOCUMENTS_TO_PROCESS = {\n",
    "    \"doc_001\": {\n",
    "        \"doc_id\": \"doc_001\",\n",
    "        \"file_path\": \"images/invoice_1.jpg\",\n",
    "        \"doc_type\": \"unknown\"  # Will be classified by triage agent\n",
    "    },\n",
    "    \"doc_002\": {\n",
    "        \"doc_id\": \"doc_002\", \n",
    "        \"file_path\": \"images/invoice_2.jpg\",\n",
    "        \"doc_type\": \"unknown\"\n",
    "    },\n",
    "    \"doc_003\": {\n",
    "        \"doc_id\": \"doc_003\",\n",
    "        \"file_path\": \"images/Invoice_3.jpg\",\n",
    "        \"doc_type\": \"unknown\"\n",
    "    },\n",
    "    \"doc_004\": {\n",
    "        \"doc_id\": \"doc_004\",\n",
    "        \"file_path\": \"images/Sample-Police-Report.jpeg\",\n",
    "        \"doc_type\": \"unknown\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cache for extracted OCR results (to avoid re-processing same images)\n",
    "OCR_CACHE = {}\n",
    "\n",
    "print(f\"\u2705 Document metadata created for {len(DOCUMENTS_TO_PROCESS)} documents\")\n",
    "print(\"\\nDocuments to process:\")\n",
    "for doc_id, doc in DOCUMENTS_TO_PROCESS.items():\n",
    "    print(f\"  - {doc_id}: {doc['file_path']}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@tool\n",
    "def extract_document_text(doc_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a document using Mistral OCR.\n",
    "    Caches results to avoid re-processing.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Document ID to extract text from\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with document details and extracted OCR text\n",
    "    \"\"\"\n",
    "    if doc_id not in DOCUMENTS_TO_PROCESS:\n",
    "        return json.dumps({\"error\": f\"Document {doc_id} not found\"})\n",
    "    \n",
    "    # Check cache first\n",
    "    if doc_id in OCR_CACHE:\n",
    "        print(f\"  [Using cached OCR for {doc_id}]\")\n",
    "        return json.dumps(OCR_CACHE[doc_id], indent=2)\n",
    "    \n",
    "    # Extract from document\n",
    "    doc_info = DOCUMENTS_TO_PROCESS[doc_id]\n",
    "    file_path = doc_info['file_path']\n",
    "    \n",
    "    print(f\"  [Running OCR on {file_path}...]\")\n",
    "    \n",
    "    try:\n",
    "        # Encode image\n",
    "        image_b64 = encode_local_file_base64(file_path=file_path)\n",
    "        \n",
    "        # Prepare OCR payload\n",
    "        ocr_payload = {\n",
    "            \"model\": \"mistral-ocr-2505\",\n",
    "            \"document\": {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": f\"data:image/jpeg;base64,{image_b64}\"\n",
    "            },\n",
    "            \"include_image_base64\": False\n",
    "        }\n",
    "        \n",
    "        # Call OCR endpoint\n",
    "        ocr_result = run_inference_with_api(API_ENDPOINT, ocr_payload)\n",
    "        \n",
    "        # Extract markdown text\n",
    "        ocr_text = get_combined_markdown(ocr_result)\n",
    "        \n",
    "        # Cache result\n",
    "        result = {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"file_path\": file_path,\n",
    "            \"ocr_text\": ocr_text,\n",
    "            \"doc_type\": doc_info['doc_type']\n",
    "        }\n",
    "        OCR_CACHE[doc_id] = result\n",
    "        \n",
    "        print(f\"  [OCR complete: {len(ocr_text)} characters extracted]\")\n",
    "        return json.dumps(result, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"OCR extraction failed: {str(e)}\"})\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_documents(doc_type: str = None) -> str:\n",
    "    \"\"\"\n",
    "    List all documents available for processing.\n",
    "    \n",
    "    Args:\n",
    "        doc_type: Filter by document type (optional)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with list of documents\n",
    "    \"\"\"\n",
    "    docs = list(DOCUMENTS_TO_PROCESS.values())\n",
    "    \n",
    "    if doc_type and doc_type != \"unknown\":\n",
    "        docs = [d for d in docs if d['doc_type'] == doc_type]\n",
    "    \n",
    "    result = [{\"doc_id\": d['doc_id'], \"file_path\": d['file_path'], \"doc_type\": d['doc_type']}\n",
    "              for d in docs]\n",
    "    \n",
    "    return json.dumps(result, indent=2)\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_duplicate_invoice(doc_id: str, vendor: str, amount: float, date: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if an invoice might be a duplicate by comparing with other processed invoices.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Current document ID (to exclude from comparison)\n",
    "        vendor: Vendor name\n",
    "        amount: Invoice amount\n",
    "        date: Invoice date\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with duplicate check results\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    # Check all cached documents\n",
    "    for cached_doc_id, cached_doc in OCR_CACHE.items():\n",
    "        if cached_doc_id == doc_id:\n",
    "            continue  # Skip self\n",
    "            \n",
    "        # Simple heuristic: check if vendor and amount appear in OCR text\n",
    "        ocr_text = cached_doc.get('ocr_text', '').lower()\n",
    "        \n",
    "        if (vendor.lower() in ocr_text and \n",
    "            str(amount) in ocr_text and\n",
    "            date in ocr_text):\n",
    "            matches.append({\n",
    "                \"doc_id\": cached_doc_id,\n",
    "                \"file_path\": cached_doc.get('file_path'),\n",
    "                \"match_reason\": \"Vendor, amount, and date found in document\"\n",
    "            })\n",
    "    \n",
    "    result = {\n",
    "        \"is_duplicate\": len(matches) > 0,\n",
    "        \"match_count\": len(matches),\n",
    "        \"matches\": matches,\n",
    "        \"note\": \"This is a heuristic check based on OCR text content\"\n",
    "    }\n",
    "    \n",
    "    return json.dumps(result, indent=2)\n",
    "\n",
    "\n",
    "print(\"\u2705 Tool functions defined:\")\n",
    "print(\"  - extract_document_text(doc_id) - Calls OCR API dynamically\")\n",
    "print(\"  - list_documents(doc_type)\")\n",
    "print(\"  - check_duplicate_invoice(doc_id, vendor, amount, date)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Bedrock model for agents\n",
    "bedrock_model_agents = BedrockModel(\n",
    "    model_id=\"us.mistral.pixtral-large-2502-v1:0\",\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "def create_triage_agent():\n",
    "    \"\"\"Agent that classifies document types\"\"\"\n",
    "    system_prompt = \"\"\"You are a document classification specialist. \n",
    "Analyze document content and classify it as: invoice, police_report, contract, form, or other.\n",
    "Provide brief reasoning for your classification.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[extract_document_text], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_invoice_analyst_agent():\n",
    "    \"\"\"Agent specialized in invoice analysis\"\"\"\n",
    "    system_prompt = \"\"\"You are an invoice analysis specialist. Your role:\n",
    "1. Extract key information (vendor, invoice number, amounts, dates)\n",
    "2. Validate invoice structure and completeness\n",
    "3. Check for anomalies (unusual amounts, missing fields, calculation errors)\n",
    "4. Use check_duplicate_invoice to detect potential duplicates\n",
    "5. Flag potential issues for review\n",
    "\n",
    "Be thorough and flag anything suspicious.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, \n",
    "                 tools=[extract_document_text, check_duplicate_invoice], \n",
    "                 system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_report_analyst_agent():\n",
    "    \"\"\"Agent specialized in official report analysis\"\"\"\n",
    "    system_prompt = \"\"\"You are an official document analyst specializing in reports.\n",
    "Extract key information such as:\n",
    "- Report numbers and dates\n",
    "- Incident types and locations  \n",
    "- Key parties involved\n",
    "- Financial impacts\n",
    "- Status and follow-up actions\n",
    "\n",
    "Provide structured summaries.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[extract_document_text], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_compliance_agent():\n",
    "    \"\"\"Agent that validates compliance rules\"\"\"\n",
    "    system_prompt = \"\"\"You are a compliance validation specialist. Check documents for:\n",
    "1. Duplicate submissions (use check_duplicate_invoice tool)\n",
    "2. Missing required fields\n",
    "3. Amounts exceeding thresholds ($5,000+)\n",
    "4. Date inconsistencies\n",
    "5. Policy violations\n",
    "\n",
    "Assign risk levels: LOW, MEDIUM, HIGH, CRITICAL.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, \n",
    "                 tools=[list_documents, extract_document_text, check_duplicate_invoice], \n",
    "                 system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_synthesis_agent():\n",
    "    \"\"\"Agent that synthesizes findings\"\"\"\n",
    "    system_prompt = \"\"\"You are an executive synthesis specialist. Create:\n",
    "1. Executive Summary\n",
    "2. Key Findings (prioritized by risk)\n",
    "3. Flagged Issues requiring immediate attention\n",
    "4. Recommended Actions\n",
    "\n",
    "Be concise and actionable.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "print(\"\u2705 Specialized agents created:\")\n",
    "print(\"  - Triage Agent\")\n",
    "print(\"  - Invoice Analyst Agent\")\n",
    "print(\"  - Report Analyst Agent\")\n",
    "print(\"  - Compliance Agent\")\n",
    "print(\"  - Synthesis Agent\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock model for agents\n",
    "bedrock_model_agents = BedrockModel(\n",
    "    model_id=\"us.mistral.pixtral-large-2502-v1:0\",\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "def create_triage_agent():\n",
    "    \"\"\"Agent that classifies document types\"\"\"\n",
    "    system_prompt = \"\"\"You are a document classification specialist. \n",
    "Analyze document content and classify it as: invoice, police_report, contract, form, or other.\n",
    "Provide brief reasoning for your classification.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[get_document], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_invoice_analyst_agent():\n",
    "    \"\"\"Agent specialized in invoice analysis\"\"\"\n",
    "    system_prompt = \"\"\"You are an invoice analysis specialist. Your role:\n",
    "1. Extract key information (vendor, invoice number, amounts, dates)\n",
    "2. Validate invoice structure and completeness\n",
    "3. Check for anomalies (unusual amounts, missing fields, calculation errors)\n",
    "4. Flag potential issues for review\n",
    "\n",
    "Be thorough and flag anything suspicious.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, \n",
    "                 tools=[get_document, check_duplicate_invoice], \n",
    "                 system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_report_analyst_agent():\n",
    "    \"\"\"Agent specialized in official report analysis\"\"\"\n",
    "    system_prompt = \"\"\"You are an official document analyst specializing in reports.\n",
    "Extract key information such as:\n",
    "- Report numbers and dates\n",
    "- Incident types and locations\n",
    "- Key parties involved\n",
    "- Financial impacts\n",
    "- Status and follow-up actions\n",
    "\n",
    "Provide structured summaries.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[get_document], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_compliance_agent():\n",
    "    \"\"\"Agent that validates compliance rules\"\"\"\n",
    "    system_prompt = \"\"\"You are a compliance validation specialist. Check documents for:\n",
    "1. Duplicate submissions\n",
    "2. Missing required fields\n",
    "3. Amounts exceeding thresholds ($5,000+)\n",
    "4. Date inconsistencies\n",
    "5. Policy violations\n",
    "\n",
    "Assign risk levels: LOW, MEDIUM, HIGH, CRITICAL.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, \n",
    "                 tools=[list_documents, get_document, check_duplicate_invoice], \n",
    "                 system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "def create_synthesis_agent():\n",
    "    \"\"\"Agent that synthesizes findings\"\"\"\n",
    "    system_prompt = \"\"\"You are an executive synthesis specialist. Create:\n",
    "1. Executive Summary\n",
    "2. Key Findings (prioritized by risk)\n",
    "3. Flagged Issues requiring immediate attention\n",
    "4. Recommended Actions\n",
    "\n",
    "Be concise and actionable.\"\"\"\n",
    "    \n",
    "    agent = Agent(model=bedrock_model_agents, tools=[], system_prompt=system_prompt)\n",
    "    return agent\n",
    "\n",
    "print(\"\u2705 Specialized agents created:\")\n",
    "print(\"  - Triage Agent\")\n",
    "print(\"  - Invoice Analyst Agent\")\n",
    "print(\"  - Report Analyst Agent\")\n",
    "print(\"  - Compliance Agent\")\n",
    "print(\"  - Synthesis Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow State Management\n",
    "\n",
    "Define the state object to track the multi-agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentWorkflowState:\n",
    "    \"\"\"State object for document processing workflow\"\"\"\n",
    "    document_ids: List[str]\n",
    "    \n",
    "    # Classification results\n",
    "    document_classifications: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    # Analysis results\n",
    "    invoice_analysis: Dict[str, Any] = field(default_factory=dict)\n",
    "    report_analysis: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    # Compliance results\n",
    "    compliance_findings: List[Dict] = field(default_factory=list)\n",
    "    flagged_issues: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    # Final output\n",
    "    executive_summary: str = \"\"\n",
    "    \n",
    "    # Tracking\n",
    "    agents_invoked: List[str] = field(default_factory=list)\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "print(\"\u2705 Workflow state management ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_document_workflow(document_ids: List[str]) -> DocumentWorkflowState:\n",
    "    \"\"\"\n",
    "    Orchestrate multi-agent document processing workflow.\n",
    "    \n",
    "    Workflow Steps:\n",
    "    1. Triage - Classify each document (calls OCR)\n",
    "    2. Specialized Analysis - Route to appropriate analyst\n",
    "    3. Compliance Check - Validate against rules\n",
    "    4. Synthesis - Generate executive summary\n",
    "    \n",
    "    Args:\n",
    "        document_ids: List of document IDs to process\n",
    "        \n",
    "    Returns:\n",
    "        DocumentWorkflowState with complete analysis\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    state = DocumentWorkflowState(document_ids=document_ids)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"\ud83d\udcc4 DOCUMENT PROCESSING WORKFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Processing {len(document_ids)} documents\\n\")\n",
    "    \n",
    "    # STEP 1: Triage and Classification\n",
    "    print(\"\\n\ud83d\udccb STEP 1: Document Triage & Classification\")\n",
    "    print(\"-\"*80)\n",
    "    triage_agent = create_triage_agent()\n",
    "    state.agents_invoked.append(\"triage\")\n",
    "    \n",
    "    for doc_id in document_ids:\n",
    "        query = f\"Classify the document type for document {doc_id}. Use extract_document_text to retrieve and analyze it.\"\n",
    "        response = triage_agent(query)\n",
    "        response_str = str(response)\n",
    "        \n",
    "        # Determine classification\n",
    "        if \"invoice\" in response_str.lower():\n",
    "            doc_type = \"invoice\"\n",
    "        elif \"police\" in response_str.lower() or \"report\" in response_str.lower():\n",
    "            doc_type = \"police_report\"\n",
    "        else:\n",
    "            doc_type = \"other\"\n",
    "        \n",
    "        state.document_classifications[doc_id] = doc_type\n",
    "        print(f\"  \u2713 {doc_id}: {doc_type}\")\n",
    "    \n",
    "    # STEP 2: Specialized Analysis\n",
    "    print(\"\\n\\n\ud83d\udd0d STEP 2: Specialized Analysis\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    invoice_agent = create_invoice_analyst_agent()\n",
    "    report_agent = create_report_analyst_agent()\n",
    "    \n",
    "    for doc_id, doc_type in state.document_classifications.items():\n",
    "        if doc_type == \"invoice\":\n",
    "            print(f\"\\n  Analyzing invoice {doc_id}...\")\n",
    "            query = f\"Analyze invoice {doc_id}. Extract all details, check for anomalies, and use check_duplicate_invoice to detect duplicates.\"\n",
    "            response = invoice_agent(query)\n",
    "            state.invoice_analysis[doc_id] = str(response)\n",
    "            state.agents_invoked.append(f\"invoice_analyst_{doc_id}\")\n",
    "            print(f\"  \u2713 Invoice analysis complete\")\n",
    "            \n",
    "        elif doc_type == \"police_report\":\n",
    "            print(f\"\\n  Analyzing police report {doc_id}...\")\n",
    "            query = f\"Analyze police report {doc_id}. Extract key incident details.\"\n",
    "            response = report_agent(query)\n",
    "            state.report_analysis[doc_id] = str(response)\n",
    "            state.agents_invoked.append(f\"report_analyst_{doc_id}\")\n",
    "            print(f\"  \u2713 Report analysis complete\")\n",
    "    \n",
    "    # STEP 3: Compliance Check\n",
    "    print(\"\\n\\n\ud83d\udee1\ufe0f  STEP 3: Compliance Validation\")\n",
    "    print(\"-\"*80)\n",
    "    compliance_agent = create_compliance_agent()\n",
    "    state.agents_invoked.append(\"compliance\")\n",
    "    \n",
    "    query = \"Review all processed invoices for compliance issues: duplicates, missing fields, amounts >$5000, anomalies. List all documents first.\"\n",
    "    response = compliance_agent(query)\n",
    "    state.compliance_findings = [str(response)]\n",
    "    print(f\"  \u2713 Compliance check complete\")\n",
    "    \n",
    "    # STEP 4: Synthesis\n",
    "    print(\"\\n\\n\ud83d\udca1 STEP 4: Executive Synthesis\")\n",
    "    print(\"-\"*80)\n",
    "    synthesis_agent = create_synthesis_agent()\n",
    "    state.agents_invoked.append(\"synthesis\")\n",
    "    \n",
    "    synthesis_context = f\"\"\"Based on document processing results:\n",
    "\n",
    "DOCUMENTS PROCESSED: {len(document_ids)}\n",
    "CLASSIFICATIONS: {state.document_classifications}\n",
    "\n",
    "INVOICE ANALYSIS:\n",
    "{state.invoice_analysis}\n",
    "\n",
    "REPORT ANALYSIS:\n",
    "{state.report_analysis}\n",
    "\n",
    "COMPLIANCE FINDINGS:\n",
    "{state.compliance_findings}\n",
    "\n",
    "Create an executive summary with key findings and recommended actions.\"\"\"\n",
    "    \n",
    "    response = synthesis_agent(synthesis_context)\n",
    "    state.executive_summary = str(response)\n",
    "    \n",
    "    state.execution_time = time.time() - start_time\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\udcdd EXECUTIVE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(state.executive_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\udcca WORKFLOW SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Documents Processed: {len(document_ids)}\")\n",
    "    print(f\"Agents Invoked: {len(state.agents_invoked)}\")\n",
    "    print(f\"Total Execution Time: {state.execution_time:.2f}s\")\n",
    "    print(f\"OCR Cache Entries: {len(OCR_CACHE)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"\u2705 Workflow orchestrator ready!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_document_workflow(document_ids: List[str]) -> DocumentWorkflowState:\n",
    "    \"\"\"\n",
    "    Orchestrate multi-agent document processing workflow.\n",
    "    \n",
    "    Workflow Steps:\n",
    "    1. Triage - Classify each document\n",
    "    2. Specialized Analysis - Route to appropriate analyst\n",
    "    3. Compliance Check - Validate against rules\n",
    "    4. Synthesis - Generate executive summary\n",
    "    \n",
    "    Args:\n",
    "        document_ids: List of document IDs to process\n",
    "        \n",
    "    Returns:\n",
    "        DocumentWorkflowState with complete analysis\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    state = DocumentWorkflowState(document_ids=document_ids)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"\ud83d\udcc4 DOCUMENT PROCESSING WORKFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Processing {len(document_ids)} documents\\n\")\n",
    "    \n",
    "    # STEP 1: Triage and Classification\n",
    "    print(\"\\n\ud83d\udccb STEP 1: Document Triage & Classification\")\n",
    "    print(\"-\"*80)\n",
    "    triage_agent = create_triage_agent()\n",
    "    state.agents_invoked.append(\"triage\")\n",
    "    \n",
    "    for doc_id in document_ids:\n",
    "        query = f\"Classify the document type for document {doc_id}. Use get_document to retrieve it.\"\n",
    "        response = triage_agent(query)\n",
    "        response_str = str(response)\n",
    "        \n",
    "        # Determine classification\n",
    "        if \"invoice\" in response_str.lower():\n",
    "            doc_type = \"invoice\"\n",
    "        elif \"police\" in response_str.lower() or \"report\" in response_str.lower():\n",
    "            doc_type = \"police_report\"\n",
    "        else:\n",
    "            doc_type = \"other\"\n",
    "        \n",
    "        state.document_classifications[doc_id] = doc_type\n",
    "        print(f\"  \u2713 {doc_id}: {doc_type}\")\n",
    "    \n",
    "    # STEP 2: Specialized Analysis\n",
    "    print(\"\\n\\n\ud83d\udd0d STEP 2: Specialized Analysis\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    invoice_agent = create_invoice_analyst_agent()\n",
    "    report_agent = create_report_analyst_agent()\n",
    "    \n",
    "    for doc_id, doc_type in state.document_classifications.items():\n",
    "        if doc_type == \"invoice\":\n",
    "            print(f\"\\n  Analyzing invoice {doc_id}...\")\n",
    "            query = f\"Analyze invoice {doc_id}. Extract all details and check for duplicates.\"\n",
    "            response = invoice_agent(query)\n",
    "            state.invoice_analysis[doc_id] = str(response)\n",
    "            state.agents_invoked.append(f\"invoice_analyst_{doc_id}\")\n",
    "            print(f\"  \u2713 Invoice analysis complete ({len(str(response))} chars)\")\n",
    "            \n",
    "        elif doc_type == \"police_report\":\n",
    "            print(f\"\\n  Analyzing police report {doc_id}...\")\n",
    "            query = f\"Analyze police report {doc_id}. Extract key incident details.\"\n",
    "            response = report_agent(query)\n",
    "            state.report_analysis[doc_id] = str(response)\n",
    "            state.agents_invoked.append(f\"report_analyst_{doc_id}\")\n",
    "            print(f\"  \u2713 Report analysis complete ({len(str(response))} chars)\")\n",
    "    \n",
    "    # STEP 3: Compliance Check\n",
    "    print(\"\\n\\n\ud83d\udee1\ufe0f  STEP 3: Compliance Validation\")\n",
    "    print(\"-\"*80)\n",
    "    compliance_agent = create_compliance_agent()\n",
    "    state.agents_invoked.append(\"compliance\")\n",
    "    \n",
    "    query = \"Review all invoices for compliance issues: duplicates, missing fields, amounts >$5000, anomalies.\"\n",
    "    response = compliance_agent(query)\n",
    "    state.compliance_findings = [str(response)]\n",
    "    print(f\"  \u2713 Compliance check complete ({len(str(response))} chars)\")\n",
    "    \n",
    "    # STEP 4: Synthesis\n",
    "    print(\"\\n\\n\ud83d\udca1 STEP 4: Executive Synthesis\")\n",
    "    print(\"-\"*80)\n",
    "    synthesis_agent = create_synthesis_agent()\n",
    "    state.agents_invoked.append(\"synthesis\")\n",
    "    \n",
    "    synthesis_context = f\"\"\"Based on document processing results:\n",
    "\n",
    "DOCUMENTS PROCESSED: {len(document_ids)}\n",
    "CLASSIFICATIONS: {state.document_classifications}\n",
    "\n",
    "INVOICE ANALYSIS:\n",
    "{state.invoice_analysis}\n",
    "\n",
    "REPORT ANALYSIS:\n",
    "{state.report_analysis}\n",
    "\n",
    "COMPLIANCE FINDINGS:\n",
    "{state.compliance_findings}\n",
    "\n",
    "Create an executive summary with key findings and recommended actions.\"\"\"\n",
    "    \n",
    "    response = synthesis_agent(synthesis_context)\n",
    "    state.executive_summary = str(response)\n",
    "    \n",
    "    state.execution_time = time.time() - start_time\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\udcdd EXECUTIVE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(state.executive_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\udcca WORKFLOW SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Documents Processed: {len(document_ids)}\")\n",
    "    print(f\"Agents Invoked: {len(state.agents_invoked)}\")\n",
    "    print(f\"Total Execution Time: {state.execution_time:.2f}s\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"\u2705 Workflow orchestrator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Process Multiple Documents\n",
    "\n",
    "Run the complete workflow to process invoices and police reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow on all documents (3 invoices + 1 police report)\n",
    "document_ids = [\"doc_001\", \"doc_002\", \"doc_003\", \"doc_004\"]\n",
    "\n",
    "result = run_document_workflow(document_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
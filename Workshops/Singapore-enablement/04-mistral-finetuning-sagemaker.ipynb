{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Mistral Models on Amazon SageMaker\n",
    "\n",
    "Welcome to this comprehensive guide on fine-tuning Mistral AI models using Amazon SageMaker! This notebook will walk you through the entire process of customizing a powerful language model for your specific use case.\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction to Mistral Models](#intro)\n",
    "2. [Why Fine-tune?](#why-finetune)\n",
    "3. [Understanding LoRA](#lora)\n",
    "4. [Dataset Preparation](#data-prep)\n",
    "5. [Training Configuration](#training-config)\n",
    "6. [Model Deployment](#deployment)\n",
    "7. [Testing & Evaluation](#testing)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "- **Mistral Model Architecture**: What makes Mistral models powerful and efficient\n",
    "- **Fine-tuning Fundamentals**: When and why to fine-tune vs. using base models\n",
    "- **LoRA (Low-Rank Adaptation)**: An efficient fine-tuning technique that saves time and money\n",
    "- **Training Parameters**: What each hyperparameter does and how to tune them\n",
    "- **SageMaker Training Jobs**: How to leverage AWS infrastructure for ML training\n",
    "- **Model Deployment**: Best practices for serving your fine-tuned model\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and Jupyter notebooks\n",
    "- AWS account with SageMaker access\n",
    "- Completed previous workshop notebooks (01-03) recommended\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "\n",
    "- **Setup**: 5 minutes\n",
    "- **Training**: 15-30 minutes (depending on dataset size)\n",
    "- **Deployment & Testing**: 10 minutes\n",
    "- **Total**: ~45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Mistral Models\n",
    "\n",
    "## ü§ñ What is Mistral?\n",
    "\n",
    "**Mistral AI** is a French AI company that has developed a family of high-performance, open-source large language models. Their models are known for:\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "1. **Efficiency**: Mistral models achieve state-of-the-art performance with fewer parameters than competitors\n",
    "   - Mistral 7B outperforms Llama 2 13B on most benchmarks\n",
    "   - Uses Grouped-Query Attention (GQA) for faster inference\n",
    "   - Sliding Window Attention for handling longer contexts efficiently\n",
    "\n",
    "2. **Open Source**: Released under Apache 2.0 license\n",
    "   - Free for commercial use\n",
    "   - Full model weights available\n",
    "   - Active community support\n",
    "\n",
    "3. **Versatility**: Excellent across multiple tasks\n",
    "   - Text generation and completion\n",
    "   - Question answering\n",
    "   - Code generation\n",
    "   - Instruction following\n",
    "   - Multilingual capabilities\n",
    "\n",
    "### Model Variants:\n",
    "\n",
    "| Model | Parameters | Context Length | Best For |\n",
    "|-------|-----------|----------------|----------|\n",
    "| Mistral 7B | 7.3B | 8K tokens | General purpose, fast inference |\n",
    "| Mistral 7B Instruct | 7.3B | 8K tokens | Chat, instruction following |\n",
    "| Mixtral 8x7B | 46.7B (12.9B active) | 32K tokens | Complex reasoning, multilingual |\n",
    "| Mistral Small | 24B | 32K tokens | Balanced performance/cost |\n",
    "\n",
    "In this notebook, we'll use **Mistral 7B Instruct v0.3**, which is optimized for instruction-following tasks.\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2: Why Fine-tune?\n",
    "\n",
    "## üéì Understanding Fine-tuning\n",
    "\n",
    "**Fine-tuning** is the process of taking a pre-trained model and further training it on your specific dataset to adapt it to your use case.\n",
    "\n",
    "### When to Fine-tune vs. Use Base Model:\n",
    "\n",
    "#### ‚úÖ Fine-tune When:\n",
    "\n",
    "1. **Domain-Specific Language**: Your use case involves specialized terminology\n",
    "   - Medical, legal, financial, technical documentation\n",
    "   - Company-specific jargon or processes\n",
    "\n",
    "2. **Consistent Style/Tone**: You need predictable output formatting\n",
    "   - Customer service responses with specific tone\n",
    "   - Technical documentation with consistent structure\n",
    "   - Brand voice alignment\n",
    "\n",
    "3. **Improved Accuracy**: Base model doesn't perform well on your task\n",
    "   - Specialized classification tasks\n",
    "   - Domain-specific question answering\n",
    "   - Custom entity recognition\n",
    "\n",
    "4. **Cost Optimization**: Smaller fine-tuned models can replace larger base models\n",
    "   - Fine-tuned 7B model may match 70B model performance on specific tasks\n",
    "   - Lower inference costs\n",
    "   - Faster response times\n",
    "\n",
    "#### ‚ùå Don't Fine-tune When:\n",
    "\n",
    "1. **Limited Data**: You have fewer than 100-200 quality examples\n",
    "2. **General Tasks**: Base model already performs well\n",
    "3. **Rapidly Changing Requirements**: Your use case changes frequently\n",
    "4. **Prompt Engineering Works**: You can achieve good results with clever prompts\n",
    "\n",
    "### Benefits of Fine-tuning Mistral Models:\n",
    "\n",
    "| Benefit | Description | Impact |\n",
    "|---------|-------------|--------|\n",
    "| **Performance** | Higher accuracy on domain-specific tasks | 20-40% improvement |\n",
    "| **Consistency** | More predictable outputs | Reduced variance |\n",
    "| **Efficiency** | Shorter prompts needed | 50-70% token savings |\n",
    "| **Cost** | Smaller model can replace larger one | 5-10x cost reduction |\n",
    "| **Latency** | Faster inference with optimized model | 2-3x speed improvement |\n",
    "| **Privacy** | Keep sensitive data in training, not prompts | Enhanced security |\n",
    "\n",
    "### Real-World Example:\n",
    "\n",
    "**Scenario**: Customer support chatbot for a SaaS company\n",
    "\n",
    "- **Before Fine-tuning**: \n",
    "  - Using GPT-4 with long prompts containing company policies\n",
    "  - Cost: $0.03 per interaction\n",
    "  - Response time: 3-5 seconds\n",
    "  - Accuracy: 75% (sometimes gives generic answers)\n",
    "\n",
    "- **After Fine-tuning Mistral 7B**:\n",
    "  - Fine-tuned on 1,000 support conversations\n",
    "  - Cost: $0.002 per interaction (15x cheaper)\n",
    "  - Response time: 0.5-1 second (5x faster)\n",
    "  - Accuracy: 92% (company-specific knowledge embedded)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -Uq sagemaker boto3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Understanding LoRA (Low-Rank Adaptation)\n",
    "\n",
    "## üî¨ What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique that dramatically reduces the computational and memory requirements of fine-tuning large language models.\n",
    "\n",
    "### Traditional Fine-tuning vs. LoRA:\n",
    "\n",
    "#### Traditional Full Fine-tuning:\n",
    "```\n",
    "Original Model: 7B parameters\n",
    "Training: Updates ALL 7B parameters\n",
    "Memory Required: ~28GB (for model weights)\n",
    "Training Time: 10-20 hours\n",
    "Storage: 14GB (full model copy)\n",
    "```\n",
    "\n",
    "#### LoRA Fine-tuning:\n",
    "```\n",
    "Original Model: 7B parameters (frozen)\n",
    "Training: Updates only ~4-8M parameters (0.05%!)\n",
    "Memory Required: ~12GB\n",
    "Training Time: 1-3 hours\n",
    "Storage: 16MB (just the LoRA adapters)\n",
    "```\n",
    "\n",
    "### How LoRA Works:\n",
    "\n",
    "Instead of modifying the original model weights, LoRA:\n",
    "\n",
    "1. **Freezes** the original pre-trained weights\n",
    "2. **Injects** trainable low-rank matrices into each layer\n",
    "3. **Trains** only these small adapter matrices\n",
    "4. **Combines** the frozen weights with adapters during inference\n",
    "\n",
    "```python\n",
    "# Mathematical representation:\n",
    "# Original: h = W‚ÇÄx\n",
    "# LoRA: h = W‚ÇÄx + BAx\n",
    "# Where:\n",
    "#   W‚ÇÄ = frozen pre-trained weights (large)\n",
    "#   B, A = trainable low-rank matrices (small)\n",
    "#   BA = the adapter (rank r << model dimension)\n",
    "```\n",
    "\n",
    "### Key LoRA Parameters:\n",
    "\n",
    "#### 1. **Rank (r)**\n",
    "- **What it is**: Dimensionality of the low-rank matrices\n",
    "- **Typical values**: 4, 8, 16, 32, 64\n",
    "- **Trade-off**:\n",
    "  - Lower rank (4-8): Faster, less memory, fewer parameters, may underfit\n",
    "  - Higher rank (32-64): Slower, more memory, more parameters, better capacity\n",
    "- **Recommendation**: Start with 16, increase if underfitting\n",
    "\n",
    "#### 2. **Alpha (Œ±)**\n",
    "- **What it is**: Scaling factor for LoRA updates\n",
    "- **Typical values**: 16, 32, 64 (often 2x the rank)\n",
    "- **Formula**: `scaling = alpha / rank`\n",
    "- **Effect**: Controls how much the adapters influence the output\n",
    "- **Recommendation**: Set to 2x your rank value\n",
    "\n",
    "#### 3. **Target Modules**\n",
    "- **What it is**: Which layers to apply LoRA to\n",
    "- **Common choices**:\n",
    "  - `[\"q_proj\", \"v_proj\"]`: Attention query and value (minimal)\n",
    "  - `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`: All attention (recommended)\n",
    "  - `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]`: Attention + FFN (maximum)\n",
    "- **Trade-off**: More modules = better adaptation but slower training\n",
    "\n",
    "#### 4. **Dropout**\n",
    "- **What it is**: Regularization to prevent overfitting\n",
    "- **Typical values**: 0.05, 0.1\n",
    "- **Effect**: Randomly drops connections during training\n",
    "- **Recommendation**: 0.05 for large datasets, 0.1 for small datasets\n",
    "\n",
    "### LoRA Configuration Example:\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank: balance between capacity and efficiency\n",
    "    lora_alpha=32,          # Alpha: 2x rank for stable training\n",
    "    target_modules=[        # Apply to all attention layers\n",
    "        \"q_proj\",           # Query projection\n",
    "        \"k_proj\",           # Key projection  \n",
    "        \"v_proj\",           # Value projection\n",
    "        \"o_proj\"            # Output projection\n",
    "    ],\n",
    "    lora_dropout=0.05,      # Light regularization\n",
    "    bias=\"none\",            # Don't train bias terms\n",
    "    task_type=\"CAUSAL_LM\"   # Causal language modeling\n",
    ")\n",
    "```\n",
    "\n",
    "### Benefits of LoRA:\n",
    "\n",
    "| Aspect | Benefit | Quantified |\n",
    "|--------|---------|------------|\n",
    "| **Memory** | Reduced GPU memory usage | 50-70% less |\n",
    "| **Speed** | Faster training | 2-3x faster |\n",
    "| **Storage** | Tiny adapter files | 16MB vs 14GB |\n",
    "| **Flexibility** | Multiple adapters per model | Unlimited tasks |\n",
    "| **Cost** | Lower compute requirements | 60-80% cheaper |\n",
    "| **Quality** | Comparable to full fine-tuning | 95-98% performance |\n",
    "\n",
    "### When to Use LoRA:\n",
    "\n",
    "‚úÖ **Perfect for**:\n",
    "- Limited GPU memory (< 24GB)\n",
    "- Multiple task-specific models\n",
    "- Rapid experimentation\n",
    "- Production deployments (easy to swap adapters)\n",
    "\n",
    "‚ùå **Consider full fine-tuning if**:\n",
    "- You need maximum possible performance\n",
    "- You have unlimited compute budget\n",
    "- The task requires fundamental model changes\n",
    "\n",
    "---\n",
    "\n",
    "# Part 4: Dataset Preparation\n",
    "\n",
    "## üìä Preparing Your Training Data\n",
    "\n",
    "Quality data is crucial for successful fine-tuning. Let's understand the format and best practices.\n",
    "\n",
    "### Data Format: JSONL (JSON Lines)\n",
    "\n",
    "Each line is a complete JSON object representing one training example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Question or instruction\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Desired response\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Dataset Size**:\n",
    "   - Minimum: 100-200 examples\n",
    "   - Recommended: 500-1,000 examples\n",
    "   - Optimal: 1,000-10,000 examples\n",
    "   - More data = better generalization\n",
    "\n",
    "2. **Quality over Quantity**:\n",
    "   - One high-quality example > ten mediocre ones\n",
    "   - Ensure responses are accurate and consistent\n",
    "   - Remove contradictory examples\n",
    "\n",
    "3. **Diversity**:\n",
    "   - Cover different phrasings of similar questions\n",
    "   - Include edge cases\n",
    "   - Represent real-world distribution\n",
    "\n",
    "4. **Length**:\n",
    "   - Keep examples under 2048 tokens when possible\n",
    "   - Longer examples = slower training\n",
    "   - Balance between context and efficiency\n",
    "\n",
    "### Example Use Cases:\n",
    "\n",
    "**Customer Support**:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"How do I reset my password?\"}, \n",
    "              {\"role\": \"assistant\", \"content\": \"To reset your password: 1. Go to login page 2. Click 'Forgot Password' 3. Enter your email...\"}]}\n",
    "```\n",
    "\n",
    "**Code Generation**:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"Write a Python function to calculate fibonacci\"}, \n",
    "              {\"role\": \"assistant\", \"content\": \"def fibonacci(n):\\n    if n <= 1:\\n        return n...\"}]}\n",
    "```\n",
    "\n",
    "**Domain-Specific QA**:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"What is the treatment for Type 2 Diabetes?\"}, \n",
    "              {\"role\": \"assistant\", \"content\": \"Treatment for Type 2 Diabetes typically includes: 1. Lifestyle modifications...\"}]}\n",
    "```\n",
    "\n",
    "For this workshop, we'll use a simple customer support dataset. In production, you would use your own domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample training dataset\n",
    "training_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How do I reset my password?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"To reset your password, go to the login page and click 'Forgot Password'. Enter your email address and follow the instructions sent to your inbox.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What are your business hours?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Our customer support team is available Monday through Friday, 9 AM to 6 PM EST. For urgent issues outside these hours, please use our emergency hotline.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How can I track my order?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"You can track your order by logging into your account and visiting the 'Orders' section. Click on the order number to see detailed tracking information.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to JSONL format\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "with open('data/train.jsonl', 'w') as f:\n",
    "    for item in training_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"Created training dataset with {len(training_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data to S3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "training_data_uri = S3Uploader.upload(\n",
    "    local_path='data/train.jsonl',\n",
    "    desired_s3_uri=f's3://{bucket}/mistral-finetuning/data'\n",
    ")\n",
    "\n",
    "print(f\"Training data uploaded to: {training_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Training Configuration\n",
    "\n",
    "## ‚öôÔ∏è Understanding Training Parameters\n",
    "\n",
    "Let's break down each training parameter and understand what it does.\n",
    "\n",
    "### Model Loading Parameters:\n",
    "\n",
    "#### **torch_dtype=torch.bfloat16**\n",
    "- **What**: Data type for model weights\n",
    "- **Options**: float32, float16, bfloat16\n",
    "- **Why bfloat16**: \n",
    "  - 50% memory savings vs float32\n",
    "  - Better numerical stability than float16\n",
    "  - Supported by modern GPUs (A100, H100, L4)\n",
    "- **Impact**: Enables training larger models on same hardware\n",
    "\n",
    "#### **device_map=\"auto\"**\n",
    "- **What**: Automatically distributes model across available GPUs\n",
    "- **Why**: Handles models larger than single GPU memory\n",
    "- **How**: Intelligently splits layers across devices\n",
    "\n",
    "### Training Hyperparameters:\n",
    "\n",
    "#### **num_train_epochs**\n",
    "- **What**: Number of complete passes through the dataset\n",
    "- **Typical values**: 1-5 epochs\n",
    "- **Guidelines**:\n",
    "  - Large dataset (>5000 examples): 1-2 epochs\n",
    "  - Medium dataset (500-5000): 3-5 epochs\n",
    "  - Small dataset (<500): 5-10 epochs\n",
    "- **Warning**: Too many epochs = overfitting\n",
    "\n",
    "#### **per_device_train_batch_size**\n",
    "- **What**: Number of examples processed together per GPU\n",
    "- **Typical values**: 1, 2, 4, 8\n",
    "- **Trade-offs**:\n",
    "  - Larger batch: Faster training, more memory, less noise\n",
    "  - Smaller batch: Slower training, less memory, more noise (can help generalization)\n",
    "- **Memory impact**: Doubling batch size ‚âà doubles memory usage\n",
    "\n",
    "#### **gradient_accumulation_steps**\n",
    "- **What**: Accumulate gradients over N steps before updating\n",
    "- **Why**: Simulate larger batch sizes without more memory\n",
    "- **Effective batch size** = `per_device_batch_size √ó gradient_accumulation_steps √ó num_gpus`\n",
    "- **Example**: \n",
    "  - batch_size=1, accumulation=4 ‚Üí effective batch=4\n",
    "  - Same result as batch_size=4, but uses 4x less memory\n",
    "\n",
    "#### **learning_rate**\n",
    "- **What**: Step size for weight updates\n",
    "- **Typical values**: 1e-5 to 5e-4\n",
    "- **Guidelines**:\n",
    "  - Full fine-tuning: 1e-5 to 5e-5 (smaller)\n",
    "  - LoRA: 1e-4 to 5e-4 (larger, because fewer parameters)\n",
    "  - Large dataset: Lower learning rate\n",
    "  - Small dataset: Higher learning rate\n",
    "- **Too high**: Training unstable, loss explodes\n",
    "- **Too low**: Training too slow, may not converge\n",
    "\n",
    "#### **warmup_steps**\n",
    "- **What**: Gradually increase learning rate from 0 to target\n",
    "- **Why**: Prevents large updates early in training\n",
    "- **Typical values**: 10-100 steps or 5-10% of total steps\n",
    "- **Formula**: `warmup_steps = 0.1 √ó total_steps`\n",
    "\n",
    "#### **fp16 / bf16**\n",
    "- **What**: Mixed precision training\n",
    "- **Benefits**:\n",
    "  - 2x faster training\n",
    "  - 50% less memory\n",
    "  - Minimal accuracy loss\n",
    "- **Choose bf16 if available** (better for LLMs)\n",
    "\n",
    "#### **logging_steps**\n",
    "- **What**: How often to log training metrics\n",
    "- **Typical values**: 10, 50, 100\n",
    "- **Impact**: More frequent = better monitoring, but more overhead\n",
    "\n",
    "#### **save_strategy**\n",
    "- **Options**: \"no\", \"steps\", \"epoch\"\n",
    "- **\"epoch\"**: Save checkpoint after each epoch\n",
    "- **\"steps\"**: Save every N steps\n",
    "- **Recommendation**: \"epoch\" for most cases\n",
    "\n",
    "### Training Configuration Summary:\n",
    "\n",
    "```python\n",
    "TrainingArguments(\n",
    "    output_dir=\"/opt/ml/model\",              # Where to save model\n",
    "    num_train_epochs=3,                       # 3 complete passes through data\n",
    "    per_device_train_batch_size=1,           # 1 example per GPU (memory constrained)\n",
    "    gradient_accumulation_steps=4,           # Effective batch size = 4\n",
    "    learning_rate=2e-4,                      # LoRA-appropriate learning rate\n",
    "    fp16=True,                               # Mixed precision for speed\n",
    "    logging_steps=10,                        # Log every 10 steps\n",
    "    save_strategy=\"epoch\",                   # Save after each epoch\n",
    "    warmup_steps=10,                         # Warm up for 10 steps\n",
    ")\n",
    "```\n",
    "\n",
    "### Recommended Configurations by Use Case:\n",
    "\n",
    "#### **Quick Experimentation** (Fast iteration):\n",
    "```python\n",
    "epochs=1, batch_size=4, learning_rate=3e-4, lora_r=8\n",
    "# Time: ~10 minutes, Quality: 70-80%\n",
    "```\n",
    "\n",
    "#### **Balanced** (Good quality, reasonable time):\n",
    "```python\n",
    "epochs=3, batch_size=2, learning_rate=2e-4, lora_r=16\n",
    "# Time: ~30 minutes, Quality: 85-90%\n",
    "```\n",
    "\n",
    "#### **Production** (Maximum quality):\n",
    "```python\n",
    "epochs=5, batch_size=1, learning_rate=1e-4, lora_r=32\n",
    "# Time: ~60 minutes, Quality: 90-95%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Training Job\n",
    "\n",
    "Now let's create our training script with all these concepts applied. We'll use the Hugging Face DLC (Deep Learning Container) with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training script\n",
    "training_script = '''#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def train():\n",
    "    # Load model and tokenizer\n",
    "    model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Load and prepare dataset\n",
    "    dataset = load_dataset('json', data_files='/opt/ml/input/data/training/train.jsonl')\n",
    "    \n",
    "    def format_chat(example):\n",
    "        messages = example['messages']\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        return {'text': text}\n",
    "    \n",
    "    dataset = dataset.map(format_chat)\n",
    "    \n",
    "    def tokenize(example):\n",
    "        return tokenizer(example['text'], truncation=True, max_length=512)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize, remove_columns=dataset['train'].column_names)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/opt/ml/model\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(\"/opt/ml/model\")\n",
    "    tokenizer.save_pretrained(\"/opt/ml/model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "'''\n",
    "\n",
    "# Save training script\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "with open('scripts/train.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(\"Training script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements file\n",
    "requirements = '''transformers==4.36.0\n",
    "datasets==2.16.0\n",
    "peft==0.7.1\n",
    "accelerate==0.25.0\n",
    "bitsandbytes==0.41.3\n",
    "'''\n",
    "\n",
    "with open('scripts/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Requirements file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Launch Training Job\n",
    "\n",
    "## üöÄ SageMaker Training Jobs Explained\n",
    "\n",
    "### What is a SageMaker Training Job?\n",
    "\n",
    "A **SageMaker Training Job** is a managed service that:\n",
    "1. Provisions compute resources (GPU instances)\n",
    "2. Downloads your training data from S3\n",
    "3. Runs your training script\n",
    "4. Uploads the trained model back to S3\n",
    "5. Cleans up resources automatically\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Managed Infrastructure** | No server management |\n",
    "| **Auto-scaling** | Scales to your needs |\n",
    "| **Cost Optimization** | Pay only for training time |\n",
    "| **Monitoring** | Built-in CloudWatch metrics |\n",
    "| **Reproducibility** | Consistent training environment |\n",
    "| **Security** | IAM-based access control |\n",
    "\n",
    "### Instance Type Selection:\n",
    "\n",
    "#### **ml.g5.2xlarge** (Recommended for this workshop)\n",
    "- **GPU**: 1x NVIDIA A10G (24GB VRAM)\n",
    "- **vCPUs**: 8\n",
    "- **RAM**: 32GB\n",
    "- **Cost**: ~$1.52/hour\n",
    "- **Best for**: Mistral 7B with LoRA\n",
    "\n",
    "#### Other Options:\n",
    "\n",
    "| Instance | GPU | VRAM | Cost/hr | Best For |\n",
    "|----------|-----|------|---------|----------|\n",
    "| ml.g5.xlarge | 1x A10G | 24GB | $1.01 | Small models (<7B) |\n",
    "| ml.g5.4xlarge | 1x A10G | 24GB | $2.03 | Faster training |\n",
    "| ml.g5.12xlarge | 4x A10G | 96GB | $6.11 | Large models (13B-30B) |\n",
    "| ml.p4d.24xlarge | 8x A100 | 320GB | $32.77 | Massive models (70B+) |\n",
    "\n",
    "### Estimator Configuration:\n",
    "\n",
    "```python\n",
    "HuggingFace(\n",
    "    entry_point='train.py',              # Your training script\n",
    "    source_dir='scripts',                # Directory containing script\n",
    "    instance_type='ml.g5.2xlarge',       # GPU instance\n",
    "    instance_count=1,                    # Number of instances\n",
    "    role=role,                           # IAM role for permissions\n",
    "    transformers_version='4.36.0',       # Hugging Face version\n",
    "    pytorch_version='2.1.0',             # PyTorch version\n",
    "    py_version='py310',                  # Python version\n",
    "    max_run=3600,                        # Max training time (1 hour)\n",
    ")\n",
    "```\n",
    "\n",
    "### Training Time Estimates:\n",
    "\n",
    "| Dataset Size | Epochs | Instance | Time | Cost |\n",
    "|--------------|--------|----------|------|------|\n",
    "| 100 examples | 3 | ml.g5.2xlarge | ~10 min | $0.25 |\n",
    "| 500 examples | 3 | ml.g5.2xlarge | ~20 min | $0.50 |\n",
    "| 1,000 examples | 3 | ml.g5.2xlarge | ~30 min | $0.75 |\n",
    "| 5,000 examples | 3 | ml.g5.2xlarge | ~2 hours | $3.00 |\n",
    "\n",
    "Now let's create and launch the SageMaker training job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='scripts',\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.36.0',\n",
    "    pytorch_version='2.1.0',\n",
    "    py_version='py310',\n",
    "    hyperparameters={\n",
    "        'epochs': 3,\n",
    "        'train_batch_size': 1,\n",
    "    },\n",
    "    environment={\n",
    "        'HUGGINGFACE_HUB_CACHE': '/tmp/.cache',\n",
    "    },\n",
    "    max_run=3600,  # 1 hour max\n",
    ")\n",
    "\n",
    "print(\"Estimator configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "huggingface_estimator.fit({'training': training_data_uri})\n",
    "\n",
    "print(\"Training job completed!\")\n",
    "print(f\"Model artifacts: {huggingface_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Model Deployment\n",
    "\n",
    "## üåê Deploying Your Fine-tuned Model\n",
    "\n",
    "After training completes, we can deploy the fine-tuned model to a SageMaker real-time endpoint for inference.\n",
    "\n",
    "### What is a SageMaker Endpoint?\n",
    "\n",
    "A **SageMaker Endpoint** is a fully managed inference service that:\n",
    "- Hosts your model on persistent compute resources\n",
    "- Provides a REST API for predictions\n",
    "- Auto-scales based on traffic\n",
    "- Handles load balancing automatically\n",
    "- Monitors model performance\n",
    "\n",
    "### Deployment Options:\n",
    "\n",
    "#### **Real-time Endpoints** (What we're using)\n",
    "- **Use case**: Low-latency, synchronous predictions\n",
    "- **Latency**: 100-500ms\n",
    "- **Cost**: Pay for instance uptime\n",
    "- **Best for**: Chatbots, interactive applications\n",
    "\n",
    "#### **Serverless Endpoints**\n",
    "- **Use case**: Intermittent traffic\n",
    "- **Latency**: 1-5 seconds (cold start)\n",
    "- **Cost**: Pay per request\n",
    "- **Best for**: Development, low-traffic apps\n",
    "\n",
    "#### **Batch Transform**\n",
    "- **Use case**: Process large datasets offline\n",
    "- **Latency**: Minutes to hours\n",
    "- **Cost**: Pay for job duration\n",
    "- **Best for**: Bulk processing, analytics\n",
    "\n",
    "### Instance Selection for Inference:\n",
    "\n",
    "| Instance | GPU | VRAM | Cost/hr | Throughput | Best For |\n",
    "|----------|-----|------|---------|------------|----------|\n",
    "| ml.g5.xlarge | 1x A10G | 24GB | $1.01 | ~10 req/sec | Development |\n",
    "| ml.g5.2xlarge | 1x A10G | 24GB | $1.52 | ~15 req/sec | Production (low traffic) |\n",
    "| ml.g5.4xlarge | 1x A10G | 24GB | $2.03 | ~20 req/sec | Production (medium traffic) |\n",
    "| ml.g5.12xlarge | 4x A10G | 96GB | $6.11 | ~60 req/sec | Production (high traffic) |\n",
    "\n",
    "### Cost Optimization Tips:\n",
    "\n",
    "1. **Right-size your instance**: Start small, scale up if needed\n",
    "2. **Use auto-scaling**: Scale down during low traffic\n",
    "3. **Delete unused endpoints**: Stop paying when not in use\n",
    "4. **Consider Serverless**: For unpredictable traffic\n",
    "5. **Use Spot instances**: Save up to 70% (for non-critical workloads)\n",
    "\n",
    "### Deployment Configuration:\n",
    "\n",
    "```python\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,           # Start with 1 instance\n",
    "    instance_type='ml.g5.2xlarge',      # GPU instance for fast inference\n",
    "    endpoint_name='mistral-finetuned'   # Unique endpoint name\n",
    ")\n",
    "```\n",
    "\n",
    "### What Happens During Deployment:\n",
    "\n",
    "1. **Model Registration**: Model artifacts uploaded to S3\n",
    "2. **Container Creation**: Inference container configured\n",
    "3. **Instance Provisioning**: GPU instance launched\n",
    "4. **Model Loading**: Model loaded into GPU memory\n",
    "5. **Health Checks**: Endpoint tested for readiness\n",
    "6. **Endpoint Active**: Ready to serve predictions\n",
    "\n",
    "**Deployment Time**: 5-10 minutes\n",
    "\n",
    "Let's deploy our fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "predictor = huggingface_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    endpoint_name=f'mistral-finetuned-{sess.default_bucket()[:8]}'\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Testing & Evaluation\n",
    "\n",
    "## üß™ Testing Your Fine-tuned Model\n",
    "\n",
    "Now let's test our fine-tuned model and understand the inference parameters.\n",
    "\n",
    "### Inference Parameters Explained:\n",
    "\n",
    "#### **max_new_tokens**\n",
    "- **What**: Maximum number of tokens to generate\n",
    "- **Typical values**: 50-512\n",
    "- **Impact**: \n",
    "  - Higher = longer responses, more cost, slower\n",
    "  - Lower = shorter responses, less cost, faster\n",
    "- **Recommendation**: Set based on expected response length\n",
    "\n",
    "#### **temperature**\n",
    "- **What**: Controls randomness in generation\n",
    "- **Range**: 0.0 to 2.0\n",
    "- **Effects**:\n",
    "  - **0.0-0.3**: Deterministic, focused, repetitive\n",
    "  - **0.4-0.7**: Balanced, natural (recommended)\n",
    "  - **0.8-1.0**: Creative, diverse\n",
    "  - **1.1-2.0**: Very random, potentially incoherent\n",
    "- **Use cases**:\n",
    "  - Factual QA: 0.1-0.3\n",
    "  - Customer support: 0.5-0.7\n",
    "  - Creative writing: 0.8-1.2\n",
    "\n",
    "#### **top_p (nucleus sampling)**\n",
    "- **What**: Considers tokens with cumulative probability up to p\n",
    "- **Range**: 0.0 to 1.0\n",
    "- **Effects**:\n",
    "  - **0.1-0.5**: Very focused, deterministic\n",
    "  - **0.6-0.9**: Balanced (recommended)\n",
    "  - **0.95-1.0**: More diverse\n",
    "- **Tip**: Use with temperature for best results\n",
    "\n",
    "#### **top_k**\n",
    "- **What**: Considers only top k most likely tokens\n",
    "- **Typical values**: 10, 20, 50\n",
    "- **Effects**:\n",
    "  - Lower k: More focused\n",
    "  - Higher k: More diverse\n",
    "- **Note**: Often used with top_p\n",
    "\n",
    "#### **repetition_penalty**\n",
    "- **What**: Penalizes repeated tokens\n",
    "- **Range**: 1.0 to 2.0\n",
    "- **Effects**:\n",
    "  - 1.0: No penalty\n",
    "  - 1.1-1.3: Reduces repetition (recommended)\n",
    "  - 1.5+: Strongly discourages repetition\n",
    "\n",
    "### Recommended Parameter Combinations:\n",
    "\n",
    "#### **Factual/Deterministic** (Customer support, QA):\n",
    "```python\n",
    "{\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.5,\n",
    "    \"top_k\": 10,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Balanced** (General conversation):\n",
    "```python\n",
    "{\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"repetition_penalty\": 1.2\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Creative** (Content generation):\n",
    "```python\n",
    "{\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 100,\n",
    "    \"repetition_penalty\": 1.3\n",
    "}\n",
    "```\n",
    "\n",
    "### Evaluating Your Fine-tuned Model:\n",
    "\n",
    "#### Qualitative Evaluation:\n",
    "1. **Relevance**: Does it answer the question?\n",
    "2. **Accuracy**: Is the information correct?\n",
    "3. **Consistency**: Does it match your brand/style?\n",
    "4. **Completeness**: Does it cover all necessary points?\n",
    "5. **Coherence**: Is the response well-structured?\n",
    "\n",
    "#### Quantitative Metrics:\n",
    "- **Perplexity**: Lower is better (measures prediction confidence)\n",
    "- **BLEU/ROUGE**: For comparing against reference answers\n",
    "- **Latency**: Response time (target: <500ms)\n",
    "- **Throughput**: Requests per second\n",
    "\n",
    "#### A/B Testing:\n",
    "Compare fine-tuned model vs. base model:\n",
    "- User satisfaction scores\n",
    "- Task completion rates\n",
    "- Response quality ratings\n",
    "\n",
    "Let's test our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "test_prompt = {\n",
    "    \"inputs\": \"How do I reset my password?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(test_prompt)\n",
    "print(\"Response:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remember to delete your endpoint when you're done to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Summary & Key Takeaways\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "Congratulations! You've successfully fine-tuned a Mistral model on Amazon SageMaker. Let's recap the key concepts:\n",
    "\n",
    "### 1. **Mistral Models**\n",
    "- High-performance, open-source LLMs\n",
    "- Efficient architecture (GQA, Sliding Window Attention)\n",
    "- Excellent for fine-tuning due to Apache 2.0 license\n",
    "- 7B model punches above its weight class\n",
    "\n",
    "### 2. **When to Fine-tune**\n",
    "‚úÖ Domain-specific language and terminology\n",
    "‚úÖ Consistent style and formatting requirements\n",
    "‚úÖ Cost optimization (smaller fine-tuned > larger base)\n",
    "‚úÖ Privacy (embed knowledge, not in prompts)\n",
    "‚ùå Limited data (<100 examples)\n",
    "‚ùå Rapidly changing requirements\n",
    "\n",
    "### 3. **LoRA Benefits**\n",
    "- **60-80% cost reduction** vs. full fine-tuning\n",
    "- **2-3x faster** training\n",
    "- **16MB adapters** vs. 14GB full model\n",
    "- **95-98% quality** of full fine-tuning\n",
    "- **Multiple adapters** per base model\n",
    "\n",
    "### 4. **Key Parameters**\n",
    "\n",
    "| Parameter | Typical Value | Impact |\n",
    "|-----------|---------------|--------|\n",
    "| LoRA rank (r) | 16 | Adapter capacity |\n",
    "| LoRA alpha | 32 | Update scaling |\n",
    "| Learning rate | 2e-4 | Training speed |\n",
    "| Epochs | 3 | Training iterations |\n",
    "| Batch size | 1-4 | Memory vs. speed |\n",
    "| Temperature | 0.7 | Response randomness |\n",
    "\n",
    "### 5. **Cost Breakdown**\n",
    "\n",
    "**Training** (ml.g5.2xlarge @ $1.52/hr):\n",
    "- 100 examples: ~$0.25 (10 min)\n",
    "- 1,000 examples: ~$0.75 (30 min)\n",
    "- 5,000 examples: ~$3.00 (2 hrs)\n",
    "\n",
    "**Inference** (ml.g5.2xlarge @ $1.52/hr):\n",
    "- Development: Delete after testing ($0)\n",
    "- Production: ~$1,100/month (24/7)\n",
    "- With auto-scaling: ~$300-500/month\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Immediate Actions:\n",
    "\n",
    "1. **Experiment with Your Data**\n",
    "   - Collect 100-500 examples from your domain\n",
    "   - Format as JSONL with user/assistant messages\n",
    "   - Run fine-tuning with default parameters\n",
    "\n",
    "2. **Optimize Parameters**\n",
    "   - Try different LoRA ranks: 8, 16, 32\n",
    "   - Adjust learning rate: 1e-4, 2e-4, 5e-4\n",
    "   - Experiment with epochs: 1, 3, 5\n",
    "\n",
    "3. **Compare Performance**\n",
    "   - Test base model vs. fine-tuned\n",
    "   - Measure accuracy on held-out test set\n",
    "   - Calculate cost savings\n",
    "\n",
    "### Advanced Topics:\n",
    "\n",
    "#### **Multi-task Fine-tuning**\n",
    "Train one model for multiple tasks:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Task: summarization\"}, ...]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Task: classification\"}, ...]}\n",
    "```\n",
    "\n",
    "#### **Instruction Tuning**\n",
    "Improve instruction-following:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"Summarize in 3 bullet points: ...\"}, ...]}\n",
    "```\n",
    "\n",
    "#### **RLHF (Reinforcement Learning from Human Feedback)**\n",
    "- Collect human preferences\n",
    "- Train reward model\n",
    "- Fine-tune with PPO\n",
    "\n",
    "#### **Quantization**\n",
    "Reduce model size further:\n",
    "- 4-bit quantization: 75% size reduction\n",
    "- 8-bit quantization: 50% size reduction\n",
    "- Minimal quality loss\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### Documentation:\n",
    "- [Mistral AI Documentation](https://docs.mistral.ai/)\n",
    "- [Hugging Face PEFT Library](https://huggingface.co/docs/peft)\n",
    "- [SageMaker Training Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "### Tutorials:\n",
    "- [Fine-tuning Best Practices](https://huggingface.co/blog/fine-tune-llms)\n",
    "- [LoRA Deep Dive](https://huggingface.co/blog/lora)\n",
    "- [SageMaker Examples](https://github.com/aws/amazon-sagemaker-examples)\n",
    "\n",
    "### Community:\n",
    "- [Mistral AI Discord](https://discord.gg/mistralai)\n",
    "- [Hugging Face Forums](https://discuss.huggingface.co/)\n",
    "- [AWS ML Community](https://aws.amazon.com/machine-learning/community/)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips\n",
    "\n",
    "1. **Start Small**: Begin with 100-200 examples, iterate quickly\n",
    "2. **Monitor Training**: Watch loss curves in CloudWatch\n",
    "3. **Version Control**: Save different adapter versions\n",
    "4. **A/B Test**: Compare models in production\n",
    "5. **Cost Optimize**: Delete endpoints when not in use\n",
    "6. **Data Quality**: 100 great examples > 1,000 mediocre ones\n",
    "7. **Regularization**: Use dropout to prevent overfitting\n",
    "8. **Evaluation**: Create a test set for objective metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Workshop Completion Checklist\n",
    "\n",
    "- [ ] Understood Mistral model architecture\n",
    "- [ ] Learned when to fine-tune vs. use base models\n",
    "- [ ] Grasped LoRA concepts and benefits\n",
    "- [ ] Prepared training data in JSONL format\n",
    "- [ ] Configured training parameters\n",
    "- [ ] Launched SageMaker training job\n",
    "- [ ] Deployed fine-tuned model\n",
    "- [ ] Tested inference with different parameters\n",
    "- [ ] Cleaned up resources\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "You've completed the Mistral Fine-tuning workshop! You now have the knowledge to:\n",
    "- Fine-tune LLMs efficiently with LoRA\n",
    "- Deploy models on SageMaker\n",
    "- Optimize costs and performance\n",
    "- Build production-ready AI applications\n",
    "\n",
    "**Questions?** Reach out to the workshop facilitators or AWS support.\n",
    "\n",
    "**Ready for more?** Check out the other notebooks in this workshop series!\n",
    "\n",
    "---\n",
    "\n",
    "### Remember to clean up your resources! üëá"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

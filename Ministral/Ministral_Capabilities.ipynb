{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ministral 3 Models: Compact Multimodal AI on Amazon Bedrock\n",
    "\n",
    "---\n",
    "\n",
    "[Ministral 3](https://mistral.ai/news/mistral-3) models are part of Mistral's latest model family, offering compact yet powerful options for various deployment scenarios. These models share the multimodal and multilingual capabilities of the larger Mistral 3 family while maintaining smaller footprints for edge and cost-efficient deployments.\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Model | Model ID | Parameters | Best For |\n",
    "|-------|----------|------------|----------|\n",
    "| **Ministral 3B** | `mistral.ministral-3-3b-instruct` | 3B | Edge devices, ultra-low latency |\n",
    "| **Ministral 8B** | `mistral.ministral-3-8b-instruct` | 8B | Balanced performance/efficiency |\n",
    "| **Ministral 14B** | `mistral.ministral-3-14b-instruct` | 14B | Higher accuracy, complex reasoning |\n",
    "\n",
    "## Key Highlights\n",
    "\n",
    "- **Multimodal**: Image understanding capabilities\n",
    "- **Multilingual**: Support for 40+ languages\n",
    "- **Apache 2.0**: Fully open-source license\n",
    "- **Best Performance-to-Cost Ratio**: Optimized for their parameter class\n",
    "- **Edge-Ready**: Designed for local and on-device deployment\n",
    "\n",
    "In this notebook, we demonstrate Ministral capabilities using Amazon Bedrock:\n",
    "\n",
    "1. Setup and model comparison\n",
    "2. Basic text generation\n",
    "3. Reasoning and problem solving\n",
    "4. Code generation\n",
    "5. Structured output (JSON)\n",
    "6. Vision and image understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install --upgrade --quiet boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REGION = \"us-west-2\"\n",
    "\n",
    "# Ministral Model IDs\n",
    "MODELS = {\n",
    "    \"3B\": \"mistral.ministral-3-3b-instruct\",\n",
    "    \"8B\": \"mistral.ministral-3-8b-instruct\",\n",
    "    \"14B\": \"mistral.ministral-3-14b-instruct\"\n",
    "}\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=REGION\n",
    ")\n",
    "\n",
    "print(\"Available Ministral Models:\")\n",
    "for name, model_id in MODELS.items():\n",
    "    print(f\"  {name}: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse(model_id, messages, system_prompt=None, max_tokens=1024, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Helper function to interact with Ministral models using the Converse API.\n",
    "    \n",
    "    Args:\n",
    "        model_id: The Ministral model ID to use\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt: Optional system prompt string\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Response from the model\n",
    "    \"\"\"\n",
    "    converse_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if system_prompt:\n",
    "        converse_params[\"system\"] = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock_client.converse(**converse_params)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_response_text(response):\n",
    "    \"\"\"Extract text content from a Converse API response.\"\"\"\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Model Comparison\n",
    "\n",
    "Let's compare response quality and latency across all three Ministral models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models on the same prompt\n",
    "test_prompt = \"Explain what an API is in 2-3 sentences, suitable for a beginner.\"\n",
    "\n",
    "print(\"Comparing Ministral Models\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model_id in MODELS.items():\n",
    "    print(f\"\\n--- Ministral {name} ---\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": test_prompt}]}]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = converse(model_id, messages, temperature=0.3)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    print(f\"Latency: {latency:.2f}s\")\n",
    "    print(f\"Response: {get_response_text(response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Text Generation\n",
    "\n",
    "Demonstrating text generation capabilities with the Ministral 8B model (balanced choice).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 8B as the default for examples\n",
    "DEFAULT_MODEL = MODELS[\"8B\"]\n",
    "\n",
    "# Question answering\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"What are the three laws of thermodynamics? Explain each briefly.\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = converse(DEFAULT_MODEL, messages, temperature=0.3)\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative writing\n",
    "creative_prompt = \"Write a haiku about cloud computing.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": creative_prompt}]}]\n",
    "\n",
    "response = converse(DEFAULT_MODEL, messages, temperature=0.8)\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reasoning and Problem Solving\n",
    "\n",
    "Testing logical reasoning capabilities across model sizes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math word problem\n",
    "math_problem = \"\"\"\n",
    "A store sells apples for $2 each and oranges for $3 each. \n",
    "If Sarah buys 5 apples and 3 oranges, and pays with a $20 bill, \n",
    "how much change does she receive?\n",
    "\n",
    "Show your work step by step.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": math_problem}]}]\n",
    "\n",
    "print(\"Math Problem - Ministral 8B\")\n",
    "print(\"=\"*50)\n",
    "response = converse(MODELS[\"8B\"], messages, temperature=0.1)\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic puzzle - compare 8B vs 14B\n",
    "logic_puzzle = \"\"\"\n",
    "If all roses are flowers, and some flowers fade quickly, \n",
    "can we conclude that some roses fade quickly?\n",
    "\n",
    "Explain your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": logic_puzzle}]}]\n",
    "\n",
    "print(\"Logic Puzzle Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name in [\"8B\", \"14B\"]:\n",
    "    print(f\"\\n--- Ministral {name} ---\")\n",
    "    response = converse(MODELS[name], messages, temperature=0.1)\n",
    "    print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Code Generation\n",
    "\n",
    "Ministral models can generate code for common programming tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple code generation\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function that checks if a string is a palindrome.\n",
    "Include a docstring and handle edge cases.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": code_prompt}]}]\n",
    "\n",
    "response = converse(MODELS[\"8B\"], messages, temperature=0.1)\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code explanation\n",
    "code_to_explain = \"\"\"\n",
    "Explain what this Python code does:\n",
    "\n",
    "```python\n",
    "def mystery(n):\n",
    "    return n if n <= 1 else mystery(n-1) + mystery(n-2)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": code_to_explain}]}]\n",
    "\n",
    "response = converse(MODELS[\"8B\"], messages, temperature=0.1)\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Structured Output (JSON)\n",
    "\n",
    "Ministral models can generate structured JSON output, useful for data extraction and API responses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON extraction\n",
    "json_prompt = \"\"\"\n",
    "Extract the following information from this text and return it as JSON:\n",
    "\n",
    "\"John Smith is a 32-year-old software engineer from Seattle. \n",
    "He has 8 years of experience and specializes in Python and AWS.\"\n",
    "\n",
    "Return a JSON object with keys: name, age, occupation, location, experience_years, skills\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": json_prompt}]}]\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that returns only valid JSON without any additional text.\"\n",
    "\n",
    "response = converse(MODELS[\"8B\"], messages, system_prompt=system_prompt, temperature=0.1)\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis with structured output\n",
    "sentiment_prompt = \"\"\"\n",
    "Analyze the sentiment of these customer reviews and return JSON:\n",
    "\n",
    "1. \"The product arrived quickly and works perfectly. Love it!\"\n",
    "2. \"Terrible experience. The item was broken and customer service was unhelpful.\"\n",
    "3. \"It's okay, nothing special but does the job.\"\n",
    "\n",
    "Return a JSON array with objects containing: review_number, sentiment (positive/negative/neutral), confidence (high/medium/low)\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": sentiment_prompt}]}]\n",
    "\n",
    "response = converse(MODELS[\"8B\"], messages, system_prompt=system_prompt, temperature=0.1)\n",
    "print(get_response_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Vision and Image Understanding\n",
    "\n",
    "Ministral models support multimodal capabilities including image understanding.\n",
    "\n",
    "> **Note**: For vision tasks, use the **InvokeModel API** with Mistral's native message format. Converse API support for images is coming soon.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import urllib.request\n",
    "\n",
    "def load_image_as_base64(image_path_or_url):\n",
    "    \"\"\"Load an image from a file path or URL and convert to base64.\"\"\"\n",
    "    if image_path_or_url.startswith(('http://', 'https://')):\n",
    "        req = urllib.request.Request(\n",
    "            image_path_or_url,\n",
    "            headers={'User-Agent': 'Mozilla/5.0'}\n",
    "        )\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            image_data = response.read()\n",
    "    else:\n",
    "        with open(image_path_or_url, 'rb') as f:\n",
    "            image_data = f.read()\n",
    "    \n",
    "    return base64.standard_b64encode(image_data).decode('utf-8')\n",
    "\n",
    "\n",
    "def analyze_image(model_id, image_source, prompt, media_type=\"image/jpeg\"):\n",
    "    \"\"\"\n",
    "    Analyze an image using Ministral's vision capabilities via InvokeModel API.\n",
    "    \n",
    "    Args:\n",
    "        model_id: The Ministral model ID to use\n",
    "        image_source: File path or URL to the image\n",
    "        prompt: Text prompt describing what to analyze\n",
    "        media_type: MIME type of the image\n",
    "    \n",
    "    Returns:\n",
    "        Model's analysis of the image\n",
    "    \"\"\"\n",
    "    image_base64 = load_image_as_base64(image_source)\n",
    "    \n",
    "    # Mistral's native multimodal format\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:{media_type};base64,{image_base64}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    \n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image analysis with Ministral 8B\n",
    "image_path = \"../Pixtral-samples/Pixtral_data/Amazon_Chart.png\"\n",
    "\n",
    "analysis_prompt = \"\"\"Analyze this chart and provide:\n",
    "1. What type of data is being shown\n",
    "2. Key metrics and their values\n",
    "3. Notable trends or patterns\n",
    "\"\"\"\n",
    "\n",
    "print(\"Image Analysis - Ministral 8B\")\n",
    "print(\"=\"*50)\n",
    "result = analyze_image(MODELS[\"8B\"], image_path, analysis_prompt, media_type=\"image/png\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vision capabilities across model sizes\n",
    "image_path = \"../Pixtral-samples/Pixtral_data/Crosstab_of_Cola_Preference_by_Age_and_Gender.png\"\n",
    "\n",
    "table_prompt = \"\"\"Extract the key insights from this table:\n",
    "1. What data categories are shown?\n",
    "2. What are the total values by gender?\n",
    "3. Which age group has the highest frequency?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Vision Comparison - Table Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name in [\"8B\", \"14B\"]:\n",
    "    print(f\"\\n--- Ministral {name} ---\")\n",
    "    start_time = time.time()\n",
    "    result = analyze_image(MODELS[name], image_path, table_prompt, media_type=\"image/png\")\n",
    "    latency = time.time() - start_time\n",
    "    print(f\"Latency: {latency:.2f}s\")\n",
    "    print(f\"Response: {result[:500]}...\")  # Truncate for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing the Right Model\n",
    "\n",
    "| Use Case | Recommended Model | Why |\n",
    "|----------|-------------------|-----|\n",
    "| Simple Q&A, classification | **Ministral 3B** | Fastest, lowest cost |\n",
    "| General purpose, balanced tasks | **Ministral 8B** | Good performance/cost ratio |\n",
    "| Complex reasoning, code generation | **Ministral 14B** | Higher capability |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ministral models offer a compelling option for cost-sensitive and latency-critical applications:\n",
    "\n",
    "- **Ministral 3B**: Ultra-efficient for simple tasks and edge deployments\n",
    "- **Ministral 8B**: Balanced performance for most general use cases\n",
    "- **Ministral 14B**: Enhanced capabilities for more demanding tasks\n",
    "\n",
    "All three models support the Converse API on Amazon Bedrock, making them easy to integrate into existing applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "This notebook uses Amazon Bedrock's serverless inference, so there are no resources to clean up. You are charged based on usage (input and output tokens processed)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

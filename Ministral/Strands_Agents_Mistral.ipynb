{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Building AI Agents with Strands and Mistral on Amazon Bedrock\n",
    "\n",
    "---\n",
    "\n",
    "[Strands Agents](https://github.com/strands-agents/sdk-python) is an open-source SDK from AWS for building production-ready AI agent systems. Combined with Mistral's powerful models on Amazon Bedrock, you can create sophisticated conversational agents with just a few lines of code.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook demonstrates Strands Agents capabilities with Mistral models:\n",
    "\n",
    "1. **Basic Agents** - Create agents with different Mistral models\n",
    "2. **Model Comparison** - Compare response quality and latency\n",
    "3. **Streaming Responses** - Real-time response handling\n",
    "4. **Multi-Turn Conversations** - Context-aware dialogue\n",
    "5. **Hooks & Callbacks** - Event-driven monitoring\n",
    "6. **Session Persistence** - Save and resume conversations\n",
    "7. **Multi-Agent Orchestration** - Chain agents for complex workflows\n",
    "8. **Practical Use Cases** - Expert panel and content pipeline\n",
    "\n",
    "## Mistral Models Used\n",
    "\n",
    "| Model | Parameters | Best For |\n",
    "|-------|------------|----------|\n",
    "| **Mistral Large 3** | 675B | Orchestration, complex reasoning |\n",
    "| **Ministral 3B** | 3B | Quick classification, routing |\n",
    "| **Ministral 8B** | 8B | Balanced tasks |\n",
    "| **Ministral 14B** | 14B | Detailed analysis, code generation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Strands Agents SDK\n",
    "%pip install --upgrade --quiet strands-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from strands import Agent\n",
    "from strands.models.bedrock import BedrockModel\n",
    "\n",
    "# Mistral Model IDs on Amazon Bedrock\n",
    "MODELS = {\n",
    "    \"large\": \"mistral.mistral-large-3-675b-instruct\",\n",
    "    \"3B\": \"mistral.ministral-3-3b-instruct\",\n",
    "    \"8B\": \"mistral.ministral-3-8b-instruct\",\n",
    "    \"14B\": \"mistral.ministral-3-14b-instruct\"\n",
    "}\n",
    "\n",
    "REGION = \"us-west-2\"\n",
    "\n",
    "print(\"Mistral Models Available:\")\n",
    "for name, model_id in MODELS.items():\n",
    "    print(f\"  {name}: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_key: str, **kwargs) -> BedrockModel:\n",
    "    \"\"\"Create a BedrockModel instance for the specified Mistral model.\"\"\"\n",
    "    return BedrockModel(\n",
    "        model_id=MODELS[model_key],\n",
    "        region_name=REGION,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# Test connection\n",
    "test_model = get_model(\"8B\")\n",
    "print(f\"Connected to Bedrock in {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-basic-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Agent Creation\n",
    "\n",
    "Creating an agent with Strands is simple - just specify a model and optionally a system prompt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-basic-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple agent with Ministral 8B\n",
    "agent = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    system_prompt=\"You are a helpful assistant. Keep responses concise (2-3 sentences max).\"\n",
    ")\n",
    "\n",
    "# Simple conversation\n",
    "response = agent(\"What is the capital of France and why is it significant?\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses across model sizes\n",
    "test_prompt = \"Explain what an API is in one sentence, suitable for a beginner.\"\n",
    "\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in [\"3B\", \"8B\", \"14B\"]:\n",
    "    agent = Agent(\n",
    "        model=get_model(model_name),\n",
    "        system_prompt=\"Give extremely concise answers in one sentence.\"\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    response = agent(test_prompt)\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    print(f\"\\n--- Ministral {model_name} ({latency:.2f}s) ---\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-streaming-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Streaming Responses\n",
    "\n",
    "Stream responses in real-time using callbacks or async iterators.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.handlers.callback_handler import PrintingCallbackHandler\n",
    "\n",
    "# Agent with callback handler for streaming output\n",
    "streaming_agent = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    callback_handler=PrintingCallbackHandler(),\n",
    "    system_prompt=\"You are a creative storyteller.\"\n",
    ")\n",
    "\n",
    "# This will stream the response as it's generated\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 50)\n",
    "response = streaming_agent(\"Write a very short story (3 sentences) about a robot learning to paint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-async-streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async streaming with stream_async()\n",
    "async def stream_response():\n",
    "    agent = Agent(\n",
    "        model=get_model(\"8B\"),\n",
    "        system_prompt=\"You explain concepts clearly and concisely.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Async streaming:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    async for event in agent.stream_async(\"What is quantum computing in 2 sentences?\"):\n",
    "        # Events are dictionaries with 'data' key for text deltas\n",
    "        if \"data\" in event and event.get(\"data\"):\n",
    "            print(event[\"data\"], end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "await stream_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conversation-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Multi-Turn Conversations\n",
    "\n",
    "Strands agents maintain conversation history automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent for multi-turn conversation\n",
    "chat_agent = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    system_prompt=\"\"\"You are a helpful AI tutor teaching Python programming.\n",
    "    Remember what the student has learned in previous messages.\n",
    "    Keep explanations brief and build on prior knowledge.\"\"\"\n",
    ")\n",
    "\n",
    "# First turn\n",
    "print(\"Turn 1:\")\n",
    "print(\"-\" * 40)\n",
    "response1 = chat_agent(\"What is a variable in Python?\")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-conversation-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second turn - agent remembers context\n",
    "print(\"Turn 2:\")\n",
    "print(\"-\" * 40)\n",
    "response2 = chat_agent(\"Can you show me an example?\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-conversation-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third turn - building on previous knowledge\n",
    "print(\"Turn 3:\")\n",
    "print(\"-\" * 40)\n",
    "response3 = chat_agent(\"What's the difference between that and a constant?\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-message-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View conversation history\n",
    "print(f\"\\nConversation has {len(chat_agent.messages)} messages\")\n",
    "for i, msg in enumerate(chat_agent.messages):\n",
    "    role = msg.get('role', 'unknown')\n",
    "    content = msg.get('content', [])\n",
    "    if content and isinstance(content, list) and len(content) > 0:\n",
    "        text = content[0].get('text', '')[:60]\n",
    "        if len(content[0].get('text', '')) > 60:\n",
    "            text += '...'\n",
    "        print(f\"  {i+1}. [{role}]: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-hooks-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Hooks and Callbacks\n",
    "\n",
    "Implement custom hooks for logging, monitoring, and extending agent behavior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-hooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.hooks import HookProvider, HookRegistry\n",
    "from strands.hooks import (\n",
    "    BeforeInvocationEvent,\n",
    "    AfterInvocationEvent\n",
    ")\n",
    "\n",
    "\n",
    "class MetricsHook(HookProvider):\n",
    "    \"\"\"Custom hook for tracking agent metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.invocation_count = 0\n",
    "        self.total_latency = 0\n",
    "        self.start_time = None\n",
    "    \n",
    "    def register_hooks(self, registry: HookRegistry) -> None:\n",
    "        registry.add_callback(BeforeInvocationEvent, self.on_invocation_start)\n",
    "        registry.add_callback(AfterInvocationEvent, self.on_invocation_end)\n",
    "    \n",
    "    def on_invocation_start(self, event: BeforeInvocationEvent) -> None:\n",
    "        self.invocation_count += 1\n",
    "        self.start_time = time.time()\n",
    "        print(f\"[METRICS] Invocation #{self.invocation_count} started\")\n",
    "    \n",
    "    def on_invocation_end(self, event: AfterInvocationEvent) -> None:\n",
    "        duration = time.time() - self.start_time\n",
    "        self.total_latency += duration\n",
    "        print(f\"[METRICS] Completed in {duration:.2f}s\")\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        avg_latency = self.total_latency / self.invocation_count if self.invocation_count > 0 else 0\n",
    "        return {\n",
    "            \"total_invocations\": self.invocation_count,\n",
    "            \"total_latency\": f\"{self.total_latency:.2f}s\",\n",
    "            \"avg_latency\": f\"{avg_latency:.2f}s\"\n",
    "        }\n",
    "\n",
    "\n",
    "# Create metrics hook\n",
    "metrics = MetricsHook()\n",
    "\n",
    "# Agent with hooks\n",
    "monitored_agent = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    hooks=[metrics],\n",
    "    system_prompt=\"You are a helpful assistant. Be concise (1-2 sentences).\"\n",
    ")\n",
    "\n",
    "print(\"Monitored agent created with custom hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-hooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the monitored agent multiple times\n",
    "questions = [\n",
    "    \"What is 2+2?\",\n",
    "    \"Name 3 planets.\",\n",
    "    \"What color is the sky?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    response = monitored_agent(q)\n",
    "    print(f\"A: {response}\")\n",
    "\n",
    "print(f\"\\n--- Final Metrics ---\")\n",
    "print(json.dumps(metrics.get_stats(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-session-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Conversation Persistence\n",
    "\n",
    "Save and restore agent conversation history for stateful assistants.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory for storing conversations\n",
    "session_dir = tempfile.mkdtemp(prefix=\"strands_sessions_\")\n",
    "print(f\"Session directory: {session_dir}\")\n",
    "\n",
    "# Create agent for conversation\n",
    "persistent_agent = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    system_prompt=\"You are a helpful assistant that remembers our conversation. Be concise.\"\n",
    ")\n",
    "\n",
    "# Start a conversation\n",
    "print(\"\\nStarting conversation...\")\n",
    "response1 = persistent_agent(\"My name is Alice and I work as a data scientist at TechCorp.\")\n",
    "print(f\"Response 1: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-session-continue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation - agent remembers context\n",
    "response2 = persistent_agent(\"What's my job title and where do I work?\")\n",
    "print(f\"Response 2: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-session-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the conversation to a file\n",
    "session_file = os.path.join(session_dir, \"alice_conversation.json\")\n",
    "\n",
    "# Agent messages are a list of dicts that can be serialized\n",
    "conversation_data = {\n",
    "    \"agent_id\": persistent_agent.agent_id,\n",
    "    \"messages\": persistent_agent.messages\n",
    "}\n",
    "\n",
    "with open(session_file, \"w\") as f:\n",
    "    json.dump(conversation_data, f, indent=2)\n",
    "\n",
    "print(f\"Conversation saved to: {session_file}\")\n",
    "print(f\"Message count: {len(persistent_agent.messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-session-restore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NEW agent and restore the conversation\n",
    "restored_agent = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    system_prompt=\"You are a helpful assistant that remembers our conversation. Be concise.\"\n",
    ")\n",
    "\n",
    "# Load the saved conversation\n",
    "with open(session_file, \"r\") as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "# Restore messages to the new agent\n",
    "restored_agent.messages = loaded_data[\"messages\"]\n",
    "print(f\"Conversation restored with {len(restored_agent.messages)} messages\")\n",
    "\n",
    "# Test that context is preserved - the agent should remember Alice's name\n",
    "response3 = restored_agent(\"What was my name again?\")\n",
    "print(f\"Response from restored agent: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-multiagent-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Multi-Agent Orchestration\n",
    "\n",
    "Chain multiple agents together for complex workflows. Each agent specializes in a specific task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pipeline-agents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a content pipeline with specialized agents\n",
    "\n",
    "# Fast classifier using smallest model\n",
    "classifier = Agent(\n",
    "    model=get_model(\"3B\"),\n",
    "    system_prompt=\"\"\"Classify the input text into exactly ONE category:\n",
    "    - TECHNICAL: Technical or scientific content\n",
    "    - BUSINESS: Business, finance, or corporate content  \n",
    "    - CREATIVE: Creative writing, stories, or artistic content\n",
    "    - GENERAL: Everything else\n",
    "    \n",
    "    Respond with ONLY the category name, nothing else.\"\"\"\n",
    ")\n",
    "\n",
    "# Specialized processors\n",
    "technical_expert = Agent(\n",
    "    model=get_model(\"14B\"),\n",
    "    system_prompt=\"\"\"You are a technical expert. Analyze the content and provide:\n",
    "    1. Key technical concepts identified\n",
    "    2. Technologies or methods mentioned\n",
    "    3. A clear technical summary\n",
    "    Be thorough but concise.\"\"\"\n",
    ")\n",
    "\n",
    "business_expert = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    system_prompt=\"\"\"You are a business analyst. Analyze the content and provide:\n",
    "    1. Key metrics and numbers\n",
    "    2. Business insights and implications\n",
    "    3. Actionable recommendations\n",
    "    Format as an executive summary.\"\"\"\n",
    ")\n",
    "\n",
    "creative_expert = Agent(\n",
    "    model=get_model(\"14B\"),\n",
    "    system_prompt=\"\"\"You are a creative writing expert. Analyze the content and provide:\n",
    "    1. Writing style and tone analysis\n",
    "    2. Literary devices or techniques used\n",
    "    3. Suggestions for improvement\n",
    "    Be constructive and insightful.\"\"\"\n",
    ")\n",
    "\n",
    "general_expert = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    system_prompt=\"\"\"Summarize the main points of the content clearly and concisely.\n",
    "    Provide key takeaways that would be useful to the reader.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Content Pipeline Agents created:\")\n",
    "print(\"  - Classifier (Ministral 3B) - fast routing\")\n",
    "print(\"  - Technical Expert (Ministral 14B)\")\n",
    "print(\"  - Business Expert (Ministral 8B)\")\n",
    "print(\"  - Creative Expert (Ministral 14B)\")\n",
    "print(\"  - General Expert (Ministral 8B)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pipeline-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(content: str) -> dict:\n",
    "    \"\"\"Process content through the multi-agent pipeline.\"\"\"\n",
    "    \n",
    "    # Step 1: Classify the content\n",
    "    print(\"Step 1: Classifying content...\")\n",
    "    category = str(classifier(content)).strip().upper()\n",
    "    print(f\"  Category: {category}\")\n",
    "    \n",
    "    # Step 2: Route to appropriate expert\n",
    "    print(\"Step 2: Routing to expert...\")\n",
    "    \n",
    "    if \"TECHNICAL\" in category:\n",
    "        expert = technical_expert\n",
    "        expert_name = \"Technical Expert\"\n",
    "    elif \"BUSINESS\" in category:\n",
    "        expert = business_expert\n",
    "        expert_name = \"Business Expert\"\n",
    "    elif \"CREATIVE\" in category:\n",
    "        expert = creative_expert\n",
    "        expert_name = \"Creative Expert\"\n",
    "    else:\n",
    "        expert = general_expert\n",
    "        expert_name = \"General Expert\"\n",
    "    \n",
    "    print(f\"  Routed to: {expert_name}\")\n",
    "    \n",
    "    # Step 3: Get expert analysis\n",
    "    print(\"Step 3: Getting expert analysis...\")\n",
    "    analysis = expert(f\"Analyze this content:\\n\\n{content}\")\n",
    "    \n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"expert\": expert_name,\n",
    "        \"analysis\": str(analysis)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pipeline-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with technical content\n",
    "technical_content = \"\"\"\n",
    "The new microservices architecture uses Kubernetes for container orchestration,\n",
    "with Redis for caching and PostgreSQL for persistent storage. The API layer\n",
    "implements GraphQL with rate limiting and JWT authentication. We've achieved\n",
    "99.9% uptime since deployment.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Processing TECHNICAL content:\")\n",
    "print(\"=\" * 60)\n",
    "result = process_content(technical_content)\n",
    "print(f\"\\nAnalysis:\\n{result['analysis']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pipeline-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with business content\n",
    "business_content = \"\"\"\n",
    "Q3 revenue increased 15% YoY to $4.2M. Customer acquisition cost decreased\n",
    "by 20% while retention improved to 94%. The board approved expansion into\n",
    "the European market starting Q1 next year, with an initial investment of $2M.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Processing BUSINESS content:\")\n",
    "print(\"=\" * 60)\n",
    "result = process_content(business_content)\n",
    "print(f\"\\nAnalysis:\\n{result['analysis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-expert-panel-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practical Use Case: Expert Panel\n",
    "\n",
    "Create an expert panel where specialists from different fields analyze a topic and a moderator synthesizes their perspectives.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-expert-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expert panel agents\n",
    "economist = Agent(\n",
    "    model=get_model(\"14B\"),\n",
    "    system_prompt=\"\"\"You are an economist. Analyze topics from an economic perspective.\n",
    "    Focus on: market dynamics, costs/benefits, incentives, and economic impacts.\n",
    "    Provide 2-3 key insights. Be analytical and data-focused.\"\"\"\n",
    ")\n",
    "\n",
    "technologist = Agent(\n",
    "    model=get_model(\"14B\"),\n",
    "    system_prompt=\"\"\"You are a technology expert. Analyze topics from a tech perspective.\n",
    "    Focus on: technical feasibility, innovation potential, implementation challenges.\n",
    "    Provide 2-3 key insights. Be practical and forward-thinking.\"\"\"\n",
    ")\n",
    "\n",
    "ethicist = Agent(\n",
    "    model=get_model(\"8B\"),\n",
    "    system_prompt=\"\"\"You are an ethics specialist. Analyze topics from an ethical perspective.\n",
    "    Focus on: societal impact, fairness, privacy, and moral considerations.\n",
    "    Provide 2-3 key insights. Be thoughtful and balanced.\"\"\"\n",
    ")\n",
    "\n",
    "moderator = Agent(\n",
    "    model=get_model(\"large\"),\n",
    "    system_prompt=\"\"\"You are a panel moderator. Your job is to:\n",
    "    1. Synthesize different expert perspectives into a coherent summary\n",
    "    2. Identify areas of agreement and disagreement\n",
    "    3. Provide a balanced, actionable conclusion\n",
    "    Be fair to all perspectives while providing clear guidance.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Expert Panel created:\")\n",
    "print(\"  - Economist (Ministral 14B)\")\n",
    "print(\"  - Technologist (Ministral 14B)\")\n",
    "print(\"  - Ethicist (Ministral 8B)\")\n",
    "print(\"  - Moderator (Mistral Large 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-panel-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_expert_panel(topic: str) -> str:\n",
    "    \"\"\"Run a full expert panel discussion on a topic.\"\"\"\n",
    "    \n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Gather perspectives from each expert\n",
    "    perspectives = {}\n",
    "    \n",
    "    print(\"\\n--- Economist's Analysis ---\")\n",
    "    perspectives['economist'] = str(economist(f\"Analyze this topic: {topic}\"))\n",
    "    print(perspectives['economist'])\n",
    "    \n",
    "    print(\"\\n--- Technologist's Analysis ---\")\n",
    "    perspectives['technologist'] = str(technologist(f\"Analyze this topic: {topic}\"))\n",
    "    print(perspectives['technologist'])\n",
    "    \n",
    "    print(\"\\n--- Ethicist's Analysis ---\")\n",
    "    perspectives['ethicist'] = str(ethicist(f\"Analyze this topic: {topic}\"))\n",
    "    print(perspectives['ethicist'])\n",
    "    \n",
    "    # Moderator synthesizes\n",
    "    synthesis_prompt = f\"\"\"Topic: {topic}\n",
    "\n",
    "Expert perspectives:\n",
    "\n",
    "ECONOMIST: {perspectives['economist']}\n",
    "\n",
    "TECHNOLOGIST: {perspectives['technologist']}\n",
    "\n",
    "ETHICIST: {perspectives['ethicist']}\n",
    "\n",
    "Please synthesize these perspectives into a balanced summary with clear conclusions.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- Moderator's Synthesis ---\")\n",
    "    print(\"=\" * 70)\n",
    "    synthesis = str(moderator(synthesis_prompt))\n",
    "    print(synthesis)\n",
    "    \n",
    "    return synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-panel-execute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the expert panel\n",
    "topic = \"The widespread adoption of AI in hiring and recruitment processes\"\n",
    "synthesis = run_expert_panel(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Conclusion & Best Practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {},
   "source": [
    "### Model Selection Guidelines\n",
    "\n",
    "| Task Type | Recommended Model | Reasoning |\n",
    "|-----------|-------------------|------------|\n",
    "| Quick Classification | **Ministral 3B** | Fastest response, lowest cost |\n",
    "| General Conversations | **Ministral 8B** | Good balance of speed and quality |\n",
    "| Complex Analysis | **Ministral 14B** | Higher accuracy for detailed work |\n",
    "| Orchestration/Synthesis | **Mistral Large 3** | Best reasoning capabilities |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Start Simple**: Begin with a single agent, add complexity as needed\n",
    "2. **Match Model to Task**: Use smaller models for simple tasks, larger for complex\n",
    "3. **Stream for UX**: Use streaming for better user experience in interactive apps\n",
    "4. **Monitor with Hooks**: Track latency and usage for optimization\n",
    "5. **Persist Sessions**: Use session managers for stateful assistants\n",
    "6. **Chain Agents**: Combine specialized agents for sophisticated workflows\n",
    "\n",
    "### Architecture Patterns\n",
    "\n",
    "| Pattern | Use Case | Example |\n",
    "|---------|----------|----------|\n",
    "| **Single Agent** | Simple Q&A, chat | Customer support bot |\n",
    "| **Classifier + Experts** | Routing to specialists | Document processing |\n",
    "| **Expert Panel** | Multi-perspective analysis | Decision support |\n",
    "| **Pipeline** | Sequential processing | Content creation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cleanup-header",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "This notebook uses Amazon Bedrock's serverless inference, so there are no persistent resources to clean up. You are charged based on token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary session directory\n",
    "import shutil\n",
    "\n",
    "if 'session_dir' in dir() and os.path.exists(session_dir):\n",
    "    shutil.rmtree(session_dir)\n",
    "    print(f\"Cleaned up session directory: {session_dir}\")\n",
    "\n",
    "print(\"\\nNotebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

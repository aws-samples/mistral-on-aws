{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Voxtral Audio Models on Amazon Bedrock\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to use **Voxtral Mini** (3B) and **Voxtral Small** (24B) audio-language models on Amazon Bedrock. These models can understand and process audio input alongside text, enabling powerful audio understanding and transcription capabilities.\n",
    "\n",
    "## Voxtral Models Overview\n",
    "\n",
    "| Model | Parameters | Context | Best For |\n",
    "|-------|------------|---------|----------|\n",
    "| **Voxtral Mini** | 3B | 32k tokens | Fast transcription, edge/local deployment |\n",
    "| **Voxtral Small** | 24B | 32k tokens | Complex audio analysis, detailed understanding |\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "- **Audio Transcription**: Convert speech to text in multiple languages\n",
    "- **Audio Understanding**: Answer questions about audio content\n",
    "- **Multi-Turn Conversations**: Discuss audio content with context\n",
    "- **Multiple Audio Files**: Process multiple audio inputs in one request\n",
    "- **Streaming**: Real-time response generation\n",
    "\n",
    "## Supported Languages\n",
    "\n",
    "English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian\n",
    "\n",
    "## Audio Specifications\n",
    "\n",
    "- **Max Duration**: ~45 seconds per request (due to ~2MB payload limit)\n",
    "- **Format**: WAV (16-bit PCM) - convert from OGG/MP3/FLAC\n",
    "- **Sample Rate**: 16kHz recommended\n",
    "- **Longer Audio**: Chunk into 30-second segments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required packages are installed\n",
    "%pip install --upgrade --quiet boto3 soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import wave\n",
    "import struct\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Voxtral Model IDs on Amazon Bedrock\n",
    "MODELS = {\n",
    "    \"mini\": \"mistral.voxtral-mini-3b-2507\",\n",
    "    \"small\": \"mistral.voxtral-small-24b-2507\"\n",
    "}\n",
    "\n",
    "REGION = \"us-west-2\"\n",
    "\n",
    "# Audio files directory (same directory as this notebook)\n",
    "AUDIO_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "\n",
    "print(\"Voxtral Models Available:\")\n",
    "for name, model_id in MODELS.items():\n",
    "    print(f\"  {name}: {model_id}\")\n",
    "print(f\"\\nConnected to Bedrock in {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-header",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "These utilities help load and convert audio files for use with Voxtral models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_convert_audio(\n",
    "    file_path: str,\n",
    "    target_sample_rate: int = 16000,\n",
    "    max_duration: Optional[float] = None\n",
    ") -> Tuple[bytes, dict]:\n",
    "    \"\"\"\n",
    "    Load an audio file and convert it to WAV format suitable for Voxtral.\n",
    "    \n",
    "    Handles various input formats (WAV, MP3, etc.) and converts to:\n",
    "    - 16-bit PCM WAV\n",
    "    - Mono channel\n",
    "    - Target sample rate (default 16kHz)\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the audio file\n",
    "        target_sample_rate: Output sample rate (16kHz recommended)\n",
    "        max_duration: Maximum duration in seconds (None for full file)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (wav_bytes, info_dict)\n",
    "    \"\"\"\n",
    "    # Load audio with soundfile\n",
    "    data, sample_rate = sf.read(file_path)\n",
    "    \n",
    "    original_duration = len(data) / sample_rate\n",
    "    \n",
    "    # Trim to max duration if specified\n",
    "    if max_duration and original_duration > max_duration:\n",
    "        samples = int(max_duration * sample_rate)\n",
    "        data = data[:samples]\n",
    "    \n",
    "    # Convert stereo to mono\n",
    "    if len(data.shape) > 1:\n",
    "        data = data.mean(axis=1)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sample_rate != target_sample_rate:\n",
    "        ratio = target_sample_rate / sample_rate\n",
    "        new_length = int(len(data) * ratio)\n",
    "        indices = np.linspace(0, len(data) - 1, new_length).astype(int)\n",
    "        data = data[indices]\n",
    "    \n",
    "    # Convert to 16-bit PCM\n",
    "    if data.dtype in ['float32', 'float64']:\n",
    "        data = (data * 32767).astype(np.int16)\n",
    "    elif data.dtype != np.int16:\n",
    "        data = data.astype(np.int16)\n",
    "    \n",
    "    # Create WAV in memory\n",
    "    wav_buffer = io.BytesIO()\n",
    "    with wave.open(wav_buffer, 'wb') as wav_file:\n",
    "        wav_file.setnchannels(1)\n",
    "        wav_file.setsampwidth(2)\n",
    "        wav_file.setframerate(target_sample_rate)\n",
    "        wav_file.writeframes(data.tobytes())\n",
    "    \n",
    "    wav_bytes = wav_buffer.getvalue()\n",
    "    \n",
    "    info = {\n",
    "        \"original_sample_rate\": sample_rate,\n",
    "        \"original_duration\": original_duration,\n",
    "        \"converted_sample_rate\": target_sample_rate,\n",
    "        \"converted_duration\": len(data) / target_sample_rate,\n",
    "        \"size_bytes\": len(wav_bytes)\n",
    "    }\n",
    "    \n",
    "    return wav_bytes, info\n",
    "\n",
    "\n",
    "def audio_to_base64(audio_bytes: bytes) -> str:\n",
    "    \"\"\"Encode audio bytes to base64 string.\"\"\"\n",
    "    return base64.b64encode(audio_bytes).decode('utf-8')\n",
    "\n",
    "\n",
    "def call_voxtral(\n",
    "    audio_base64: str,\n",
    "    text_prompt: str,\n",
    "    model: str = \"small\",\n",
    "    max_tokens: int = 500,\n",
    "    temperature: float = 0.7\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Call Voxtral model with audio and text input.\n",
    "    \n",
    "    Args:\n",
    "        audio_base64: Base64 encoded audio (WAV format)\n",
    "        text_prompt: Text question or instruction\n",
    "        model: 'mini' or 'small'\n",
    "        max_tokens: Maximum tokens in response\n",
    "        temperature: Sampling temperature (0.0-1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Response dictionary from Bedrock\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_audio\",\n",
    "                        \"input_audio\": {\n",
    "                            \"data\": audio_base64,\n",
    "                            \"format\": \"wav\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": text_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[model],\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    return json.loads(response['body'].read())\n",
    "\n",
    "\n",
    "def call_voxtral_text_only(\n",
    "    text_prompt: str,\n",
    "    model: str = \"small\",\n",
    "    max_tokens: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Call Voxtral model with text-only input (no audio).\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": text_prompt}],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[model],\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    return json.loads(response['body'].read())\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audio-files-header",
   "metadata": {},
   "source": [
    "## 3. Load Sample Audio Files\n",
    "\n",
    "We'll use the audio files included in this directory for our examples:\n",
    "\n",
    "| File | Description | Duration |\n",
    "|------|-------------|----------|\n",
    "| `jfk.wav` | JFK inaugural speech excerpt | ~11s |\n",
    "| `librispeech_1.wav` | LibriSpeech audiobook sample | ~3.5s |\n",
    "| `VOA_News_Headlines_(May_5,_2009).ogg` | Voice of America news broadcast | ~5 min |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio file paths\n",
    "AUDIO_FILES = {\n",
    "    \"jfk\": \"jfk.wav\",                    # JFK speech excerpt (~11s)\n",
    "    \"librispeech\": \"librispeech_1.wav\",  # LibriSpeech sample (~3.5s)\n",
    "    \"voa_news\": \"VOA_News_Headlines_(May_5,_2009).ogg\",  # VOA News broadcast (~5 min)\n",
    "}\n",
    "\n",
    "# Load and convert audio files\n",
    "print(\"Loading audio files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "audio_data = {}\n",
    "\n",
    "for name, filename in AUDIO_FILES.items():\n",
    "    file_path = filename  # Files are in same directory as notebook\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Set max duration based on file type\n",
    "        # JFK: limit to 5s (large file at 44.1kHz stereo)\n",
    "        # VOA News: limit to 30s (API has ~2MB payload limit)\n",
    "        if name == \"jfk\":\n",
    "            max_dur = 5.0\n",
    "        elif name == \"voa_news\":\n",
    "            max_dur = 30.0  # Use first 30 seconds (fits within 2MB limit)\n",
    "        else:\n",
    "            max_dur = None\n",
    "            \n",
    "        wav_bytes, info = load_and_convert_audio(file_path, max_duration=max_dur)\n",
    "        \n",
    "        audio_data[name] = {\n",
    "            \"bytes\": wav_bytes,\n",
    "            \"base64\": audio_to_base64(wav_bytes),\n",
    "            \"info\": info\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name.upper()} ({filename})\")\n",
    "        print(f\"  Original: {info['original_duration']:.2f}s @ {info['original_sample_rate']}Hz\")\n",
    "        print(f\"  Converted: {info['converted_duration']:.2f}s @ {info['converted_sample_rate']}Hz\")\n",
    "        print(f\"  Size: {info['size_bytes']:,} bytes ({info['size_bytes']/1024:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"\\n{name.upper()}: File not found - {file_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Loaded {len(audio_data)} audio file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transcription-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Audio Transcription\n",
    "\n",
    "Let's transcribe speech from our audio files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transcribe-jfk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe JFK speech\n",
    "print(\"Transcribing JFK Speech...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"jfk\" in audio_data:\n",
    "    result = call_voxtral(\n",
    "        audio_base64=audio_data[\"jfk\"][\"base64\"],\n",
    "        text_prompt=\"Transcribe this audio exactly as spoken.\",\n",
    "        model=\"small\",\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTranscription:\")\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "    print(f\"\\nTokens used: {result['usage']}\")\n",
    "else:\n",
    "    print(\"JFK audio file not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transcribe-librispeech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe LibriSpeech sample\n",
    "print(\"Transcribing LibriSpeech Sample...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"librispeech\" in audio_data:\n",
    "    result = call_voxtral(\n",
    "        audio_base64=audio_data[\"librispeech\"][\"base64\"],\n",
    "        text_prompt=\"Transcribe this audio exactly as spoken.\",\n",
    "        model=\"small\",\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTranscription:\")\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "    print(f\"\\nTokens used: {result['usage']}\")\n",
    "else:\n",
    "    print(\"LibriSpeech audio file not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6qdtlmzikxd",
   "metadata": {},
   "source": [
    "### Longer Audio: VOA News Broadcast\n",
    "\n",
    "The VOA News file demonstrates transcription of longer, more complex audio with multiple topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gkxj9qjhu8k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe VOA News broadcast (first 30 seconds)\n",
    "print(\"Transcribing VOA News Broadcast...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"voa_news\" in audio_data:\n",
    "    info = audio_data[\"voa_news\"][\"info\"]\n",
    "    print(f\"Audio: {info['converted_duration']:.1f} seconds of news broadcast\")\n",
    "    print(f\"Original file duration: {info['original_duration']:.1f} seconds (~5 minutes)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = call_voxtral(\n",
    "        audio_base64=audio_data[\"voa_news\"][\"base64\"],\n",
    "        text_prompt=\"Transcribe this news broadcast completely and accurately.\",\n",
    "        model=\"small\",\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTranscription:\")\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "    print(f\"\\nTokens used: {result['usage']}\")\n",
    "else:\n",
    "    print(\"VOA News audio file not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s07dkphhnh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A about VOA News content\n",
    "print(\"Question & Answer: VOA News Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"voa_news\" in audio_data:\n",
    "    questions = [\n",
    "        \"What news stories are covered in this broadcast? List them briefly.\",\n",
    "        \"What is the main headline or lead story?\",\n",
    "        \"What geographic regions or countries are mentioned?\",\n",
    "    ]\n",
    "    \n",
    "    for q in questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = call_voxtral(\n",
    "            audio_base64=audio_data[\"voa_news\"][\"base64\"],\n",
    "            text_prompt=q,\n",
    "            model=\"small\",\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        print(f\"A: {result['choices'][0]['message']['content']}\")\n",
    "else:\n",
    "    print(\"VOA News audio file not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "syj7yyb22u",
   "metadata": {},
   "source": [
    "### Transcribing Longer Audio: Chunking Strategy\n",
    "\n",
    "For audio longer than ~45 seconds, you must chunk the audio and make sequential API calls. Here's a practical approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mgx3w68c2wo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio(\n",
    "    file_path: str,\n",
    "    chunk_duration: float = 30.0,\n",
    "    model: str = \"small\",\n",
    "    overlap: float = 2.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio files longer than the API limit by chunking.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio file (any format soundfile supports)\n",
    "        chunk_duration: Duration of each chunk in seconds (default 30s)\n",
    "        model: 'mini' or 'small'\n",
    "        overlap: Overlap between chunks to avoid cutting words (default 2s)\n",
    "    \n",
    "    Returns:\n",
    "        Full transcription as a single string\n",
    "    \"\"\"\n",
    "    # Load the full audio file\n",
    "    data, sample_rate = sf.read(file_path)\n",
    "    total_duration = len(data) / sample_rate\n",
    "    \n",
    "    print(f\"Total audio duration: {total_duration:.1f}s\")\n",
    "    print(f\"Chunk size: {chunk_duration}s with {overlap}s overlap\")\n",
    "    \n",
    "    # Calculate number of chunks\n",
    "    effective_chunk = chunk_duration - overlap\n",
    "    num_chunks = int(np.ceil(total_duration / effective_chunk))\n",
    "    print(f\"Processing {num_chunks} chunks...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    transcriptions = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Calculate start and end for this chunk\n",
    "        start_time = i * effective_chunk\n",
    "        end_time = min(start_time + chunk_duration, total_duration)\n",
    "        \n",
    "        # Extract chunk\n",
    "        start_sample = int(start_time * sample_rate)\n",
    "        end_sample = int(end_time * sample_rate)\n",
    "        chunk_data = data[start_sample:end_sample]\n",
    "        \n",
    "        # Convert chunk to WAV format\n",
    "        # Resample to 16kHz if needed\n",
    "        target_sr = 16000\n",
    "        if sample_rate != target_sr:\n",
    "            ratio = target_sr / sample_rate\n",
    "            new_length = int(len(chunk_data) * ratio)\n",
    "            indices = np.linspace(0, len(chunk_data) - 1, new_length).astype(int)\n",
    "            chunk_data = chunk_data[indices]\n",
    "        \n",
    "        # Convert to 16-bit PCM\n",
    "        if chunk_data.dtype in ['float32', 'float64']:\n",
    "            chunk_data = (chunk_data * 32767).astype(np.int16)\n",
    "        \n",
    "        # Create WAV bytes\n",
    "        wav_buffer = io.BytesIO()\n",
    "        with wave.open(wav_buffer, 'wb') as wav_file:\n",
    "            wav_file.setnchannels(1)\n",
    "            wav_file.setsampwidth(2)\n",
    "            wav_file.setframerate(target_sr)\n",
    "            wav_file.writeframes(chunk_data.tobytes())\n",
    "        \n",
    "        chunk_b64 = base64.b64encode(wav_buffer.getvalue()).decode('utf-8')\n",
    "        \n",
    "        # Call Voxtral for this chunk\n",
    "        print(f\"Chunk {i+1}/{num_chunks}: {start_time:.1f}s - {end_time:.1f}s\", end=\" \")\n",
    "        \n",
    "        result = call_voxtral(\n",
    "            audio_base64=chunk_b64,\n",
    "            text_prompt=\"Transcribe this audio exactly. Do not add any commentary.\",\n",
    "            model=model,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        chunk_text = result['choices'][0]['message']['content'].strip()\n",
    "        transcriptions.append(chunk_text)\n",
    "        print(f\"({len(chunk_text)} chars)\")\n",
    "    \n",
    "    # Combine transcriptions\n",
    "    # Note: With overlap, there may be some repeated words at chunk boundaries\n",
    "    full_transcription = \" \".join(transcriptions)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total transcription: {len(full_transcription)} characters\")\n",
    "    \n",
    "    return full_transcription\n",
    "\n",
    "\n",
    "# Transcribe first 2 minutes of VOA News (4 chunks of 30s each)\n",
    "print(\"Transcribing 2 minutes of VOA News using chunking...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# We'll transcribe 120 seconds (2 minutes) to demonstrate\n",
    "# Load file info first\n",
    "voa_data, voa_sr = sf.read(\"VOA_News_Headlines_(May_5,_2009).ogg\")\n",
    "voa_duration = len(voa_data) / voa_sr\n",
    "\n",
    "# Create a temporary trimmed version for the demo (2 minutes)\n",
    "demo_duration = min(120.0, voa_duration)  # 2 minutes or less\n",
    "\n",
    "# Save trimmed version temporarily\n",
    "trimmed_samples = int(demo_duration * voa_sr)\n",
    "trimmed_data = voa_data[:trimmed_samples]\n",
    "sf.write(\"_temp_voa_2min.wav\", trimmed_data, voa_sr)\n",
    "\n",
    "# Transcribe using chunking\n",
    "full_transcript = transcribe_long_audio(\n",
    "    \"_temp_voa_2min.wav\",\n",
    "    chunk_duration=30.0,\n",
    "    model=\"small\",\n",
    "    overlap=2.0\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FULL TRANSCRIPTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(full_transcript)\n",
    "\n",
    "# Clean up temp file\n",
    "os.remove(\"_temp_voa_2min.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Comparison\n",
    "\n",
    "Compare transcription quality and latency between Voxtral Mini and Small.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Mini vs Small on the same audio\n",
    "print(\"Model Comparison: Voxtral Mini vs Small\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if \"librispeech\" in audio_data:\n",
    "    test_audio = audio_data[\"librispeech\"][\"base64\"]\n",
    "    test_prompt = \"describe the tone of the speaker then transcribe the audio in addition to the first task.\"\n",
    "    \n",
    "    print(f\"Audio: LibriSpeech ({audio_data['librispeech']['info']['converted_duration']:.2f}s)\")\n",
    "    print(f\"Prompt: {test_prompt}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for model_name in [\"mini\", \"small\"]:\n",
    "        start = time.time()\n",
    "        result = call_voxtral(\n",
    "            audio_base64=test_audio,\n",
    "            text_prompt=test_prompt,\n",
    "            model=model_name,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        print(f\"\\n--- Voxtral {model_name.title()} ({latency:.2f}s) ---\")\n",
    "        print(result['choices'][0]['message']['content'])\n",
    "        print(f\"\\nTokens: {result['usage']}\")\n",
    "else:\n",
    "    print(\"No audio loaded for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Streaming Responses\n",
    "\n",
    "Stream responses in real-time for better user experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_voxtral(\n",
    "    audio_base64: str,\n",
    "    text_prompt: str,\n",
    "    model: str = \"small\",\n",
    "    max_tokens: int = 500\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream response from Voxtral model.\n",
    "    \n",
    "    Yields:\n",
    "        Text chunks as they're generated\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_audio\",\n",
    "                        \"input_audio\": {\n",
    "                            \"data\": audio_base64,\n",
    "                            \"format\": \"wav\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": text_prompt}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    response = bedrock.invoke_model_with_response_stream(\n",
    "        modelId=MODELS[model],\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    for event in response['body']:\n",
    "        chunk = event.get('chunk')\n",
    "        if chunk:\n",
    "            data = json.loads(chunk['bytes'].decode())\n",
    "            if 'choices' in data and data['choices']:\n",
    "                delta = data['choices'][0].get('delta', {})\n",
    "                content = delta.get('content', '')\n",
    "                if content:\n",
    "                    yield content\n",
    "\n",
    "\n",
    "# Test streaming with JFK audio\n",
    "print(\"Streaming Transcription:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"jfk\" in audio_data:\n",
    "    full_response = \"\"\n",
    "    for chunk in stream_voxtral(\n",
    "        audio_base64=audio_data[\"jfk\"][\"base64\"],\n",
    "        text_prompt=\"Transcribe\",\n",
    "        model=\"small\"\n",
    "    ):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        full_response += chunk\n",
    "    \n",
    "    print(f\"\\n\\nTotal characters streamed: {len(full_response)}\")\n",
    "else:\n",
    "    print(\"JFK audio not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Audio Understanding\n",
    "\n",
    "Ask questions about the audio content beyond simple transcription.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask analytical questions about the JFK speech\n",
    "print(\"Audio Understanding - JFK Speech Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"jfk\" in audio_data:\n",
    "    questions = [\n",
    "        \"What is the main message of this speech?\",\n",
    "        \"Describe the speaker's tone and delivery style.\",\n",
    "        \"What historical context might this speech be from?\"\n",
    "    ]\n",
    "    \n",
    "    for q in questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = call_voxtral(\n",
    "            audio_base64=audio_data[\"jfk\"][\"base64\"],\n",
    "            text_prompt=q,\n",
    "            model=\"small\",\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(f\"A: {result['choices'][0]['message']['content']}\")\n",
    "else:\n",
    "    print(\"JFK audio not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiturn-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Multi-Turn Conversations\n",
    "\n",
    "Have follow-up conversations about audio content.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxtralConversation:\n",
    "    \"\"\"Manage multi-turn conversations with Voxtral.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"small\"):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "    \n",
    "    def send_with_audio(self, audio_base64: str, text: str, max_tokens: int = 500) -> str:\n",
    "        \"\"\"Send a message with audio.\"\"\"\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_audio\", \"input_audio\": {\"data\": audio_base64, \"format\": \"wav\"}},\n",
    "                {\"type\": \"text\", \"text\": text}\n",
    "            ]\n",
    "        }\n",
    "        self.messages.append(user_message)\n",
    "        return self._get_response(max_tokens)\n",
    "    \n",
    "    def send_text(self, text: str, max_tokens: int = 500) -> str:\n",
    "        \"\"\"Send a text-only follow-up message.\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": text})\n",
    "        return self._get_response(max_tokens)\n",
    "    \n",
    "    def _get_response(self, max_tokens: int) -> str:\n",
    "        \"\"\"Get response from model.\"\"\"\n",
    "        payload = {\"messages\": self.messages, \"max_tokens\": max_tokens}\n",
    "        \n",
    "        response = bedrock.invoke_model(\n",
    "            modelId=MODELS[self.model],\n",
    "            body=json.dumps(payload),\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['body'].read())\n",
    "        assistant_content = result['choices'][0]['message']['content']\n",
    "        \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "        return assistant_content\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "# Multi-turn conversation about JFK speech\n",
    "print(\"Multi-Turn Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"jfk\" in audio_data:\n",
    "    conv = VoxtralConversation(model=\"small\")\n",
    "    \n",
    "    # Turn 1: Initial question with audio\n",
    "    print(\"\\nTurn 1 - With Audio:\")\n",
    "    print(\"-\" * 40)\n",
    "    response1 = conv.send_with_audio(\n",
    "        audio_base64=audio_data[\"jfk\"][\"base64\"],\n",
    "        text=\"Who is speaking and what are they talking about?\"\n",
    "    )\n",
    "    print(f\"User: Who is speaking and what are they talking about?\")\n",
    "    print(f\"Assistant: {response1}\")\n",
    "    \n",
    "    # Turn 2: Follow-up (text only)\n",
    "    print(\"\\nTurn 2 - Text Only:\")\n",
    "    print(\"-\" * 40)\n",
    "    response2 = conv.send_text(\"When was this speech given and why was it significant?\")\n",
    "    print(f\"User: When was this speech given and why was it significant?\")\n",
    "    print(f\"Assistant: {response2}\")\n",
    "    \n",
    "    # Turn 3: Another follow-up\n",
    "    print(\"\\nTurn 3 - Text Only:\")\n",
    "    print(\"-\" * 40)\n",
    "    response3 = conv.send_text(\"What was the full quote that includes 'ask not'?\")\n",
    "    print(f\"User: What was the full quote that includes 'ask not'?\")\n",
    "    print(f\"Assistant: {response3}\")\n",
    "    \n",
    "    print(f\"\\nConversation history: {len(conv.messages)} messages\")\n",
    "else:\n",
    "    print(\"JFK audio not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-audio-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Multiple Audio Files\n",
    "\n",
    "Process multiple audio inputs in a single request for comparison.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_voxtral_multi_audio(\n",
    "    audio_list: list,\n",
    "    text_prompt: str,\n",
    "    model: str = \"small\",\n",
    "    max_tokens: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Call Voxtral with multiple audio files.\n",
    "    \n",
    "    Args:\n",
    "        audio_list: List of base64-encoded audio strings\n",
    "        text_prompt: Text question about the audio files\n",
    "        model: 'mini' or 'small'\n",
    "        max_tokens: Maximum tokens in response\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    \n",
    "    for audio_b64 in audio_list:\n",
    "        content.append({\n",
    "            \"type\": \"input_audio\",\n",
    "            \"input_audio\": {\"data\": audio_b64, \"format\": \"wav\"}\n",
    "        })\n",
    "    \n",
    "    content.append({\"type\": \"text\", \"text\": text_prompt})\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[model],\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    return json.loads(response['body'].read())\n",
    "\n",
    "\n",
    "# Compare two audio files\n",
    "print(\"Multiple Audio Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"jfk\" in audio_data and \"librispeech\" in audio_data:\n",
    "    print(\"Audio 1: JFK Speech\")\n",
    "    print(\"Audio 2: LibriSpeech Sample\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = call_voxtral_multi_audio(\n",
    "        audio_list=[\n",
    "            audio_data[\"jfk\"][\"base64\"],\n",
    "            audio_data[\"librispeech\"][\"base64\"]\n",
    "        ],\n",
    "        text_prompt=\"Compare these two audio samples. Describe the differences in content, speaker style, and audio quality.\",\n",
    "        model=\"small\",\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "    print(f\"\\nTokens: {result['usage']}\")\n",
    "else:\n",
    "    print(\"Need both audio files loaded for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Best Practices & Limitations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "| Use Case | Recommended Model | Reasoning |\n",
    "|----------|-------------------|------------|\n",
    "| Quick classification | **Voxtral Mini** | Fast, low latency |\n",
    "| Simple transcription | **Voxtral Mini** | Cost-effective for basic tasks |\n",
    "| Detailed analysis | **Voxtral Small** | Better comprehension |\n",
    "| Complex conversations | **Voxtral Small** | Stronger reasoning |\n",
    "\n",
    "### Audio Format Requirements\n",
    "\n",
    "| Specification | Recommendation |\n",
    "|---------------|----------------|\n",
    "| **Format** | WAV (16-bit PCM) - convert from MP3/OGG/FLAC |\n",
    "| **Sample Rate** | 16kHz for speech |\n",
    "| **Channels** | Mono recommended |\n",
    "| **Max Duration** | ~45 seconds per request (see payload limits below) |\n",
    "\n",
    "### InvokeModel API Limitations\n",
    "\n",
    "| Limitation | Value | Notes |\n",
    "|------------|-------|-------|\n",
    "| **Request payload** | ~2 MB | Voxtral-specific limit on Bedrock |\n",
    "| **Max audio per request** | ~45 seconds | At 16kHz mono 16-bit WAV |\n",
    "| **Recommended chunk size** | 30 seconds | With 2s overlap for word boundaries |\n",
    "\n",
    "**Why ~2MB?** While Bedrock's InvokeModel API allows up to 25MB payloads, Voxtral models on Bedrock have a stricter ~2MB limit. This restricts audio to ~45 seconds per request.\n",
    "\n",
    "### Transcribing Long Audio (30+ minutes)\n",
    "\n",
    "For audio longer than ~45 seconds, you **must chunk the audio** and make sequential API calls:\n",
    "\n",
    "1. **Cannot use multiple audio files** - Multiple `input_audio` blocks still count against the ~2MB limit\n",
    "2. **Must chunk sequentially** - Process 30-second segments one at a time\n",
    "3. **Use overlap** - 2-second overlap prevents cutting words at boundaries\n",
    "4. **Combine results** - Concatenate transcriptions (may have minor repetition at boundaries)\n",
    "\n",
    "See the `transcribe_long_audio()` function above for a working implementation.\n",
    "\n",
    "### Supported Input Formats\n",
    "\n",
    "| Format | Support | Notes |\n",
    "|--------|---------|-------|\n",
    "| **WAV** | Direct | 16-bit PCM recommended |\n",
    "| **OGG** | Convert | Use soundfile to convert to WAV |\n",
    "| **MP3** | Convert | May cause errors; convert to WAV first |\n",
    "| **FLAC** | Convert | Use soundfile to convert to WAV |\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "1. **Payload Size**: ~2MB limit restricts audio to ~45 seconds per request\n",
    "2. **MP3 Format**: Not directly supported - must convert to WAV first\n",
    "3. **Audio URLs**: Only base64-encoded audio is supported (no URL references)\n",
    "4. **Converse API**: Does not support audio content blocks\n",
    "5. **Tool Calling**: Not available for Voxtral models on Bedrock\n",
    "\n",
    "### Tips for Best Results\n",
    "\n",
    "1. **Convert audio**: Always convert to 16-bit PCM WAV at 16kHz mono\n",
    "2. **Chunk long audio**: Use 30-second chunks with 2-second overlap\n",
    "3. **Calculate size**: At 16kHz mono: 1 second ≈ 32KB WAV ≈ 43KB base64\n",
    "4. **Use clear prompts**: Be specific about transcription vs. analysis\n",
    "5. **Use streaming**: For interactive applications, stream for better UX\n",
    "6. **Multi-turn for context**: Use conversations for follow-up questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "This notebook uses Amazon Bedrock's serverless inference, so there are no persistent resources to clean up. You are charged based on token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any conversation history\n",
    "if 'conv' in dir():\n",
    "    conv.clear()\n",
    "\n",
    "print(\"Notebook complete!\")\n",
    "print(\"\\nVoxtral models tested:\")\n",
    "for name, model_id in MODELS.items():\n",
    "    print(f\"  - {name}: {model_id}\")\n",
    "print(\"\\nAudio files used:\")\n",
    "for name in audio_data.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

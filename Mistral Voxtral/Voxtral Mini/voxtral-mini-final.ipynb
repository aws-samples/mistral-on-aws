{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b115619-5cb1-48a4-ac06-e6025613ea74",
   "metadata": {},
   "source": [
    "# Voxtral Mini Audio-Language Model Deployment on SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy and use Mistral AI's Voxtral Mini model on Amazon SageMaker for real-time audio and text processing.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Voxtral-Mini-3B-2507 - A multimodal model that can process both audio and text\n",
    "- **Capabilities**: Audio transcription, audio understanding, text generation, and multimodal conversations\n",
    "- **Deployment**: Real-time inference endpoint on SageMaker with GPU acceleration\n",
    "\n",
    "## Key Features Demonstrated\n",
    "1. **Text-only conversations** - Standard language model interactions\n",
    "2. **Audio-only processing** - Direct audio analysis and description\n",
    "3. **Audio + text combinations** - Guided audio analysis with specific questions\n",
    "4. **Multi-audio processing** - Analyze multiple audio files together\n",
    "5. **Multi-turn conversations** - Complex dialogues with mixed media\n",
    "6. **Transcription-only mode** - Pure speech-to-text conversion\n",
    "\n",
    "\n",
    "## Below solution is tested in the following environment setup\n",
    "1. This solution is tested in **us-west-2** region.  Please modify the region based on your requirement. \n",
    "2. SageMaker Domain JupyterLab with instance **ml.g6.8xlarge** and **100GB storage**\n",
    "3. Ensure you have enough quota for **ml.g6.8xlarge** to host this model on SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416333e3-20cd-4702-b551-ba8868a5664a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2e3c9e-73ac-46b5-aad8-b72403d8a6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:05:49.222437Z",
     "iopub.status.busy": "2025-08-12T14:05:49.221960Z",
     "iopub.status.idle": "2025-08-12T14:05:49.224875Z",
     "shell.execute_reply": "2025-08-12T14:05:49.224485Z",
     "shell.execute_reply.started": "2025-08-12T14:05:49.222419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84023f2-c0c6-48f6-8be3-a216aa5d93b5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-12T14:05:50.805181Z",
     "iopub.status.busy": "2025-08-12T14:05:50.804465Z",
     "iopub.status.idle": "2025-08-12T14:05:52.135929Z",
     "shell.execute_reply": "2025-08-12T14:05:52.135312Z",
     "shell.execute_reply.started": "2025-08-12T14:05:50.805164Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 2)) (4.55.0)\n",
      "Requirement already satisfied: librosa>=0.10.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: soundfile>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 8)) (0.34.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 9)) (2.32.4)\n",
      "Requirement already satisfied: mistral-common in /opt/conda/lib/python3.12/site-packages (from -r code/requirements.txt (line 10)) (1.8.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->-r code/requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->-r code/requirements.txt (line 1)) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->-r code/requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->-r code/requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->-r code/requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->-r code/requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->-r code/requirements.txt (line 1)) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.55.0->-r code/requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.55.0->-r code/requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.55.0->-r code/requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.55.0->-r code/requirements.txt (line 2)) (0.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.55.0->-r code/requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->-r code/requirements.txt (line 8)) (1.1.5)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.12/site-packages (from librosa>=0.10.0->-r code/requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.12/site-packages (from soundfile>=0.12.0->-r code/requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate>=0.24.0->-r code/requirements.txt (line 6)) (5.9.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->-r code/requirements.txt (line 9)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->-r code/requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->-r code/requirements.txt (line 9)) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->-r code/requirements.txt (line 9)) (2025.6.15)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7 in /opt/conda/lib/python3.12/site-packages (from mistral-common->-r code/requirements.txt (line 10)) (2.11.7)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /opt/conda/lib/python3.12/site-packages (from mistral-common->-r code/requirements.txt (line 10)) (4.23.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from mistral-common->-r code/requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from mistral-common->-r code/requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /opt/conda/lib/python3.12/site-packages (from mistral-common->-r code/requirements.txt (line 10)) (11.2.1)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /opt/conda/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common->-r code/requirements.txt (line 10)) (2.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral-common->-r code/requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral-common->-r code/requirements.txt (line 10)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral-common->-r code/requirements.txt (line 10)) (0.4.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.0->-r code/requirements.txt (line 4)) (2.22)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common->-r code/requirements.txt (line 10)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common->-r code/requirements.txt (line 10)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common->-r code/requirements.txt (line 10)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common->-r code/requirements.txt (line 10)) (0.25.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.12/site-packages (from numba>=0.51.0->librosa>=0.10.0->-r code/requirements.txt (line 3)) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from pooch>=1.1->librosa>=0.10.0->-r code/requirements.txt (line 3)) (4.3.8)\n",
      "Requirement already satisfied: pycountry>=23 in /opt/conda/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common->-r code/requirements.txt (line 10)) (24.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa>=0.10.0->-r code/requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=2.1.0->-r code/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.1.0->-r code/requirements.txt (line 1)) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b0444-c84f-4b59-8b8c-bcebf7bac411",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "Install required packages and initialize SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f18a90a-e1e8-4cb0-9857-8ec48683e5ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:05:55.545098Z",
     "iopub.status.busy": "2025-08-12T14:05:55.544655Z",
     "iopub.status.idle": "2025-08-12T14:05:57.214630Z",
     "shell.execute_reply": "2025-08-12T14:05:57.214153Z",
     "shell.execute_reply.started": "2025-08-12T14:05:55.545075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker role: arn:aws:iam::459006231907:role/service-role/AmazonSageMaker-ExecutionRole-20250606T154579\n",
      "S3 bucket: 3p-projects\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for SageMaker deployment and audio processing\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import json\n",
    "import base64\n",
    "import numpy as np\n",
    "import librosa\n",
    "from typing import Dict, List, Any, Optional\n",
    "import time\n",
    "import io\n",
    "\n",
    "# Initialize SageMaker session and get execution role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()  # IAM role for SageMaker operations\n",
    "bucket = \"<UPDATE_WITH_YOUR_BUCKET>\"  # S3 bucket for storing model artifacts\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f2d9c-5ef6-46bd-9792-2189c6bee4b0",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Model\n",
    "\n",
    "Download the Voxtral Mini model from Hugging Face and prepare it for SageMaker deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c6534a-5078-49fe-a035-70f4076efca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:06:45.100348Z",
     "iopub.status.busy": "2025-08-12T14:06:45.100123Z",
     "iopub.status.idle": "2025-08-12T14:06:45.103333Z",
     "shell.execute_reply": "2025-08-12T14:06:45.102850Z",
     "shell.execute_reply.started": "2025-08-12T14:06:45.100333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Voxtral Mini model from Hugging Face\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "device = \"cuda\"  # Use GPU for faster model loading\n",
    "repo_id = \"mistralai/Voxtral-Mini-3B-2507\"  # Official Mistral AI model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32ca1897-cfdd-4c66-bbb0-4eb0364ddc6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:06:46.739211Z",
     "iopub.status.busy": "2025-08-12T14:06:46.738824Z",
     "iopub.status.idle": "2025-08-12T14:06:46.859992Z",
     "shell.execute_reply": "2025-08-12T14:06:46.859447Z",
     "shell.execute_reply.started": "2025-08-12T14:06:46.739194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44459ee193ca48aaafd855d9a0f0aac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./model\n"
     ]
    }
   ],
   "source": [
    "# Save model and processor locally for packaging\n",
    "local_path = \"./model\"\n",
    "\n",
    "# Save both model and processor to local directory\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=repo_id, \n",
    "    local_dir=local_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    ignore_patterns=[\"consolidated.safetensors\"]  # Skip this file\n",
    ")\n",
    "\n",
    "print(f\"Model saved to: {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69eda86b-258c-47d7-89f9-bf7ba36f8c1d",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-12T14:07:11.813767Z",
     "iopub.status.busy": "2025-08-12T14:07:11.813426Z",
     "iopub.status.idle": "2025-08-12T14:07:14.429027Z",
     "shell.execute_reply": "2025-08-12T14:07:14.428424Z",
     "shell.execute_reply.started": "2025-08-12T14:07:11.813752Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  pigz\n",
      "0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 63.6 kB of archives.\n",
      "After this operation, 162 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pigz amd64 2.6-1 [63.6 kB]\n",
      "Fetched 63.6 kB in 0s (169 kB/s)m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package pigz.\n",
      "(Reading database ... 14572 files and directories currently installed.)\n",
      "Preparing to unpack .../archives/pigz_2.6-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking pigz (2.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up pigz (2.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "#Please based on your environment, select the correct method to install the pigz package\n",
    "!sudo apt install pigz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af846d5a-1ac1-48fb-89a7-592ae2d39c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Package model as tar.gz and upload to S3\n",
    "# Using pigz for faster compression (parallel gzip)\n",
    "!tar --use-compress-program=pigz -cf model.tar.gz -C model/ .\n",
    "\n",
    "# Upload compressed model to S3\n",
    "sess = sagemaker.session.Session()\n",
    "prefix = \"voxtral\"  # S3 prefix for organization\n",
    "model_uri = sess.upload_data(\n",
    "    'model.tar.gz', \n",
    "    bucket=bucket, \n",
    "    key_prefix=f\"{prefix}/huggingface/mini\"\n",
    ")\n",
    "\n",
    "# Clean up local files to save disk space\n",
    "!rm model.tar.gz\n",
    "!rm -rf model\n",
    "\n",
    "print(f\"Model uploaded to: {model_uri}\")\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907aba6e-9821-41a7-b5ef-c78d7e40d79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31601bad-534a-4b91-8c0d-f1dde1215f2a",
   "metadata": {},
   "source": [
    "## 3. Deploy Model on SageMaker\n",
    "\n",
    "Create a real-time inference endpoint for the Voxtral Mini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d635d91-eaaf-4152-b8b4-d00bbad0025b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:09:33.905819Z",
     "iopub.status.busy": "2025-08-12T14:09:33.905311Z",
     "iopub.status.idle": "2025-08-12T14:09:33.909290Z",
     "shell.execute_reply": "2025-08-12T14:09:33.908891Z",
     "shell.execute_reply.started": "2025-08-12T14:09:33.905799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: voxtral-mini-model-1755007773\n",
      "Container image: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.6.0-transformers4.49.0-gpu-py312-cu124-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# Configuration for SageMaker deployment\n",
    "id = int(time.time())  # Unique identifier for resources\n",
    "model_name = f'voxtral-mini-model-{id}'\n",
    "\n",
    "# SageMaker container image with PyTorch and Transformers\n",
    "# NOTE: Update this URI for your specific AWS region\n",
    "image = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.6.0-transformers4.49.0-gpu-py312-cu124-ubuntu22.04\"\n",
    "\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Container image: {image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c789d3d-2d40-424c-85fe-34eb78b848eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:09:58.254947Z",
     "iopub.status.busy": "2025-08-12T14:09:58.254396Z",
     "iopub.status.idle": "2025-08-12T14:09:58.363106Z",
     "shell.execute_reply": "2025-08-12T14:09:58.362632Z",
     "shell.execute_reply.started": "2025-08-12T14:09:58.254929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker model configuration created\n"
     ]
    }
   ],
   "source": [
    "# Create SageMaker model configuration\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "voxtral_mini_model = HuggingFaceModel(\n",
    "    model_data=model_uri,              # S3 location of model artifacts\n",
    "    role=role,                         # IAM execution role\n",
    "    image_uri=image,                   # Container image\n",
    "    entry_point=\"inference.py\",        # Custom inference script\n",
    "    source_dir='code',                 # Directory containing inference code\n",
    "    name=model_name,\n",
    "    env={\n",
    "        # Increase timeouts and payload limits for audio processing\n",
    "        'MMS_MAX_REQUEST_SIZE': '2000000000',    \n",
    "        'MMS_MAX_RESPONSE_SIZE': '2000000000',    \n",
    "        'MMS_DEFAULT_RESPONSE_TIMEOUT': '900'    \n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"SageMaker model configuration created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4155363-7a89-4291-ad21-f35e4bb7b22b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:10:02.466266Z",
     "iopub.status.busy": "2025-08-12T14:10:02.465829Z",
     "iopub.status.idle": "2025-08-12T14:30:25.898769Z",
     "shell.execute_reply": "2025-08-12T14:30:25.898246Z",
     "shell.execute_reply.started": "2025-08-12T14:10:02.466250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying endpoint: voxtral-mini-real-time-endpoint-1755007773\n",
      "This will take approximately 20 minutes...\n",
      "-----------!✅ Endpoint deployed successfully: voxtral-mini-real-time-endpoint-1755007773\n",
      "CPU times: user 13min 56s, sys: 30.8 s, total: 14min 27s\n",
      "Wall time: 20min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Deploy the model to a real-time inference endpoint\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "endpoint_name = f'voxtral-mini-real-time-endpoint-{id}'\n",
    "\n",
    "print(f\"Deploying endpoint: {endpoint_name}\")\n",
    "print(\"This will take approximately 20 minutes...\")\n",
    "\n",
    "predictor = voxtral_mini_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g6.8xlarge\",              # GPU instance for fast inference\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),                # Handle JSON requests\n",
    "    deserializer=JSONDeserializer(),            # Handle JSON responses\n",
    "    container_startup_health_check_timeout=900, # Extended startup timeout\n",
    "    model_data_download_timeout=900,            # Extended download timeout\n",
    ")\n",
    "\n",
    "print(f\"✅ Endpoint deployed successfully: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2269be8c-17b6-4eda-8f4b-4714218f1b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d7c51d4-cb87-48af-bb5a-608b7abc59d1",
   "metadata": {},
   "source": [
    "## 4. Test Different Use Cases\n",
    "\n",
    "Now let's test the deployed model with various input types and scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13951fe-af22-4654-853c-9dbe706b03e7",
   "metadata": {},
   "source": [
    "### 4.1 Text-Only Conversation\n",
    "\n",
    "Test the model's text generation capabilities without any audio input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0113d96-cec1-45f1-832d-61fba41f3a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:32:43.824669Z",
     "iopub.status.busy": "2025-08-12T14:32:43.824215Z",
     "iopub.status.idle": "2025-08-12T14:32:44.975752Z",
     "shell.execute_reply": "2025-08-12T14:32:44.975210Z",
     "shell.execute_reply.started": "2025-08-12T14:32:43.824652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Testing text-only conversation...\n",
      "Response: Hello! I'm functioning as intended, thank you. How about you? How's your day going?\n"
     ]
    }
   ],
   "source": [
    "# Simple text-only conversation\n",
    "payload = {\n",
    "    \"text\": \"Hello, how are you today?\",\n",
    "    \"max_new_tokens\": 100,      # Limit response length\n",
    "    \"temperature\": 0.3          # Lower temperature for more focused responses\n",
    "}\n",
    "\n",
    "print(\"🔤 Testing text-only conversation...\")\n",
    "response = predictor.predict(payload)\n",
    "print(\"Response:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cbc24-6b36-4d22-93bd-e9dab067e105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f877218-7ea0-422e-90cb-da9e72a5006c",
   "metadata": {},
   "source": [
    "### 4.2 Audio-Only Analysis\n",
    "\n",
    "Test the model's ability to understand and describe audio content without additional text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1fe7442-c2f2-4ee6-9d3a-a827c1d29f2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:32:48.945387Z",
     "iopub.status.busy": "2025-08-12T14:32:48.944945Z",
     "iopub.status.idle": "2025-08-12T14:32:58.660621Z",
     "shell.execute_reply": "2025-08-12T14:32:58.660012Z",
     "shell.execute_reply.started": "2025-08-12T14:32:48.945367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎵 Testing audio-only analysis...\n",
      "Response: The audio describes a dramatic moment in a baseball game, likely a playoff or championship game, where a home run is hit, and the team is celebrating their potential to win the American League Championship. Here's a breakdown:\n",
      "\n",
      "1. **Pitcher and Batter**: The pitcher is 0-1, meaning he has not yet struck out the batter. The batter is Edgar Martinez, a known power hitter.\n",
      "\n",
      "2. **Swing and Hit**: The pitcher swings and misses, and the batter hits a home run. The ball goes over the left field line, resulting in a home run.\n",
      "\n",
      "3. **Base Running**: The runner, likely named Joy, is at first base. Junior is at third base, and the team is trying to wave him in to score.\n",
      "\n",
      "4. **Potential Run**: If Junior scores, it would be a potential run for the team, which is already celebrating the home run.\n",
      "\n",
      "5. **American League Championship**: The team is aiming to play for the American League Championship, which is a significant achievement in baseball.\n",
      "\n",
      "6. **Celebration**: The team is excited and celebrating the home run, with the announcer expressing disbelief and amazement.\n",
      "\n",
      "The audio captures the thrill and excitement of a pivotal moment in a baseball game, with the potential for a significant victory.\n"
     ]
    }
   ],
   "source": [
    "# Audio-only analysis - model describes what it hears\n",
    "payload = {\n",
    "    \"audio\": [\n",
    "        \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\"\n",
    "    ],\n",
    "    \"max_new_tokens\": 800,     # Allow longer response for detailed analysis\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "\n",
    "print(\"🎵 Testing audio-only analysis...\")\n",
    "response = predictor.predict(payload)\n",
    "print(\"Response:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2d801-f135-4b8b-9b6b-cfffe3e786bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89d4ccbd-08a9-437e-9bba-0d784ad590c2",
   "metadata": {},
   "source": [
    "### 4.3 Local Audio File (Base64 Encoding)\n",
    "\n",
    "Test processing local audio files by encoding them as base64 strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "821f2c20-08d8-4322-8ba1-871c7f2ac48e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:33:17.554504Z",
     "iopub.status.busy": "2025-08-12T14:33:17.553892Z",
     "iopub.status.idle": "2025-08-12T14:33:20.447621Z",
     "shell.execute_reply": "2025-08-12T14:33:20.447084Z",
     "shell.execute_reply.started": "2025-08-12T14:33:17.554487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Testing base64 encoded local audio file...\n",
      "Response: In this audio, the commentator is describing a dramatic moment in a baseball game where a home run is hit, and the team is celebrating their potential to play for the American League Championship. The commentator expresses disbelief and excitement, repeatedly saying \"I don't believe it\" and \"my, oh my.\"\n"
     ]
    }
   ],
   "source": [
    "# Process local audio file via base64 encoding\n",
    "audio_file_path = \"winning_call.mp3\" \n",
    "\n",
    "# Read the audio file as binary data (preserves original format)\n",
    "with open(audio_file_path, 'rb') as audio_file:\n",
    "    audio_bytes = audio_file.read()\n",
    "\n",
    "# Encode the audio file to base64 for transmission\n",
    "audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "\n",
    "payload = {\n",
    "    \"audio\": [audio_b64],           # Base64 encoded audio\n",
    "    \"text\": \"What is being said in this audio?\",  # Specific question\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "\n",
    "print(\"📁 Testing base64 encoded local audio file...\")\n",
    "response = predictor.predict(payload)\n",
    "print(\"Response:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39447c4d-dce6-41cd-ab08-e93debb50852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0172ea-d13c-4b94-8d42-fb5eee50430e",
   "metadata": {},
   "source": [
    "### 4.4 Multi-Audio Analysis\n",
    "\n",
    "Test the model's ability to analyze multiple audio files together and answer specific questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f97a76e-7d23-45be-8056-53c0869cc0d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:33:27.192387Z",
     "iopub.status.busy": "2025-08-12T14:33:27.192051Z",
     "iopub.status.idle": "2025-08-12T14:33:29.959133Z",
     "shell.execute_reply": "2025-08-12T14:33:29.958564Z",
     "shell.execute_reply.started": "2025-08-12T14:33:27.192371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎵🎵 Testing multiple audio analysis...\n",
      "Response: The first audio clip references a nursery rhyme, specifically \"Mary Had a Little Lamb.\" The second audio clip references baseball, specifically the 1969 American League Championship Series between the Baltimore Orioles and the New York Mets.\n"
     ]
    }
   ],
   "source": [
    "# Analyze multiple audio files with a specific question\n",
    "payload = {\n",
    "    \"audio\": [\n",
    "        \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/mary_had_lamb.mp3\",\n",
    "        \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\"\n",
    "    ],\n",
    "    \"text\": \"What sport and what nursery rhyme are referenced in these audio clips?\",\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"do_sample\": False,         # Use deterministic generation\n",
    "    \"temperature\": 1.0\n",
    "}\n",
    "\n",
    "print(\"🎵🎵 Testing multiple audio analysis...\")\n",
    "response = predictor.predict(payload)\n",
    "print(\"Response:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d9916-6775-455f-838e-00add9b5d4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab934f7-2639-430e-8f80-87b470ef4742",
   "metadata": {},
   "source": [
    "### 4.5 Multi-Turn Conversation Format\n",
    "\n",
    "Test complex multi-turn conversations with mixed audio and text content using the structured conversation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c9d502-c79b-4edd-b0af-8f1d3020cb05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:36:31.652657Z",
     "iopub.status.busy": "2025-08-12T14:36:31.652327Z",
     "iopub.status.idle": "2025-08-12T14:36:39.715227Z",
     "shell.execute_reply": "2025-08-12T14:36:39.714579Z",
     "shell.execute_reply.started": "2025-08-12T14:36:31.652638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Testing multi-turn conversation format...\n",
      "Response: The previous audio was a political speech, while this audio is a sports commentary. The speaker is describing a baseball game, specifically a home run hit by Edgar Martinez, and the excitement of the moment. The speaker expresses disbelief and joy, contrasting with the previous audio's serious tone and focus on political themes.\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with audio and text across multiple exchanges\n",
    "payload = {\n",
    "    \"conversation\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"audio\",\n",
    "                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"audio\", \n",
    "                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Describe briefly what you can hear.\"},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            # Assistant's previous response (simulating conversation history)\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"The audio begins with the speaker delivering a farewell address in Chicago, reflecting on his eight years as president and expressing gratitude to the American people. The audio then transitions to a weather report, stating that it was 35 degrees in Barcelona the previous day, but the temperature would drop to minus 20 degrees the following day.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"audio\",\n",
    "                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Ok, now compare this new audio with the previous one.\"},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "\n",
    "print(\"💬 Testing multi-turn conversation format...\")\n",
    "response = predictor.predict(payload)\n",
    "print(\"Response:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282ffdc-7d06-48cb-8b59-26408593b23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7768c8d1-aa52-441a-9e0e-81d85ec2e1fc",
   "metadata": {},
   "source": [
    "### 4.6 Transcription-Only Mode\n",
    "\n",
    "Test the model's pure speech-to-text transcription capabilities without additional analysis or commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "795896cb-87e3-4921-846e-85702d6c054c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T14:36:51.123664Z",
     "iopub.status.busy": "2025-08-12T14:36:51.123119Z",
     "iopub.status.idle": "2025-08-12T14:37:13.163020Z",
     "shell.execute_reply": "2025-08-12T14:37:13.162373Z",
     "shell.execute_reply.started": "2025-08-12T14:36:51.123648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Testing transcription-only mode...\n",
      "Response: This week, I traveled to Chicago to deliver my final farewell address to the nation, following in the tradition of presidents before me. It was an opportunity to say thank you. Whether we've seen eye-to-eye or rarely agreed at all, my conversations with you, the American people, in living rooms and schools, at farms and on factory floors, at diners and on distant military outposts, All these conversations are what have kept me honest, kept me inspired, and kept me going. Every day, I learned from you. You made me a better president, and you made me a better man. Over the course of these eight years, I've seen the goodness, the resilience, and the hope of the American people. I've seen neighbors looking out for each other as we rescued our economy from the worst crisis of our lifetimes. I've hugged cancer survivors who finally know the security of affordable health care. I've seen communities like Joplin rebuild from disaster, and cities like Boston show the world that no terrorist will ever break the American spirit. I've seen the hopeful faces of young graduates and our newest military officers. I've mourned with grieving families searching for answers, and I found grace in a Charleston church. I've seen our scientists help a paralyzed man regain his sense of touch, and our wounded warriors walk again. I've seen our doctors and volunteers rebuild after earthquakes and stop pandemics in their tracks. I've learned from students who are building robots and curing diseases, and who will change the world in ways we can't even imagine. I've seen the youngest of children remind us of our obligations to care for our refugees, to work in peace, and above all, to look out for each other. That's what's possible when we come together in the slow, hard, sometimes frustrating, but always vital work of self-government. But we can't take our democracy for granted. All of us, regardless of party, should throw ourselves into the work of citizenship. Not just when there's an election. Not just when our own narrow interest is at stake. But over the full span of a lifetime. If you're tired of arguing with strangers on the Internet, try to talk with one in real life. If something needs fixing, lace up your shoes and do some organizing. If you're disappointed by your elected officials, then grab a clipboard, get some signatures, and run for office yourself. Our success depends on\n"
     ]
    }
   ],
   "source": [
    "# Pure transcription mode - convert speech to text only\n",
    "payload = {\n",
    "    \"audio\": [\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\"],\n",
    "    \"transcribe_only\": True,    # Enable transcription-only mode\n",
    "    \"language\": \"en\",           # Specify language for better accuracy\n",
    "    \"max_new_tokens\": 500       # Allow sufficient tokens for full transcription\n",
    "}\n",
    "\n",
    "print(\"📝 Testing transcription-only mode...\")\n",
    "response = predictor.predict(payload)\n",
    "print(\"Response:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8023d",
   "metadata": {},
   "source": [
    "## 5. Summary and Next Steps\n",
    "\n",
    "### What We've Demonstrated\n",
    "\n",
    "This notebook showed how to successfully deploy and use Mistral AI's Voxtral Mini model on SageMaker for various audio and text processing tasks:\n",
    "\n",
    "1. **✅ Text-only conversations** - Standard language model interactions\n",
    "2. **✅ Audio-only analysis** - Automatic audio content description and understanding  \n",
    "3. **✅ Guided audio analysis** - Audio processing with specific text instructions\n",
    "4. **✅ Multi-audio processing** - Simultaneous analysis of multiple audio files\n",
    "5. **✅ Multi-turn conversations** - Complex dialogues with mixed media content\n",
    "6. **✅ Pure transcription** - Speech-to-text conversion without additional analysis\n",
    "\n",
    "### Key Features of Our Implementation\n",
    "\n",
    "- **Flexible input formats**: Supports URLs, base64 encoded files, and conversation structures\n",
    "- **Memory optimized**: Handles large audio files with automatic cleanup\n",
    "- **Production ready**: Comprehensive error handling and logging\n",
    "- **Scalable**: Real-time inference endpoint with GPU acceleration\n",
    "\n",
    "### Supported Audio Formats\n",
    "- MP3, WAV, M4A, and other common audio formats\n",
    "- Both remote URLs and local files (via base64 encoding)\n",
    "- Multiple audio files in a single request\n",
    "\n",
    "### Next Steps\n",
    "1. **Scale the deployment** - Add auto-scaling policies for production workloads\n",
    "2. **Add monitoring** - Set up CloudWatch metrics and alarms\n",
    "3. **Implement caching** - Cache frequent requests to reduce costs\n",
    "4. **Add security** - Implement authentication and authorization\n",
    "5. **Optimize costs** - Consider using spot instances or scheduled scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53064c01",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "**⚠️ Important**: Remember to delete your SageMaker endpoint when you're done testing to avoid ongoing charges.\n",
    "\n",
    "Uncomment and run the cell below to delete the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Delete the SageMaker endpoint to avoid charges\n",
    "\n",
    "print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "print(\"✅ Endpoint deleted successfully\")\n",
    "\n",
    "# You can also delete the model if no longer needed:  \n",
    "predictor.delete_model()\n",
    "print(\"✅ Model deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

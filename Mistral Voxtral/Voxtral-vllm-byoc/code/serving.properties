# Voxtral Mini vLLM Serving Configuration for SageMaker BYOC
# ============================================================
# This configuration file sets up vLLM to serve the Voxtral Mini model
# on Amazon SageMaker using a custom container with vLLM v0.10.0+

# Model configuration for BYOC deployment (as per official requirements)
engine=Python
option.model_id=mistralai/Voxtral-Small-24B-2507
option.tensor_parallel_degree=4
option.rolling_batch=vllm
option.dtype=bfloat16

# vLLM specific settings for Voxtral (as per official HF documentation)
option.tokenizer_mode=mistral
option.config_format=mistral
option.load_format=mistral
option.trust_remote_code=true
option.download_dir=/tmp/model_cache

# Memory and performance optimization (for vLLM v0.10.0+)
option.gpu_memory_utilization=0.9
# Voxtral has 32k token context length
option.max_model_len=32768
option.pipeline_parallel_size=1
option.block_size=16
option.swap_space=8
option.cpu_offload_gb=4 

# Audio processing settings (as per Voxtral specifications)
option.limit_mm_per_prompt=audio:8
option.max_seq_len_to_capture=32768
# Voxtral supports up to 30 minutes for transcription, 40 minutes for understanding
option.mm_processor_kwargs={"audio_sampling_rate": 16000, "audio_max_length": 1800.0}

# Request handling (optimized for BYOC)
option.max_rolling_batch_size=16
option.max_rolling_batch_prefill_tokens=8192
option.batch_size=16

# Generation parameters (as recommended by Voxtral)
option.max_new_tokens=2048
# Temperature 0.2 for chat, 0.0 for transcription (configurable at request time)
option.temperature=0.2
option.top_p=0.95
option.repetition_penalty=1.0

# System optimization (latest vLLM features)
option.enable_chunked_prefill=true
option.enforce_eager=false
option.enable_prefix_caching=true
option.use_v2_block_manager=true
option.kv_cache_dtype=auto

# Logging and monitoring (BYOC optimized)
option.disable_log_stats=false
option.log_level=INFO
option.enable_metrics=true

# Health check and timeouts (extended for BYOC)
option.model_loading_timeout=3600
option.health_check_grace_period=600

# OpenAI API compatibility
option.served_model_name=mistralai/Voxtral-Small-24B-2507
option.api_key_enabled=false
option.response_role=assistant

# Performance tuning (optimized for custom container)
option.scheduler_delay_factor=0.0
option.enable_batch_padding=true
option.preemption_mode=swap
option.worker_use_ray=true
option.distributed_executor_backend=ray

# Container-specific optimizations
option.engine_use_ray=true
option.disable_custom_all_reduce=false
option.worker_multiproc_method=spawn
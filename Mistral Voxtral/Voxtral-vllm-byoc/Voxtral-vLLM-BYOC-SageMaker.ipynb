{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxtral vLLM BYOC Deployment on SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy Mistral AI's Voxtral models using a custom vLLM container (BYOC - Bring Your Own Container) on Amazon SageMaker.\n",
    "\n",
    "## Overview\n",
    "- **Models**: Voxtral-Mini-3B-2507 and Voxtral-Small-24B-2507\n",
    "- **Engine**: vLLM v0.10.0+ (required for Voxtral support)\n",
    "- **Deployment**: Custom Docker container with BYOC approach\n",
    "- **Features**: Multimodal audio+text processing, function calling (Small model), transcription\n",
    "- **Context**: 32k token context length, up to 30min audio transcription, 40min audio understanding\n",
    "\n",
    "## Key Advantages of BYOC Approach\n",
    "1. **Latest vLLM version** - Control over vLLM version (v0.10.0+) with Voxtral support\n",
    "2. **Official configurations** - Uses official Voxtral server parameters\n",
    "3. **Full control** - Complete control over container environment and dependencies\n",
    "4. **Future-proof** - Easy updates to new vLLM versions\n",
    "5. **Flexible architecture** - Separate container image from model code for faster iterations\n",
    "\n",
    "## Supported Models\n",
    "- **Voxtral-Mini-3B-2507**: Text + audio processing, ml.g6.4xlarge instance\n",
    "- **Voxtral-Small-24B-2507**: Text + audio + function calling, ml.g6.12xlarge instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "Install required packages and initialize SageMaker session.\n",
    "\n",
    "**Prerequisites**: Docker image should be built and pushed before running this notebook. See README for build instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "AWS Account ID: 459006231907\n",
      "AWS Region: us-west-2\n",
      "SageMaker role: arn:aws:iam::459006231907:role/mistral-workshop-SageMakerExecutionRole\n",
      "S3 bucket: 3p-projects\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for SageMaker BYOC deployment\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Initialize SageMaker session and get execution role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()  # IAM role for SageMaker operations\n",
    "# role = \"arn:aws:iam::459006231907:role/service-role/AmazonSageMaker-ExecutionRole-20250606T154579\" \n",
    "bucket = \"3p-projects\"  # S3 bucket for storing model artifacts\n",
    "\n",
    "# Get AWS account and region information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"AWS Account ID: {account_id}\")\n",
    "print(f\"AWS Region: {region}\")\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Container Configuration\n",
    "\n",
    "Configure the custom container image URI. The Docker image should be pre-built with vLLM v0.10.0+ and base dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom container will be built and pushed to:\n",
      "Repository: voxtral-vllm-byoc\n",
      "Image URI: 459006231907.dkr.ecr.us-west-2.amazonaws.com/voxtral-vllm-byoc:latest\n"
     ]
    }
   ],
   "source": [
    "# Configuration for custom container\n",
    "repository_name = \"voxtral-vllm-byoc\"\n",
    "image_tag = \"latest\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repository_name}:{image_tag}\"\n",
    "\n",
    "print(f\"Custom container will be built and pushed to:\")\n",
    "print(f\"Repository: {repository_name}\")\n",
    "print(f\"Image URI: {image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ BYOC model artifacts ready:\n",
      "  ‚úÖ model.py (23287 bytes)\n",
      "  ‚úÖ serving.properties (2581 bytes)\n",
      "  ‚úÖ requirements.txt (693 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Prepare BYOC model artifacts from code directory\n",
    "# Files are already organized in the code/ directory structure\n",
    "byoc_code_dir = \"./code\"\n",
    "\n",
    "# Verify required files exist in code directory\n",
    "required_files = [\"model.py\", \"serving.properties\", \"requirements.txt\"]\n",
    "missing_files = []\n",
    "\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(byoc_code_dir, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"‚ùå Missing required files: {missing_files}\")\n",
    "    print(\"Please ensure all files are in the code/ directory.\")\n",
    "else:\n",
    "    print(\"üìÅ BYOC model artifacts ready:\")\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(byoc_code_dir, file)\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"  ‚úÖ {file} ({file_size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ BYOC configuration uploaded to: s3://3p-projects/voxtral-vllm-byoc/code\n"
     ]
    }
   ],
   "source": [
    "# Upload BYOC configuration to S3\n",
    "prefix = \"voxtral-vllm-byoc\"  # Se bucket folder prefix\n",
    "\n",
    "byoc_config_uri = sagemaker_session.upload_data(\n",
    "    path=byoc_code_dir, \n",
    "    bucket=bucket, \n",
    "    key_prefix=f\"{prefix}/code\"\n",
    ")\n",
    "\n",
    "print(f\"üì§ BYOC configuration uploaded to: {byoc_config_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy Custom vLLM Model on SageMaker\n",
    "\n",
    "Create a real-time inference endpoint using the custom vLLM container.\n",
    "\n",
    "**Note**: Docker build and push should be completed before running this notebook. See README for build instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: voxtral-vllm-byoc-model-1755863967\n",
      "Endpoint name: voxtral-vllm-byoc-endpoint-1755863967\n",
      "Custom container image: 459006231907.dkr.ecr.us-west-2.amazonaws.com/voxtral-vllm-byoc:latest\n",
      "\n",
      "üìã Instance recommendations:\n",
      "  - Voxtral-Mini-3B-2507: ml.g6.4xlarge (tensor_parallel_degree=1)\n",
      "  - Voxtral-Small-24B-2507: ml.g6.12xlarge (tensor_parallel_degree=4)\n"
     ]
    }
   ],
   "source": [
    "# Configuration for SageMaker BYOC deployment\n",
    "timestamp = int(time.time())\n",
    "model_name = f'voxtral-vllm-byoc-model-{timestamp}'\n",
    "endpoint_name = f'voxtral-vllm-byoc-endpoint-{timestamp}'\n",
    "\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "print(f\"Custom container image: {image_uri}\")\n",
    "print(f\"\\nüìã Instance recommendations:\")\n",
    "print(f\"  - Voxtral-Mini-3B-2507: ml.g6.4xlarge (tensor_parallel_degree=1)\")\n",
    "print(f\"  - Voxtral-Small-24B-2507: ml.g6.12xlarge (tensor_parallel_degree=4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data configuration: {\n",
      "  \"S3DataSource\": {\n",
      "    \"S3Uri\": \"s3://3p-projects/voxtral-vllm-byoc/code/\",\n",
      "    \"S3DataType\": \"S3Prefix\",\n",
      "    \"CompressionType\": \"None\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Prepare model data configuration for BYOC\n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{byoc_config_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Model data configuration: {json.dumps(model_data, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker model configuration for custom vLLM container\n",
    "voxtral_byoc_model = Model(\n",
    "    image_uri=image_uri,  # Use our custom container\n",
    "    model_data=model_data,  # Contains model.py, serving.properties, requirements.txt\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    env={\n",
    "        # Environment variables for our custom container\n",
    "        'MODEL_CACHE_DIR': '/opt/ml/model',\n",
    "        'TRANSFORMERS_CACHE': '/tmp/transformers_cache',\n",
    "        'HF_HOME': '/tmp/hf_home',\n",
    "        'VLLM_WORKER_MULTIPROC_METHOD': 'spawn',\n",
    "        'SAGEMAKER_BIND_TO_PORT': '8080',\n",
    "        'SAGEMAKER_BIND_TO_HOST': '0.0.0.0'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Deploying BYOC vLLM endpoint: voxtral-vllm-byoc-endpoint-1755863967\n",
      "This will take approximately 8-10 minutes...\n",
      "----------------!‚úÖ BYOC vLLM Endpoint deployed successfully: voxtral-vllm-byoc-endpoint-1755863967\n",
      "CPU times: user 181 ms, sys: 24.5 ms, total: 205 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Deploy the custom vLLM model to a real-time inference endpoint\n",
    "print(f\"üöÄ Deploying BYOC vLLM endpoint: {endpoint_name}\")\n",
    "print(\"This will take approximately 8-10 minutes...\")\n",
    "\n",
    "try:\n",
    "    predictor = voxtral_byoc_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.g6.12xlarge\",    # For Voxtral-Mini: use ml.g6.4xlarge, for Voxtral-Small: use ml.g6.12xlarge     \n",
    "        endpoint_name=endpoint_name,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "        container_startup_health_check_timeout=1200,  # Extended timeout for model loading\n",
    "        model_data_download_timeout=1800,\n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ BYOC vLLM Endpoint deployed successfully: {endpoint_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Deployment failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Custom vLLM Deployment\n",
    "\n",
    "Test the deployed model with various input types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Test Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing endpoint health...\n",
      "Response: Hello! Yes, I'm here and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Test endpoint health\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "try:\n",
    "    # Simple health check payload\n",
    "    health_payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello, are you working?\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Testing endpoint health...\")\n",
    "    response = predictor.predict(health_payload)\n",
    "    print(f\"Response: {response['choices'][0]['message']['content']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Health check failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text-Only Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Testing text-only conversation with custom vLLM...\n",
      "Response: Hello! I'd be happy to explain the advantages of using vLLM (Virtual Large Language Model) for model inference.\n",
      "\n",
      "1. **Scalability**: vLLM is designed to handle large language models efficiently. It can scale to models with billions of parameters, making it suitable for tasks that require high computational resources.\n",
      "\n",
      "2. **Efficiency**: vLLM uses a technique called \"virtual memory\" to manage the model's parameters. This allows it to use less physical memory than traditional methods, making it more efficient and reducing the cost of inference.\n",
      "\n",
      "3. **Speed**: vLLM can perform inference faster than many other methods. It achieves this by using a combination of techniques, including model parallelism and pipelining.\n",
      "\n",
      "4. **Flexibility**: vLLM supports a wide range of models and can be used for various tasks, such as text generation, translation, and summarization.\n",
      "\n",
      "5. **Ease of Use**: vLLM is designed to be easy to use.\n",
      "\n",
      "üìä Token Usage:\n",
      "  Prompt tokens: 21\n",
      "  Completion tokens: 200\n",
      "  Total tokens: 221\n"
     ]
    }
   ],
   "source": [
    "# Test text-only conversation using OpenAI format\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello! Can you tell me about the advantages of using vLLM for model inference?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "print(\"üî§ Testing text-only conversation with custom vLLM...\")\n",
    "try:\n",
    "    response = predictor.predict(payload)\n",
    "    print(\"Response:\", response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    \n",
    "    # Print usage statistics if available\n",
    "    if \"usage\" in response:\n",
    "        usage = response[\"usage\"]\n",
    "        print(f\"\\nüìä Token Usage:\")\n",
    "        print(f\"  Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "        print(f\"  Completion tokens: {usage.get('completion_tokens', 'N/A')}\")\n",
    "        print(f\"  Total tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Text conversation test failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Audio Understanding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Testing audio file url for transcription...\n",
      "Response: And the 0-1 pitch on the way to Edgar Martinez, swung on and lined down the left field line for a base hit. Here comes Joey. Here is Junior to third base. They're going to wave him in. The throw to the plate will be late. The Mariners are going to play for the American League Championship. I don't believe it. It just continues. My, oh my.\n",
      "\n",
      "üìä Token Usage:\n",
      "  Prompt tokens: 384\n",
      "  Completion tokens: 85\n",
      "  Total tokens: 469\n"
     ]
    }
   ],
   "source": [
    "# Test audio file url for transcription\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Transcribe this audio file\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"audio\",\n",
    "                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 300,\n",
    "    \"temperature\": 0.0,  # Use 0.0 for transcription tasks\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "print(\"üéµ Testing audio file url for transcription...\")\n",
    "try:\n",
    "    response = predictor.predict(payload)\n",
    "    print(\"Response:\", response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    \n",
    "    # Print usage statistics if available\n",
    "    if \"usage\" in response:\n",
    "        usage = response[\"usage\"]\n",
    "        print(f\"\\nüìä Token Usage:\")\n",
    "        print(f\"  Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "        print(f\"  Completion tokens: {usage.get('completion_tokens', 'N/A')}\")\n",
    "        print(f\"  Total tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Audio file url test failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Testing audio file base64 for audio understanding...\n",
      "Response: The audio describes a dramatic moment in a baseball game. Here's a breakdown:\n",
      "\n",
      "- The pitcher throws a pitch to Edgar Martinez.\n",
      "- Martinez hits the ball, which goes down the left field line for a base hit.\n",
      "- Jay Buhner (referred to as \"Joy\" in the audio) and Ken Griffey Jr. (referred to as \"Junior\") advance to third base and home plate, respectively.\n",
      "- The throw to the plate is late, allowing Griffey Jr. to score.\n",
      "- This play allows the Seattle Mariners to advance to the American League Championship.\n",
      "\n",
      "The commentator expresses disbelief and excitement about the Mariners' advancement.\n",
      "\n",
      "üìä Token Usage:\n",
      "  Prompt tokens: 387\n",
      "  Completion tokens: 133\n",
      "  Total tokens: 520\n"
     ]
    }
   ],
   "source": [
    "# Test audio file base64 for audio understanding\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load audio file and encode as base64\n",
    "with open(\"winning_call.mp3\", \"rb\") as audio_file:\n",
    "    audio_data = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What do you hear in this audio?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"audio\",\n",
    "                    \"data\": f\"data:audio/mp3;base64,{audio_data}\"  # Base64 encoded audio\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 300,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "print(\"üéµ Testing audio file base64 for audio understanding...\")\n",
    "\n",
    "try:\n",
    "    response = predictor.predict(payload)\n",
    "    print(\"Response:\", response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    \n",
    "    # Print usage statistics if available\n",
    "    if \"usage\" in response:\n",
    "        usage = response[\"usage\"]\n",
    "        print(f\"\\nüìä Token Usage:\")\n",
    "        print(f\"  Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "        print(f\"  Completion tokens: {usage.get('completion_tokens', 'N/A')}\")\n",
    "        print(f\"  Total tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Audio file base64 test failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Multiple Audio Files Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Testing multiple audio files support...\n",
      "Response: The first audio is a historical speech by Thomas Edison, where he recites a nursery rhyme, \"Mary Had a Little Lamb,\" which was the first words he spoke into the phonograph. The second audio is a sports commentary, specifically a baseball game, where the commentator describes a play that leads to a significant win for the Mariners. The similarities are that both audios involve a historical moment being described, but the differences lie in the content and context, with one being a technological milestone and the other a sports achievement.\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple audio files \n",
    "multi_audio_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"audio\",\n",
    "                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/mary_had_lamb.mp3\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"audio\", \n",
    "                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Compare these two audio files. What similarities and differences do you notice?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 400,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "print(\"üéµ Testing multiple audio files support...\")\n",
    "try:\n",
    "    response = predictor.predict(multi_audio_payload)\n",
    "    print(\"Response:\", response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Multiple audio test failed: {str(e)}\")\n",
    "    print(\"üí° This is expected if the model doesn't support multimodal processing yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Transcribe-only mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Testing transcribe-only mode...\n",
      "Response: This week, I traveled to Chicago to deliver my final farewell address to the nation, following in the tradition of presidents before me. It was an opportunity to say thank you. Whether we've seen eye to eye or rarely agreed at all, my conversations with you, the American people, in living rooms and schools, at farms and on factory floors, at diners and on distant military outposts, all these conversations are what have kept me honest, kept me inspired, and kept me going. Every day, I learned from you. You made me a better president, and you made me a better man. Over the course of these eight years, I've seen the goodness, the resilience, and the hope of the American people. I've seen neighbors looking out for each other as we rescued our economy from the worst crisis of our lifetimes. I've hugged cancer survivors who finally know the security of affordable health care. I've seen communities like Joplin rebuild from disaster and cities like Boston show the world that no terrorist will ever break the American spirit. I've seen the hopeful faces of young graduates and our newest military officers. I've mourned with grieving families searching for answers, and I found grace in a Charleston church. I've seen our scientists help a paralyzed man regain his sense of touch and our wounded warriors walk again. I've seen our doctors and volunteers rebuild after earthquakes and stop pandemics in their tracks. I've learned from students who are\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"audio\",\n",
    "                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 300,\n",
    "    \"temperature\": 0.0, #¬†set temperature as 0 for transcribe-only mode\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "print(\"üéµ Testing transcribe-only mode...\")\n",
    "try:\n",
    "    response = predictor.predict(payload)\n",
    "    print(\"Response:\", response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transcribe-only test failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Function calling - only supported by Voxtral Small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Function Calling...\n",
      "Response: I'll help you with that.\n",
      "üîß Tool calls found:\n",
      "  Function: get_current_weather\n",
      "  Args: {'location': 'Madrid', 'format': 'celsius'}\n",
      "  Result: It's sunny in Madrid with 25¬∞C\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define weather tool configuration\n",
    "WEATHER_TOOL = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather for a specific location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mock weather function\n",
    "def mock_weather(location, format=\"celsius\"):\n",
    "    \"\"\"Always returns sunny weather at 25¬∞C/77¬∞F\"\"\"\n",
    "    temp = 77 if format.lower() == \"fahrenheit\" else 25\n",
    "    unit = \"¬∞F\" if format.lower() == \"fahrenheit\" else \"¬∞C\"\n",
    "    return f\"It's sunny in {location} with {temp}{unit}\"\n",
    "\n",
    "# Test payload with audio\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"audio\",\n",
    "                \"path\": \"https://huggingface.co/datasets/patrickvonplaten/audio_samples/resolve/main/fn_calling.wav\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.95,\n",
    "    \"tools\": [WEATHER_TOOL]\n",
    "}\n",
    "\n",
    "\n",
    "print(\"üß™ Testing Function Calling...\")\n",
    "try:\n",
    "    # Try audio first, then text if that fails\n",
    "    response = predictor.predict(payload)\n",
    "\n",
    "    message = response[\"choices\"][0][\"message\"]\n",
    "    print(f\"Response: {message['content']}\")\n",
    "\n",
    "    # Check for tool calls\n",
    "    if \"tool_calls\" in message:\n",
    "        print(\"üîß Tool calls found:\")\n",
    "        for tool_call in message[\"tool_calls\"]:\n",
    "            func_name = tool_call[\"function\"][\"name\"]\n",
    "            func_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "            print(f\"  Function: {func_name}\")\n",
    "            print(f\"  Args: {func_args}\")\n",
    "\n",
    "            # Execute mock function\n",
    "            if func_name == \"get_current_weather\":\n",
    "                result = mock_weather(**func_args)\n",
    "                print(f\"  Result: {result}\")\n",
    "    else:\n",
    "        print(\"‚ùå No tool calls found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup Resources\n",
    "\n",
    "**Important**: Remember to delete resources when done to avoid charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Deleting endpoint: voxtral-vllm-byoc-endpoint-1755863967\n",
      "‚úÖ Endpoint deleted successfully\n"
     ]
    }
   ],
   "source": [
    "# Delete SageMaker endpoint\n",
    "print(f\"üóëÔ∏è Deleting endpoint: {endpoint_name}\")\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "print(\"‚úÖ Endpoint deleted successfully\")\n",
    "\n",
    "\n",
    "# # Delete ECR repository (optional)\n",
    "# ecr_client = boto3.client('ecr')\n",
    "# ecr_client.delete_repository(\n",
    "#     repositoryName='voxtral-vllm-byoc',\n",
    "#     force=True\n",
    "# )\n",
    "# print(\"‚úÖ ECR repository deleted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **‚úÖ Flexible BYOC Architecture** - Separated container image from model code\n",
    "2. **‚úÖ Dynamic Code Deployment** - Model artifacts provided via S3 model_data\n",
    "3. **‚úÖ Multi-Model Support** - Both Voxtral Mini and Small models supported\n",
    "4. **‚úÖ SageMaker Integration** - Custom model and endpoint creation\n",
    "5. **‚úÖ Multimodal Processing** - Audio + text processing with proper formatting\n",
    "6. **‚úÖ Function Calling** - Tool calling support for Voxtral-Small model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

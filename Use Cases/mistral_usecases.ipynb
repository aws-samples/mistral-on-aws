{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24694bff-a63c-463c-832b-588c653f128f",
   "metadata": {},
   "source": [
    "# Master the Art of Crafting, Optimizing, and Customizing Prompts for Mistral Models\n",
    "\n",
    "*This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "Mistral AI foundation models are now generally available on Amazon Bedrock. In this demo notebook, we demonstrate the top use cases with Mistral models on Amazon Bedrock:\n",
    "\n",
    "* Conversation Agents/Q&A\n",
    "* Text Summarization and Classification\n",
    "* Code Generation\n",
    "* Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936104f-513e-4d66-827a-1f6b0ba8d30a",
   "metadata": {},
   "source": [
    "---\n",
    "## Mistral Model Selection\n",
    "\n",
    "### 1. Mistral 7B Instruct\n",
    "\n",
    "- **Description:** A 7B dense Transformer model, fast-deployed and easily customizable. Small yet powerful for a variety of use cases.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 2. Mixtral 8X7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 3. Mistral Small\n",
    "\n",
    "- **Description:** Mistral Small is a highly efficient large language model optimized for high-volume, low-latency language-based tasks. It provides outstanding performance at a cost-effective price point. Key features of Mistral Small include RAG specialization, coding proficiency, and multilingual capabilities.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Optimized for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation\n",
    "\n",
    "\n",
    "### 4. Mistral Large\n",
    "\n",
    "- **Description:** Mistral AIâ€™s most advanced large language model, Mistral Large is a cutting-edge text generation model with top-tier reasoning capabilities. Its precise instruction-following abilities enables application development and tech stack modernization at scale.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** Natively fluent in English, French, Spanish, German, and Italian\n",
    "- **Supported Use Cases:** precise instruction following, text summarization, translation, complex multilingual reasoning tasks, math and coding tasks including code generation\n",
    "\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Mistral Model Selection Guide](https://docs.mistral.ai/guides/model-selection/)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08945134-9047-426d-944d-aeb8e0c9e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "mistral_small_id = \"mistral.mistral-small-2402-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d644621-a083-44d9-b3e5-4a13022b51d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bedrock APIs\n",
    "\n",
    "### [Bedrock's InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-runtime_example_bedrock-runtime_InvokeModel_MistralAi_section.html)\n",
    "\n",
    "#### InvokeModel API supported papameters\n",
    "\n",
    "The Mistral AI models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "The Mistral AI models have the following inference parameters:\n",
    "\n",
    "- Temperature - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- Top P - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- Top K - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- Maximum Length - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- Stop sequences - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "\n",
    "### [Bedrock's Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-runtime_example_bedrock-runtime_Converse_Mistral_section.html)\n",
    "\n",
    "#### Benefits of using Converse API\n",
    "\n",
    "\n",
    "- Natively adds function calling (tool use)\n",
    "- Simplifies prompt syntax for certain models - e.g: [INST] tag for Mistral\n",
    "- Unifies system prompts across models\n",
    "- Common parameters are separated from model-specific parameters\n",
    "- Does not cover image generation and embeddings\n",
    "- Unifies API for multi-turn conversations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f256f5-1c3b-45f3-af4d-1d4d7b6a2909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a329e3-0deb-4688-aca5-c9e507f5f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self.bedrock = boto3.client(service_name=\"bedrock-runtime\")\n",
    "        \n",
    "    def invoke(self, prompt, temperature=0.0, max_tokens=128):\n",
    "        body = json.dumps({\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"prompt\": prompt, \n",
    "            \"stop\": [\"</s>\"]\n",
    "        })\n",
    "        response = self.bedrock.invoke_model(\n",
    "            body=body, \n",
    "            modelId=self.model_id)\n",
    "\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body['outputs'][0]['text']\n",
    "    \n",
    "    \n",
    "    \n",
    "llm = LLM(mixtral8x7b_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c92529-d4d1-4674-bde2-2ce0567a2b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"<s>[INST] What is the capital of France? [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f15d639-a290-4b42-8f09-cf128724bbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris. It's located in the north-central part of the country and is one of the most populous and visited cities in the European Union. Paris is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Champs-Ã‰lysÃ©es. It's also famous for its cuisine, fashion, and art scene. The city has a rich history and has played a significant role in shaping Western civilization.\n"
     ]
    }
   ],
   "source": [
    "response_text = llm.invoke(\n",
    "    prompt,\n",
    "    temperature=0.0,\n",
    "    max_tokens=128,\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559992a2-ac85-4ce7-8848-d471b6cb7120",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conversation Agents/Q&A\n",
    "\n",
    "Mistral models can be utilized for developing powerful chatbots due to their ability to understand and generate human-like responses, while optimized for low latency, high throughput and cost efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c61df-02e3-4038-b28a-38733f323195",
   "metadata": {},
   "source": [
    "### Example: Chatbot with persona\n",
    "\n",
    "Chatbot with persona AI assistant will play the role of AWS customer service assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fddfa-0fae-46af-a594-735d18bd7d73",
   "metadata": {},
   "source": [
    "When implementing a chatbot, it needs to retain the context of previous interactions. Let's begin incorporating the chat history into the prompt and storing the history in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa1ae89-45b7-4ea1-bd9d-9ac742b24ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def chat_history_to_string(memory):\n",
    "    history_str = \"\"\n",
    "    for chat_item in memory:\n",
    "        role = chat_item.get(\"role\", \"\")\n",
    "        content = chat_item.get(\"content\", \"\")\n",
    "        history_str += f\"{role}: {content}\\n\\n\"\n",
    "    return history_str.strip()\n",
    "\n",
    "def format_conversation(user_input: str, memory: List[Dict[str, str]] = []) -> str:\n",
    "    \n",
    "    history = chat_history_to_string(memory)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    <s>[INST] You are a knowledgeable helpful AWS customer service assistant. You are helpful and provide general guidance from the context less than 100 words in the scope.[/INST]\n",
    "    {history} \n",
    "    <s>[INST] {user_input} [/INST]\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def chat_with_agent(user_input: str, memory: List[Dict[str, str]]):\n",
    "    response = llm.invoke(\n",
    "        format_conversation(user_input, memory),\n",
    "        temperature=0.0,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    display(Markdown(response))\n",
    "    memory.append({\"role\": \"customer\", \"content\": user_input})\n",
    "    memory.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4531a38-ddce-4621-9f73-55b22567c785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Hello! Welcome to AWS customer service. I'm here to help. What can I assist you with today regarding AWS? Please keep your questions concise for a prompt response."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memory = []\n",
    "chat_with_agent(\"Hi there\", memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529f8b1c-f04c-440f-95e0-a683bd470241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " To select an EC2 instance type, consider:\n",
       "\n",
       "1. Workload: Compute-intensive, memory-intensive, or storage-intensive.\n",
       "2. Instance family: General purpose, compute optimized, memory optimized, or storage optimized.\n",
       "3. Size: Number of vCPUs and memory.\n",
       "4. Pricing: On-demand, reserved, or spot instances.\n",
       "5. Additional features: GPU, FPGA, or bare metal.\n",
       "\n",
       "For more details, visit: [EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_agent(\"How to select an EC2 instance type?\", memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e40edc2-9321-4278-a7f1-66f3bb4e2214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    Absolutely! The EC2 instance selection guidance I provided is generally applicable to Linux workloads. AWS offers a wide variety of instances optimized for different Linux distributions. You can choose an instance type based on your specific requirements for compute, memory, and storage, as well as your budget.\n",
       "\n",
       "For more information on Linux instances, visit: [Linux on AWS](https://aws.amazon.com/linux/)\n",
       "\n",
       "Please let me know if you have any other questions or concerns. I'm here to help!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_agent(\"Cool. Will that work for my Linux workload?\", memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e76346a1-1719-4c6b-a6e6-f8961d56533f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    You're welcome! I'm glad I could help. If you have any more questions or concerns in the future, don't hesitate to reach out. Have a great day! ðŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_agent(\"That's all. Thank you.\", memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad8b4e-d5dd-4f3f-a857-105b80606e0a",
   "metadata": {},
   "source": [
    "**Let's ask a question that is not specialty of this persona.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab153c9c-4b2b-4726-b64a-21a4d36da3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " I'm sorry for any confusion, but I'm an AWS customer service assistant and I don't have the ability to diagnose or fix car problems. I can certainly help you with any questions or issues you have related to Amazon Web Services. Is there something specific you'd like assistance with in regards to AWS?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_agent(\"How to fix my car?\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0ae66-40ed-4c5b-8037-98eda7e1feba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Summarization and Classification\n",
    "Text summarization is a crucial task that extracts the most important information from text based documents while retaining its core meaning. With a context window up to 32k tokens, organizations can use Mistral models to streamline their various document summarization needs from summarizing news articles and research papers to distilling critical information from long documents that enable downstream solutions such as document classification, question and answering, and decision support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db391903-6ed0-4b69-b989-1e41f74ac843",
   "metadata": {},
   "source": [
    "### Example 1: Summarization\n",
    "\n",
    "This example provides a summary of the blog post content [Mistral AI models now available on Amazon Bedrock](https://aws.amazon.com/blogs/aws/mistral-ai-models-now-available-on-amazon-bedrock/) and proposes relevant questions and answers in a markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd365075-bb1e-4223-a3fb-a34dde0eeb97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://aws.amazon.com/blogs/aws/mistral-ai-models-now-available-on-amazon-bedrock/')\n",
    "blog = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9250057-28bf-47b0-8107-90f42db48837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "<s>[INST] Your task is to write a summary about a blog post. \n",
    "When presented with the blog post, come up with quiz/answers to ask the viewers. \n",
    "Write the summary in the markdown format. \n",
    "\n",
    "# Blog post: \n",
    "{blog}\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes presented in the blog post.\n",
    "\n",
    "## Quiz: \n",
    "Generate three distinct questions that can be asked about the blog post. For each question:\n",
    "- After \"Question: \", describe the problem \n",
    "- After \"Choices: \", possible answers (single choice)\n",
    "    - A \n",
    "    - B\n",
    "    - C\n",
    "- After \"Answer: \", show the correct choice\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d879b7-d5e7-4ef3-8407-bc8f7e6293f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Summary:\n",
      "In this blog post, AWS announced the availability of Mistral AI models on Amazon Bedrock. Mistral AI offers a balance of cost and performance, fast inference speed, transparency and trust, and is accessible to a wide range of users. The two high-performing Mistral AI models, Mistral 7B and Mixtral 8x7B, are now available on Amazon Bedrock, joining other leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon. This integration provides users the flexibility to choose optimal high-performing foundation models in Amazon Bedrock.\n",
      "\n",
      "# Quiz:\n",
      "\n",
      "Question: What are the two high-performing Mistral AI models available on Amazon Bedrock?\n",
      "\n",
      "Choices:\n",
      "- A. Mistral 7B and Mixtral 8x7B\n",
      "- B. Mistral 6B and Mixtral 7x7B\n",
      "- C. Mistral 8B and Mixtral 9x7B\n",
      "\n",
      "Answer: A\n",
      "\n",
      "Question: Which AI companies are currently offering foundation models on Amazon Bedrock?\n",
      "\n",
      "Choices:\n",
      "- A. AI21 Labs, Anthropic, Cohere, Meta, Stability AI, Amazon, and Mistral AI\n",
      "- B. AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon\n",
      "- C. AI21 Labs, Anthropic, Cohere, Meta, Stability AI, Amazon, Mistral AI, and Google\n",
      "\n",
      "Answer: A\n",
      "\n",
      "Question: What benefits does Mistral AI offer to users on Amazon Bedrock?\n",
      "\n",
      "Choices:\n",
      "- A. Balance of cost and performance, fast inference speed, transparency and trust, and accessibility\n",
      "- B. High cost and low performance, slow inference speed, lack of transparency and trust, and limited accessibility\n",
      "- C. Low cost and high performance, slow inference speed, transparency and trust, and wide accessibility\n",
      "\n",
      "Answer: A\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "        message,\n",
    "        temperature=0.0,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc7a3d-d603-4939-bebe-e0d18ea311d5",
   "metadata": {},
   "source": [
    "### Example 2: Classification\n",
    "\n",
    "Data classification is crucial for ensuring the proper protection, management, and controlled access of an organization's information assets based on their sensitivity levels. In the following example, we provided the LLM with different data sensitivity levels and their definitions, allowing it to classify user inquiries accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53a0c7fc-d805-4497-ad65-d7e3dcfc0f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_classification_template(user_inquery):\n",
    "    query = f\"\"\"<s>[INST]\n",
    "You are an AI assistant tasked with classifying data based on its sensitivity level. The sensitivity levels and their definitions are:\n",
    "\n",
    "Sensitive: Data that is to have the most limited access and requires a high degree of integrity. This is typically data that will do the most damage to the organization should it be disclosed.\n",
    "Confidential: Data that might be less restrictive within the company but might cause damage if disclosed.\n",
    "Private: Private data is usually compartmental data that might not do the company damage but must be kept private for other reasons. Human resources data is one example of data that can be classified as private.\n",
    "Proprietary: Proprietary data is data that is disclosed outside the company on a limited basis or contains information that could reduce the company's competitive advantage, such as the technical specifications of a new product.\n",
    "Public: Public data is the least sensitive data used by the company and would cause the least harm if disclosed. This could be anything from data used for marketing to the number of employees in the company.\n",
    "\n",
    "For each user inquery provided, classify it into one of the above sensitivity levels. Do not include the word \"Category\". Do not provide explanations or notes.\n",
    "\n",
    "<<<\n",
    "Inquiry: {user_inquery}\n",
    ">>>\n",
    "\n",
    "[/INST]\"\"\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140df20c-b312-46c5-8772-401407749072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User inquery: I'm an HR recruiter. What data classifiction category are resumes gathered based on referral by employees?\n",
      "\n",
      "Category:  Private\n",
      "\n",
      "User inquery: I require the financial statements for the past three fiscal years.\n",
      "\n",
      "Category:  Confidential\n",
      "\n",
      "User inquery: I need access to the personnel files containing employee social security numbers and financial information.\n",
      "\n",
      "Category:  Sensitive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_inqueries = [\n",
    "    \"I'm an HR recruiter. What data classifiction category are resumes gathered based on referral by employees?\",\n",
    "    \"I require the financial statements for the past three fiscal years.\",\n",
    "    \"I need access to the personnel files containing employee social security numbers and financial information.\"\n",
    "]\n",
    "\n",
    "for user_inquery in user_inqueries:\n",
    "    response = llm.invoke(\n",
    "        fill_classification_template(user_inquery),\n",
    "        temperature=0.0,\n",
    "        max_tokens=128,\n",
    "    )\n",
    "    print(f\"User inquery: {user_inquery}\\n\")\n",
    "    print(f\"Category: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79623d60-0bdd-4110-9e69-240dcdb08faf",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Code Generation\n",
    "\n",
    "Mistral models can help developers write code faster and more efficiently, and enhance code quality by automating some of the repetitive coding tasks, such as generating boilerplate code, inserting comments, suggesting code snippets, or translating code from one programming language to another. By fine-tuning Mistral models on legacy languages, Mistral models can empower organizations to complete their language upgrade and other legacy infrastructure transformation initiatives quickly and cost effectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9cc49-77fb-4071-9480-a9cac86a2be8",
   "metadata": {},
   "source": [
    "### Example 1: Python Code Generation\n",
    "\n",
    "In the earlier classification example, we demonstrated the model's capability to classify data sensitivity. For the first code generation example, we can instruct the Mistral model to write a rule-based data classification Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41605edc-f0f9-489c-841f-847aa9ebade5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = \"\"\"\n",
    "<s>[INST]\n",
    "You are an experienced Python developer tasked with creating data classification function.\n",
    "The function should analyze the input text and determine the appropriate classification category based on predefined rules or patterns.\n",
    "\n",
    "1. Prompt the user to enter text for data classification. \n",
    "2. Implement the following classification rules. \n",
    "    - Sensitive: if the input text contains \"password\", \"social security number\", \"credit card number\"\n",
    "    - Confidential: if the input text contains \"confidential\", \"internal use only\"\n",
    "    - Private: if the input text contains \"private\", \"personal\"\n",
    "    - Proprietary: if the input text contains \"proprietary\", \"trade secret\"\n",
    "    - Public: Does not contain any sensitive, confidential, private, or proprietary information.\n",
    "2. The function should return the classified category as a string (e.g., \"Sensitive\", \"Confidential\", \"Private\", \"Proprietary\", or \"Public\").\n",
    "3. User Interface:\n",
    "    - Create a simple command-line interface (CLI) that prompts the user to enter a string of text.\n",
    "    - Provide clear instructions on how to use the program.\n",
    "4. Error Handling: Implement error handling mechanisms to gracefully handle invalid inputs or exceptions.\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fec48db-0f74-462e-a6f2-5bd9c58e3c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a Python function that meets the requirements:\n",
       "\n",
       "```python\n",
       "import re\n",
       "\n",
       "def classify_data(input_text):\n",
       "    classifications = {\n",
       "        \"Sensitive\": re.search(r\"password|social security number|credit card number\", input_text),\n",
       "        \"Confidential\": re.search(r\"confidential|internal use only\", input_text),\n",
       "        \"Private\": re.search(r\"private|personal\", input_text),\n",
       "        \"Proprietary\": re.search(r\"proprietary|trade secret\", input_text),\n",
       "        \"Public\": not any(classification for classification in (\n",
       "            classifications.values()\n",
       "            if classification is not None\n",
       "        ))\n",
       "    }\n",
       "\n",
       "    for category, search_result in classifications.items():\n",
       "        if search_result is not None:\n",
       "            return category\n",
       "\n",
       "def main():\n",
       "    print(\"Data Classification Program\")\n",
       "    print(\"--------------------------\")\n",
       "    input_text = input(\"Enter the text to classify: \")\n",
       "    classification = classify_data(input_text)\n",
       "    print(f\"\\nClassification: {classification.capitalize() if classification else 'Public'}\")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "This script defines a `classify_data` function that takes a string as input and returns a classification category based on the predefined rules. The `main` function handles user input and displays the classification result.\n",
       "\n",
       "To use this script, simply run it and enter the text you want to classify when prompted. The script will then output the classification category. If the input text does not contain any sensitive, confidential, private, or proprietary information, the script will classify it as \"Public\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "        message,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d8bdd7-e61f-467e-88aa-5a1ae455f76d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the code from the response and run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683cc4c-23de-4a5a-8132-b2310aeb45ab",
   "metadata": {},
   "source": [
    "### Example 2: Data science\n",
    "\n",
    "A real world scenario is given a csv file and let the LLM to generate code to analyze the data, generate some visualizations and clean up the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee4cf6e-1e0a-4584-aa90-416cfca9b441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books.csv has been created!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = [\n",
    "    [\"book_id\", \"title\", \"author\", \"genre\", \"publish_date\", \"publisher\", \"pages\", \"rating\"],\n",
    "    [1, 'The Great Gatsby', 'F. Scott Fitzgerald', 'Fiction', '1925-04-10', \"Charles Scribner's Sons\", 180, 4.3],\n",
    "    [2, 'To Kill a Mockingbird', 'Harper Lee', 'Fiction', '1960-07-11', 'J. B. Lippincott & Co.', 281, 4.2],\n",
    "    [3, '1984', 'George Orwell', 'Fiction', '1949-06-08', 'Secker & Warburg', 328, 4.1],\n",
    "    [4, 'Pride and Prejudice', 'Jane Austen', 'Fiction', '1813-01-28', 'Thomas Egerton', 279, 4.4],\n",
    "    [5, 'The Catcher in the Rye', 'J. D. Salinger', 'Fiction', '1951-07-16', 'Little Brown and Company', 214, 3.8],\n",
    "    [6, 'The Hobbit', 'J.R.R. Tolkien', 'Fantasy', '1937-09-21', 'George Allen & Unwin', 310, 4.5],\n",
    "    [7, 'The Lord of the Rings', 'J.R.R. Tolkien', 'Fantasy', '1954-07-29', 'George Allen & Unwin', 1178, 4.7],\n",
    "    [8, 'Harry Potter and the Sorcerer\\'s Stone', 'J.K. Rowling', 'Fantasy', '1997-06-26', 'Bloomsbury', 309, 4.5],\n",
    "    [9, 'The Da Vinci Code', 'Dan Brown', 'Mystery', '2003-03-18', 'Doubleday', 454, 3.6],\n",
    "    [10, 'Angels & Demons', 'Dan Brown', 'Mystery', '2000-05-01', 'Atria Books', 620, 3.9],\n",
    "    [11, 'The Girl on the Train', 'Paula Hawkins', 'Mystery', '2015-01-13', 'Riverhead Books', 323, 3.8],\n",
    "    [12, 'Gone Girl', 'Gillian Flynn', 'Mystery', '2012-06-05', 'Crown Publishing Group', 422, 4.1],\n",
    "    [13, 'The Notebook', 'Nicholas Sparks', 'Romance', '1996-10-01', 'Warner Books', 224, 4.0],\n",
    "    [14, 'Outlander', 'Diana Gabaldon', 'Romance', '1991-06-01', 'Delacorte Press', 850, 4.2],\n",
    "    [15, 'The Fault in Our Stars', 'John Green', 'Romance', '2012-01-10', 'Dutton Books', 313, 4.4]\n",
    "]\n",
    "\n",
    "# Write data to books.csv\n",
    "with open('books.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"books.csv has been created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c83f313-d69c-4236-8f42-6d0069913515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = \"\"\"<s>[INST]You are a data analyst working with a dataset containing information about various books. The dataset is in the form of a CSV file named 'books.csv' and includes the following columns:\n",
    "\n",
    "- 'book_id': Unique identifier for each book\n",
    "- 'title': Title of the book\n",
    "- 'author': Author of the book\n",
    "- 'genre': Genre of the book (e.g., Fiction, Non-Fiction, Mystery, Romance)\n",
    "- 'publish_date': Publication date of the book\n",
    "- 'publisher': Publisher of the book\n",
    "- 'pages': Number of pages in the book\n",
    "- 'rating': Average rating of the book on a scale of 1 to 5\n",
    "\n",
    "Your task is to write Python code using pandas library to perform the following data analysis and exploration:\n",
    "\n",
    "1. Load the 'books.csv' file into a pandas DataFrame.\n",
    "2. Display the first few rows of the DataFrame to get an overview of the data.\n",
    "3. Check for any missing values in the DataFrame and handle them appropriately (e.g., drop rows or fill with a suitable value).\n",
    "4. Convert the 'publish_date' column to a datetime format.\n",
    "5. Create a new column 'age' that calculates the number of years since the book was published.\n",
    "6. Group the books by genre and calculate the mean rating for each genre.\n",
    "7. Identify the top 5 books with the highest ratings.\n",
    "8. Create a scatter plot showing the relationship between the number of pages and the book rating.\n",
    "9. Generate a bar chart displaying the count of books published by each publisher.\n",
    "10. Export the processed and cleaned DataFrame to a new CSV file named 'books_cleaned.csv'.\n",
    "\n",
    "Your code should be well-commented, easy to read, and follow best practices for data analysis and visualization using pandas and matplotlib (or any other suitable library). Feel free to add any additional analysis or visualizations that you think would be useful for exploring and understanding the book dataset.\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3fa3d22-5790-4f85-8f54-259c78ff8282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from datetime import datetime\n",
      "\n",
      "# 1. Load the 'books.csv' file into a pandas DataFrame.\n",
      "df = pd.read_csv('books.csv')\n",
      "\n",
      "# 2. Display the first few rows of the DataFrame to get an overview of the data.\n",
      "print(df.head())\n",
      "\n",
      "# 3. Check for any missing values in the DataFrame and handle them appropriately (e.g., drop rows or fill with a suitable value).\n",
      "missing_values = df.isnull().sum()\n",
      "print(\"\\nMissing Values:\")\n",
      "print(missing_values)\n",
      "\n",
      "# Since there are no missing values, no need to drop or fill any values.\n",
      "\n",
      "# 4. Convert the 'publish_date' column to a datetime format.\n",
      "df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
      "\n",
      "# 5. Create a new column 'age' that calculates the number of years since the book was published.\n",
      "today = datetime.today()\n",
      "df['age'] = (today.year - df['publish_date'].dt.year) - \\\n",
      "            ((today.month, today.day) < (df['publish_date'].dt.month, df['publish_date'].dt.day))\n",
      "\n",
      "# 6. Group the books by genre and calculate the mean rating for each genre.\n",
      "grouped_by_genre = df.groupby('genre')['rating'].mean()\n",
      "print(\"\\nMean Ratings by Genre:\")\n",
      "print(grouped_by_genre)\n",
      "\n",
      "# 7. Identify the top 5 books with the highest ratings.\n",
      "top_5_books = df.nlargest(5, 'rating')\n",
      "print(\"\\nTop 5 Books with Highest Ratings:\")\n",
      "print(top_5_books)\n",
      "\n",
      "# 8. Create a scatter plot showing the relationship between the number of pages and the book rating.\n",
      "plt.scatter(df['pages'], df['rating'])\n",
      "plt.xlabel('Number of Pages')\n",
      "plt.ylabel('Rating')\n",
      "plt.title('Relationship between Number of Pages and Book Rating')\n",
      "plt.show()\n",
      "\n",
      "# 9. Generate a bar chart displaying the count of books published by each publisher.\n",
      "grouped_by_publisher = df.groupby('publisher').size()\n",
      "plt.bar(grouped_by_publisher.index, grouped_by_publisher.values)\n",
      "plt.xlabel('Publisher')\n",
      "plt.ylabel('Number of Books')\n",
      "plt.title('Number of Books Published by Each Publisher')\n",
      "plt.show()\n",
      "\n",
      "# 10. Export the processed and cleaned DataFrame to a new CSV file named 'books_cleaned.csv'.\n",
      "df.to_csv('books_cleaned.csv', index=False)\n",
      "\n",
      "# Additional Analysis:\n",
      "# Calculate the correlation between the number of pages and the book rating.\n",
      "print(\"\\nCorrelation between Number of Pages and Rating:\")\n",
      "print(df['pages'].corr(df['rating']))\n",
      "\n",
      "# Find the most common genre in the dataset.\n",
      "most_common_genre = df['genre'].value_counts().idxmax()\n",
      "print(\"\\nMost Common Genre:\")\n",
      "print(most_common_genre)\n",
      "\n",
      "# Find the oldest book in the dataset.\n",
      "oldest_book = df.loc[df['age'].idxmin()]\n",
      "print(\"\\nOldest Book:\")\n",
      "print(oldest_book)\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "        message,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cc2bfe4-a1f4-4aed-8f9d-2d5e4ee49d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the code from the response and run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a594b40-6a7b-443c-8db2-6c12e77633ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Function Calling\n",
    "Function calling is the ability to reliably connect a large language model (LLM) to external tools and enable effective tool usage and interaction with external APIs. Mistral models provide the ability for building LLM powered chatbots or agents that need to retrieve context for the model or interact with external tools by converting natural language into API calls to retrieve specific domain knowledge. From conversational agents and math problem solving to API integration and information extraction, multiple use cases can benefit from this capability provided by Mistral models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0dfe8-5053-4fa6-8952-143fef77c1fb",
   "metadata": {},
   "source": [
    "### Example: Custom tool\n",
    "\n",
    "Let's start by building a custom tool to give models access to the current date, a piece of information that is very relevant to answer queries about the real world, and that LLMs do not have access to, as they lack awareness of present vs. past time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d954fbb8-f9a3-4f8e-a705-f60cf9610a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438af4a-70db-4b31-8670-bd8df177bcfb",
   "metadata": {},
   "source": [
    "Two functions have been defined:\n",
    "\n",
    "- `get_age_from_birthdate` calculates the age based on the provided birthdate.\n",
    "\n",
    "- `get_ticket_price_by_age` takes a person's age as input and calculates the appropriate ticket price based on that age. It then returns the calculated ticket price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b58fab0-79ca-49a7-a2a8-3193934b9259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_age_from_birthdate(birthdate: str) -> int:\n",
    "    birthdate = datetime.strptime(birthdate, \"%Y-%m-%d\").date()\n",
    "    today = date.today()\n",
    "    age = today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))\n",
    "    return age\n",
    "\n",
    "def get_ticket_price_by_age(age: int) -> int:\n",
    "    if age <= 3:\n",
    "        return 0\n",
    "    elif age >= 60:\n",
    "        return 30\n",
    "    return 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19706e9b-5784-4ec2-83ca-4a16b420b5c2",
   "metadata": {},
   "source": [
    "We can write the Open API schema to describe the two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1254cc97-1448-4431-9568-ceb938a13b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"toolChoice\": {\"auto\": {}},\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_age_from_birthdate\",\n",
    "                \"description\": \"Calculates the age based on the provided birthdate.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"birthdate\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The birthdate in the format YYYY-MM-DD.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"birthdate\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_ticket_price_by_age\",\n",
    "                \"description\": \"Returns the ticket price based on the provided age.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"The age of the person.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"age\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1041fea6-582b-4789-8440-cfe8a932078f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def use_tool(messages):\n",
    "    tool_use = messages[-1][\"content\"][-1].get(\"toolUse\")\n",
    "    if tool_use:\n",
    "        tool_name = tool_use[\"name\"]\n",
    "        tool_input = tool_use[\"input\"]\n",
    "        print(f\"Tool Name: {tool_name}\")\n",
    "        print(f\"Tool Input: {json.dumps(tool_input, indent=2)}\")\n",
    "\n",
    "        # Process the tool call\n",
    "        tool_result = \"\"\n",
    "        if tool_name == \"get_age_from_birthdate\":\n",
    "             tool_result = get_age_from_birthdate(tool_input[\"birthdate\"])\n",
    "        elif tool_name == \"get_ticket_price_by_age\":\n",
    "             tool_result = get_ticket_price_by_age(tool_input[\"age\"])\n",
    "        print(f\"Tool Result: {tool_result} \\n\")\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"toolResult\": {\n",
    "                        \"toolUseId\": tool_use[\"toolUseId\"],\n",
    "                        \"content\": [\n",
    "                            {\"text\": json.dumps(tool_result)}\n",
    "                        ],\n",
    "                        \"status\": \"success\",\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877aee8-c7c9-4726-b1b5-26bdc0cde22b",
   "metadata": {},
   "source": [
    "Below is the prompt template implementing the function calling, where we pass the tools available to the LLM, instruct the LLM to generate both reasoning traces and task-specific actions in an interleaved manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e20dab0-3c0b-480c-86bd-b3435b060d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are a ticket price agent for an event venue. Your job is to provide the ticket price based on the customer's age.\n",
    "You have access to a set of tools, but only use them when needed.  \n",
    "If you do not have enough information to use a tool correctly, ask a user follow up questions to get the required inputs.\n",
    "Do not call any of the tools unless you have the required data from a user.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29dac1-4c4c-46a5-9536-74d3d9a5308d",
   "metadata": {},
   "source": [
    "Below is the step-by-step execution of the function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11f9552e-9543-4276-ae74-c8245e3825e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "def process_message(messages):\n",
    "    \"\"\"Process the given messages and generate a response using the Bedrock model.\"\"\"\n",
    "    system_prompts = [{\"text\": SYSTEM_PROMPT}]\n",
    "\n",
    "    inference_config = {\"temperature\": 0.0, \"maxTokens\": 400}\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=mistral_large_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        toolConfig=tool_config,\n",
    "        inferenceConfig=inference_config,\n",
    "    )\n",
    "    message = response[\"output\"][\"message\"]\n",
    "    stop_reason = response.get(\"stopReason\")\n",
    "    return message, stop_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f225428-d300-4bf0-833c-43e59f778c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_bot(user_input):\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": [{\"text\": user_input}]\n",
    "        }\n",
    "    )\n",
    "    bot_response, stop_reason = process_message(messages)\n",
    "    \n",
    "    while stop_reason == \"tool_use\":\n",
    "        messages.append(bot_response)\n",
    "        tool_response = use_tool(messages)\n",
    "        messages.append(tool_response)\n",
    "        bot_response, stop_reason = process_message(messages)\n",
    "    return bot_response[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca5c5e5c-843d-4db0-9193-45204ddc4a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Function calling step-by-step:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: get_age_from_birthdate\n",
      "Tool Input: {\n",
      "  \"birthdate\": \"1999-01-02\"\n",
      "}\n",
      "Tool Result: 25 \n",
      "\n",
      "Tool Name: get_ticket_price_by_age\n",
      "Tool Input: {\n",
      "  \"age\": 25\n",
      "}\n",
      "Tool Result: 60 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Final anawer:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your age, the ticket price for the event is $60.\n"
     ]
    }
   ],
   "source": [
    "question = \"I was born on Jan 02, 1999. How much do I need to pay for the ticket?\"\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown('**Function calling step-by-step:**'))\n",
    "response = ask_bot(question)\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"**Final anawer:**\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a9f85-bc28-4e39-95e6-631ec85f9da9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Distributors\n",
    "\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

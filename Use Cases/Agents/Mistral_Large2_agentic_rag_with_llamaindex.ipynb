{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c020350b-d116-4a18-b38e-cd539f4180ed",
   "metadata": {},
   "source": [
    "# Agentic RAG application using the Mistral Large 2 Model and LlamaIndex\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Agentic RAG (Retrieval-Augmented Generation) applications represent an advanced approach in AI that integrates large language models (LLMs) with external knowledge retrieval and autonomous agent capabilities. These systems dynamically access and process information, break down complex tasks, utilise external tools, apply reasoning, and adapt to various contexts. They go beyond simple question-answering by performing multi-step processes, making decisions, and generating complex outputs.\n",
    "\n",
    "In this notebook, we demonstrate an example of building an agentic RAG application using the LlamaIndex framework. This application serves as a technology discovery and research tool, using the Mistral Large 2 model via Bedrock Converse as the LLM to orchestrate agent flow and generate responses. It interacts with well-known websites, such as Arxiv, GitHub, and TechCrunch, and can access knowledge bases containing documentation and internal knowledge.\n",
    "\n",
    "This application can be further expanded to accommodate broader use cases requiring dynamic interaction with internal and external APIs, as well as the integration of internal knowledge bases to provide more context-aware responses to user queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- At the time of writing this notebook, the Mistral Large 2 model is only available in the `us-west-2` region.\n",
    "- Create a SageMaker notebook instance and select `ml.t3.medium` as the instance type.\n",
    "- Create a new SageMaker execution role and grant it Bedrock full access.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n",
    "\n",
    "This solution uses the LlamaIndex framework to build an agent flow with two main components: [AgentRunner and AgentWorker](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner/). The AgentRunner serves as an orchestrator that manages conversation history, creates and maintains tasks, executes task steps, and provides a user-friendly interface for interactions. The AgentWorker handles the step-by-step reasoning and task execution.\n",
    "\n",
    "For reasoning and task planning, we use Mistral Large 2 model from Amazon Bedrock. The agent integrates with GitHub, arXiv, and TechCrunch APIs, while also accessing internal knowledge through Bedrock Knowledge Bases and Amazon OpenSearch Serverless to provide context-aware answers.\n",
    "\n",
    "\n",
    "<img src=\"imgs/llamaindex-agentic-rag-mistral-large2-arch.png\" width=\"600\" alt=\"architecture\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12801837-d6cd-44ac-ab1b-17f0b4688874",
   "metadata": {},
   "source": [
    "## Install Packages \n",
    "\n",
    "Install below ptyhon packages: \n",
    "- **llama-index**: an open-source framework that helps build applications using LLMs. \n",
    "- **llama-index-llms-bedrock-converse**: Bedrock Converse integration with LlamaIndex.  \n",
    "- **llama-index-retrievers-bedrock**: Bedrock Knowledge Bases integration with LlamaIndex. \n",
    "- **llama-index-tools-arxiv**: A prebuilt tool to query arxiv.org\n",
    "- **feedparser**: A Python library for parsing for downloading and parsing syndicated feeds including RSS, Atom & RDF Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a21c87-2c96-4658-b026-f39f5697335e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install llama-index -q\n",
    "%pip install llama-index-llms-bedrock-converse -q\n",
    "%pip install llama-index-retrievers-bedrock -q\n",
    "%pip install llama-index-tools-arxiv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb91dd7-924d-4379-a06f-4039862663fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda install feedparser -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b3949-6348-4d72-a1ef-9766109c65cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2f50c-99d7-47a1-8c5e-ae12a8392581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize and configure the BedrockConverse LLM with the Mistral Large 2 model and set it as the default in Settings\n",
    "\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = BedrockConverse(model=\"mistral.mistral-large-2407-v1:0\", max_tokens = 2048)\n",
    "Settings.llm = BedrockConverse(model=\"mistral.mistral-large-2407-v1:0\", max_tokens = 2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d6b40-83e6-432d-90ef-8d31b1c8da26",
   "metadata": {},
   "source": [
    "## API tools integration \n",
    "\n",
    "We implement two functions to interact with GitHub and TechCrunch APIs. To ensure clear communication between the agent and the LLM model, we follow Python function best practices including:\n",
    "- Type hints for parameter and return value validation\n",
    "- Detailed docstrings explaining function purpose, parameters, and expected returns\n",
    "- Clear function descriptions\n",
    "\n",
    "For arXiv integration, we leverage LlamaIndex's pre-built tool instead of creating a custom function. You can explore other available pre-built tools in the [LlamaIndex documentation](https://docs.llamaindex.ai/en/stable/api_reference/tools/) to avoid duplicating existing solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694f1aa-fcac-4667-8d21-fcbf1a3ff926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to search GitHub repositories by topic, sorting by stars or update date, and return top results\n",
    "\n",
    "import requests\n",
    "\n",
    "def github_search(topic: str, num_results: int = 3, sort_by: str = \"stars\") -> list:\n",
    "    \"\"\"\n",
    "    Retrieve a specified number of GitHub repositories based on a given topic, \n",
    "    ranked by the specified criteria.\n",
    "\n",
    "    This function uses the GitHub API to search for repositories related to a \n",
    "    specific topic or keyword. The results can be sorted by the number of stars \n",
    "    (popularity) or the most recent update, with the most relevant repositories \n",
    "    appearing first according to the chosen sorting method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic : str\n",
    "        The topic or keyword to search for in GitHub repositories.\n",
    "        The topic cannot contain blank spaces.\n",
    "    num_results : int, optional\n",
    "        The number of repository results to retrieve. Defaults to 3.\n",
    "    sort_by : str, optional\n",
    "        The criterion for sorting the results. Options include:\n",
    "        - 'stars': Sort by the number of stars (popularity).\n",
    "        - 'updated': Sort by the date of the last update (most recent first).\n",
    "        Defaults to 'stars'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A list of dictionaries, where each dictionary contains information \n",
    "        about a repository. Each dictionary includes:\n",
    "        - 'html_url': The URL of the repository.\n",
    "        - 'description': A brief description of the repository.\n",
    "        - 'stargazers_count': The number of stars (popularity) the repository has.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    url = f\"https://api.github.com/search/repositories?q=topic:{topic}&sort={sort_by}&order=desc\"\n",
    "\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    code_repos = [\n",
    "        {\n",
    "            'html_url': item['html_url'],\n",
    "            'description': item['description'],\n",
    "            'stargazers_count': item['stargazers_count'],\n",
    "            # 'topics': item['topics']\n",
    "        }\n",
    "        for item in response['items'][:num_results]\n",
    "    ]\n",
    "    \n",
    "    return code_repos\n",
    "\n",
    "github_tool = FunctionTool.from_defaults(fn=github_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883388e-7eff-46b3-b537-12fd4b24eae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to search for TechCrunch news articles by topic and return details for a specified number of results\n",
    "\n",
    "import feedparser\n",
    "    \n",
    "def news_search(topic: str, num_results: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve a specified number of news articles from TechCrunch based on a given topic.\n",
    "\n",
    "    This function queries the TechCrunch RSS feed to search for news articles related to the \n",
    "    provided topic and returns a list of the most relevant articles. Each article includes \n",
    "    details such as the title, link, publication date, and a summary or description.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic : str\n",
    "        The keyword or subject to search for in the TechCrunch news feed.\n",
    "        The topic cannot contain blank spaces.\n",
    "        If multiple words are needed, connect them with \"+\" (e.g., artificial+intelligence).\n",
    "    num_results : int, optional\n",
    "        The number of articles to retrieve from the search results. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A list of dictionaries, where each dictionary contains information about a retrieved \n",
    "        news article. Each dictionary includes:\n",
    "        - 'title': The title of the article.\n",
    "        - 'link': The URL to the article.\n",
    "        - 'published': The publication date of the article.\n",
    "        - 'summary': A brief summary or description of the article, if available.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    url = f\"https://techcrunch.com/tag/{topic}/feed/\"\n",
    "    feed = feedparser.parse(url)\n",
    "    \n",
    "    news = []\n",
    "    \n",
    "    # Loop through the top num_results articles\n",
    "    for entry in feed.entries[:num_results]:\n",
    "        # Create a dictionary for each article\n",
    "        article = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "            'published': entry.published,\n",
    "            'summary': entry.summary if hasattr(entry, 'summary') else entry.description if hasattr(entry, 'description') else None\n",
    "        }\n",
    "\n",
    "        # Add the article dictionary to the list\n",
    "        news.append(article)\n",
    "    \n",
    "    return news\n",
    "\n",
    "news_tool = FunctionTool.from_defaults(fn=news_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f9a6c-2d98-4587-a502-ad66eb52614a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import and configure the ArxivToolSpec from LlamaIndex prebuilt tools\n",
    "\n",
    "from llama_index.tools.arxiv import ArxivToolSpec\n",
    "\n",
    "arxiv_tool = ArxivToolSpec()\n",
    "api_tools = arxiv_tool.to_tool_list()\n",
    "\n",
    "# Consolidate all tools into one list. \n",
    "api_tools.extend([news_tool, github_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f6fb6-1257-42b4-9b9f-b1bf463f10d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up an agent with access to GitHub, arXiv, and TechCrunch APIs, using a system prompt to guide interactions.\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "import time\n",
    "current_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a technology expert with access to the GitHub API, arXiv API, and TechCrunch API. \n",
    "You can search for the latest code repositories, papers, and news related to technology.\n",
    "Always try to use the tools available to you. \n",
    "If you don’t know the answer, do not make up any information; simply say: Sorry, I don’t know.\n",
    "\n",
    "Current time is: {current_time}\n",
    "\"\"\"\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    api_tools, \n",
    "    llm=llm, \n",
    "    verbose=False, # Set verbose=True to display the full trace of steps. \n",
    "    system_prompt = system_prompt,\n",
    "    # allow_parallel_tool_calls = True # Uncomment this line to allow multiple tool invocations\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af59152-876a-4051-93ce-794f8d799f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Can you give me top 2 papers about GenAI, and recent news about bedrock\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4afe6b-fe5e-4265-b36b-9d6c564e699c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple chatbot UI. Enter \"exit\" to quit. \n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"-\" * 120)\n",
    "    print(\" New question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d7192-7c86-4d4d-a672-cb32564a035b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agent.memory.get() # retrieve conversation history\n",
    "# agent.memory.reset() # clear the chat memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056a046-2b98-44f9-9711-7fa057e4eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test questions: \n",
    "# 1. any news about GenAI\n",
    "# 2. can you give me top5 github code repo related to bedrock\n",
    "# 3. can you show me the top 3 paper that releted to GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d09000-c72b-477a-aaa4-453eb1746cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "427f3df8-00aa-4c3f-a50b-fb0c3faf3e9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Documents RAG Integration - with Bedrock Knowledge Bases Service\n",
    "\n",
    "Below, we download two PDF files of decision guide documents from the AWS website, which provide recommendations for selecting GenAI and ML services in different scenarios, and outline what should be evaluated and considered in the decision-making process. You can provide and replace these with your internal business documents in this step.\n",
    "\n",
    "We use Amazon Bedrock Knowledge Bases to build the RAG framework. You can create a Bedrock Knowledge Base from the [AWS console](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html) or follow this [notebook example](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/02_KnowledgeBases_and_RAG/0_create_ingest_documents_test_kb.ipynb) to create it programmatically. \n",
    "\n",
    "Download files using the commands below, then upload them to the S3 bucket you created for the Knowledge Base. You can select different embedding models and chunking strategies that work better for your data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a2d37-2189-4eea-bb08-2963707ec966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download test documents from below links\n",
    "\n",
    "!wget -O genai_on_aws.pdf \"https://docs.aws.amazon.com/pdfs/decision-guides/latest/generative-ai-on-aws-how-to-choose/generative-ai-on-aws-how-to-choose.pdf?did=wp_card&trk=wp_card#guide\"\n",
    "!wget -O ml_on_aws.pdf \"https://docs.aws.amazon.com/pdfs/decision-guides/latest/machine-learning-on-aws-how-to-choose/machine-learning-on-aws-how-to-choose.pdf?did=wp_card&trk=wp_card#guide\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd541fe-7e75-469a-884f-b8f028f70e4d",
   "metadata": {},
   "source": [
    "- Upload the test documents to the S3 bucket that was added as a data source to the Knowledge Base you created. Then sync the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaaa3e-4a9e-42b1-9111-82350dfaf1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After you create the knowledge base, provide Bedrock Knowledge Base ID \n",
    "knowledge_base_id = \"[KNOWLEDGE_BASE_ID]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80cd10-23da-4d60-9e0e-83bbd5e30bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure a knowledge base retriever using AmazonKnowledgeBasesRetriever\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "# maximum number of relevant text chunks that will be retrieved\n",
    "# If you need quick, focused answers: lower numbers (1-3)\n",
    "# If you need detailed, comprehensive answers: higher numbers (5-10)\n",
    "top_k = 3\n",
    "\n",
    "# search mode options: HYBRID, SEMANTIC\n",
    "# HYBRID search combines the strengths of semantic search and keyword search \n",
    "# Balances semantic understanding with exact matching\n",
    "search_mode = \"HYBRID\"\n",
    "\n",
    "kb_retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=knowledge_base_id,\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": top_k,\n",
    "            \"overrideSearchType\": search_mode,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "kb_engine = RetrieverQueryEngine(retriever=kb_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e10c73-6813-4df8-b1fa-1b46a80d1dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a query tool for Bedrock Knowledge Base\n",
    "\n",
    "kb_tool = QueryEngineTool(\n",
    "        query_engine=kb_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"guide_tool\",\n",
    "            description=\"\"\"\n",
    "            These decision guides help users select appropriate AWS machine learning and generative AI services based on specific needs. \n",
    "            They cover pre-built solutions, customizable platforms, and infrastructure options for ML workflows, \n",
    "            while outlining how generative AI can automate processes, personalize content, augment data, reduce costs, \n",
    "            and enable faster experimentation in various business contexts.\"\"\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e721af-ae9b-4e6f-bea4-c21723012316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "current_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a technology expert with access to the GitHub API, arXiv API, and TechCrunch API. \n",
    "You can search for the latest code repositories, research papers, and news related to technology.\n",
    "You have access to the Amazon Bedrock user guide, which provides information about services offered by Bedrock, \n",
    "such as Agents, Knowledge Bases, Guardrails, Model Evaluation, and Model Fine-Tuning. \n",
    "It also provides third-party foundation models and Amazon LLMs via the Bedrock platform \n",
    "Always utilise the tools at your disposal.\n",
    "If you don’t know the answer, do not make up any information; simply say: Sorry, I don’t know.\n",
    "\n",
    "Current time is: {current_time}\n",
    "\"\"\"\n",
    "\n",
    "# Update the agent to include all API tools and the Knowledge Base tool.\n",
    "\n",
    "all_tools = api_tools +[kb_tool]\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    all_tools, \n",
    "    llm=llm, \n",
    "    verbose=True, # Set verbose=True to display the full trace of steps. \n",
    "    system_prompt = system_prompt,\n",
    "    # allow_parallel_tool_calls = True  # Uncomment this line to allow multiple tool invocations\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3791539-2e61-452e-b69c-4e9c3283ec38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"I don't have many ML experts, but I want to build a GenAI application. Which AWS service should I choose?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400e55c-55f6-405d-a486-ec14e78ab25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple chatbot UI. Enter \"exit\" to quit. \n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"-\" * 120)\n",
    "    print(\" New question: \")\n",
    "    # what services bedrock platform is offering\n",
    "    #  what are the LLM models available from bedrock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b898f6-e756-4136-90e7-dcbb7c234ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agent.memory.reset() # clear the chat memory\n",
    "# agent.memory.get() # retrieve conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc4fc4-8f64-461e-b9fe-688f6007d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test question: \n",
    "# 1. I don't have many ML experts, but I want to build a GenAI application. Which AWS service should I choose?\n",
    "# 2. whats the benefits of using bedrock service\n",
    "# 3. can you give me top 5 git repos related to bedrock "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad478b4b-350c-47b5-a31f-2ebbbc864935",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook shows how we can combine LLMs (Mistral Large 2), internet searching tools, and knowledge bases to build an intelligent research helper. We can see how this solution works well for finding and understanding technical information, and it can easily be made more powerful by adding more data sources and features."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

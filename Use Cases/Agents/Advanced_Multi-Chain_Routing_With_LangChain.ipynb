{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f502fd5-212e-442f-a75e-2bc63ca2c1a6",
   "metadata": {},
   "source": [
    "# Advanced Multi-Chain Routing with LangChain and Mistral Models\n",
    "\n",
    "*This notebook should work well with the **`Data Science 3.0`**  or **`Python 3 (ipkernal)`** kernel in SageMaker Studio*\n",
    "\n",
    "In the realm of generative AI application, there is often a need to leverage multiple data sources and capabilities to provide comprehensive and accurate responses to user queries. LangChain, a powerful framework for developing applications with large language models (LLMs), offers a flexible and modular approach to building such systems.\n",
    "\n",
    "One of the key advantages of LangChain is its ability to combine multiple chains, each specialized in a specific task, into a single pipeline. This multi-chain routing approach allows for the seamless integration of different language models, data sources, and processing capabilities, enabling the creation of sophisticated and tailored solutions.\n",
    "\n",
    "## Flexibility in Choosing LLMs with Multi-Chain Routing\n",
    "\n",
    "By leveraging the multi-chain routing capability of LangChain, developers can incorporate multiple language models into a single pipeline, allowing each model to contribute its strengths and expertise to the overall solution. This approach enables the creation of more robust and accurate systems that can handle complex queries and tasks.\n",
    "\n",
    "## Mistral AI in Amazon Bedrock\n",
    "\n",
    "There are [four Mistral models available on Amazon Bedrock](https://aws.amazon.com/bedrock/mistral/) by the time of writing this notebook, offering flexibility to developers.\n",
    "\n",
    "1. **Mistral Large**: Mistral AIâ€™s most advanced large language model, Mistral Large is a cutting-edge text generation model with top-tier reasoning capabilities. Its precise instruction-following abilities enables application development and tech stack modernization at scale.\n",
    "2. **Mistral 7B**: A 7B dense Transformer, fast-deployed and easily customizable. Small, yet powerful for a variety of use cases.\n",
    "3. **Mixtral 8X7B**: A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral AI 7B. Uses 12B active parameters out of 45B total.\n",
    "4. **Mistral Small**: Mistral Small is perfectly suited for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation.\n",
    "\n",
    "## Use Case: Financial Services Industry (FSI)\n",
    "\n",
    "To demonstrate the power of multi-chain routing with LangChain and Mistral models, we will explore a use case in the Financial Services Industry (FSI). In this scenario, a user wants to:\n",
    "\n",
    "1. **Check Investment**: Determine if they have invested in a particular stock by querying a SQL database.\n",
    "2. **Check Financial Reports**: Retrieve and analyze public financial reports and shareholder letters related to the stock using a Retrieval-Augmented Generation (RAG) chain.\n",
    "3. **Check News**: Search for and retrieve relevant news articles about the stock or company using a search chain.\n",
    "\n",
    "By combining these three capabilities into a single pipeline, the user can obtain a comprehensive overview of their investment, the company's performance, and the latest news and developments, all through a single query.\n",
    "\n",
    "Throughout this notebook, we will walk through the process of setting up the individual chains, defining the routing logic, and integrating the Mistral models to power the multi-chain routing system.\n",
    "\n",
    "\n",
    "## [Langchain Expression Language (LCEL)](https://python.langchain.com/v0.1/docs/expression_language/)\n",
    "\n",
    "LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. In this notebook, we will use LCEL to implement the multiple chains and the orchestration workflow. To make it easy, we will implement our custom chains with [Runnable interface](https://python.langchain.com/v0.1/docs/expression_language/interface/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f0257-cc61-4bd6-bfa3-77b208fc2627",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df461d1-a3cf-439f-a42c-5b337654d8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet langchain langchain-aws faiss-cpu duckduckgo-search --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfde071-43ba-480c-81f6-0d5b87b3b31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mistral_large_model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "mistral_8x7b_model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_7b_model_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "mistral_small_model_id = \"mistral.mistral-small-2402-v1:0\"\n",
    "\n",
    "# modify to the region of your choice\n",
    "aws_region = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfffa6-db54-44ae-a368-5d31728bd941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=aws_region,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7414ed-5c82-476b-b2ba-027ac905634c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mistral_large_llm = BedrockLLM(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=mistral_large_model_id,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "mistral_8x7b_llm = BedrockLLM(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=mistral_8x7b_model_id,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "mistral_7b_llm = BedrockLLM(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=mistral_7b_model_id,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "mistral_small_llm = BedrockLLM(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=mistral_small_model_id,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c0038-17d3-4dff-a58c-2b13bdd44233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a local work directory to store data\n",
    "workdir = \"workspace\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(workdir, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print(\"The directory already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25789e79-bbbc-4650-b4a1-7bfb8a8a5c90",
   "metadata": {},
   "source": [
    "---\n",
    "## SQL Chain: Text-to-SQL Translation with LLMs\n",
    "\n",
    "One of the key components of our multi-chain routing system is the SQL Chain, which enables users to query structured data stored in databases using natural language queries. This capability is made possible through the text-to-SQL translation capabilities of large language models (LLMs) like Mistral.\n",
    "\n",
    "LLMs have demonstrated remarkable performance in understanding and generating natural language, and this ability extends to translating natural language queries into structured SQL queries. By leveraging the language understanding and generation capabilities of LLMs, we can bridge the gap between human-friendly natural language and the structured query language used by databases.\n",
    "\n",
    "Since we want the Text-to-SQL to be accurate and executable, Mistral Large is used for SQL chain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2b230-a39d-4849-bbe1-b13229fc4b0e",
   "metadata": {},
   "source": [
    "Populate stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69206be-bf40-42b2-b3df-5f652fd72d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stock_data = [\n",
    "    [\"AAPL\", \"Apple Inc.\", \"Technology\", 20, \"NASDAQ\", \"USD\"],\n",
    "    [\"MSFT\", \"Microsoft Corporation\", \"Technology\", 18, \"NASDAQ\", \"USD\"],\n",
    "    [\"AMZN\", \"Amazon.com, Inc.\", \"Consumer Cyclical\", 99, \"NASDAQ\", \"USD\"],\n",
    "    [\"NVDA\", \"NVIDIA Corporation\", \"Technology\", 12, \"NASDAQ\", \"USD\"],\n",
    "    [\"TSLA\", \"Tesla, Inc.\", \"Consumer Cyclical\", 10, \"NASDAQ\", \"USD\"],\n",
    "    [\"JPM\", \"JPMorgan Chase & Co.\", \"Finance\", 20, \"NYSE\", \"USD\"],\n",
    "    [\"JNJ\", \"Johnson & Johnson\", \"Healthcare\", 41, \"NYSE\", \"USD\"],\n",
    "    [\"XOM\", \"Exxon Mobil Corporation\", \"Energy\", 33, \"NYSE\", \"USD\"],\n",
    "    [\"WMT\", \"Walmart Inc.\", \"Consumer Defensive\",29, \"NYSE\", \"USD\"],\n",
    "    [\"PG\", \"Procter & Gamble Company\", \"Consumer Defensive\", 35, \"NYSE\", \"USD\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2662cd-a321-425d-b09a-6b3a09e86f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(stock_data, columns=[\"ticker\", \"name\", \"sector\", \"shares\", \"exchange\", \"currency\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bd9a8-df2f-42a8-b6dc-856194048983",
   "metadata": {},
   "source": [
    "Now import the data into a database table. SQLite is a lightweight, self-contained relational database that stores data in a single file. It allows applications to manage structured data without requiring a separate database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24f0ff-5c70-420f-9a3a-c57be560cf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_file = f\"{workdir}/investment.db\"\n",
    "ddl = '''CREATE TABLE IF NOT EXISTS stocks\n",
    "(ticker TEXT, name TEXT, sector TEXT, shares INTEGER, exchange TEXT, currency TEXT)'''\n",
    "\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(db_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute(ddl)\n",
    "\n",
    "# Insert data from the DataFrame into the table\n",
    "df.to_sql('stocks', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c5a14-d663-4436-825e-7e8b9f133b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_sql(sql_query):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql_query)\n",
    "    result = cur.fetchall()\n",
    "    conn.close\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee51f55-0cda-4813-a3c8-54171381d361",
   "metadata": {},
   "source": [
    "SQL Chain can be broken down into two tasts:\n",
    "\n",
    "1. Text-to-SQL\n",
    "2. Generate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317af69c-84fa-483a-b9e9-4d9d892b633e",
   "metadata": {},
   "source": [
    "Take a look at [Langchain Runnable interface](https://python.langchain.com/v0.1/docs/expression_language/interface/), to understand the following components and their input and output types: Prompt, OutputParser, Retriever, Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9ca00-1f57-4613-8dca-d3026cead8fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "SQL_PROMPT_TEMPLATE = f'''<s>[INST]Based on the provided SQL table schema below, write a SQL query that would answer the question.\n",
    "<schema> {ddl} </schema>\n",
    "<question> {{question}} </question>\n",
    "Just generate the SQL query without explanations\n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "text_to_sql_prompt = PromptTemplate.from_template(SQL_PROMPT_TEMPLATE)\n",
    "text_to_sql = text_to_sql_prompt | mistral_large_llm | StrOutputParser() | RunnableLambda(func=execute_sql) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa374e3-e784-4229-975e-38511651e525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FINAL_ANSWER_PROMPT_TEMPLATE = '''<s>[INST]Given the SQL query result: \n",
    "{result}\n",
    "Produce a final response to the original question: \n",
    "{question}\n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "final_answer_prompt = PromptTemplate.from_template(FINAL_ANSWER_PROMPT_TEMPLATE)\n",
    "\n",
    "sql_chain = (\n",
    "    {\n",
    "        \"result\": text_to_sql,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | final_answer_prompt\n",
    "    | mistral_large_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef7e93-163e-42bc-b5be-ee2b9939fc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ConsoleCallbackHandler() will print information about the chain's execution to the console, helping you see what's happening under the hood.\n",
    "sql_chain.invoke(\n",
    "    {\"question\": \"Which stock I have the most shares?\"}, \n",
    "    config={'callbacks': [ConsoleCallbackHandler()]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c5dd1-a9eb-4947-86b2-fafabfa7dd4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG Chain: Augmenting Language Models with Retrieval\n",
    "\n",
    "In addition to the SQL Chain for querying structured data, our multi-chain routing system incorporates a Retrieval-Augmented Generation (RAG) chain for retrieving and processing unstructured data, such as financial reports and shareholder letters.\n",
    "\n",
    "Mistral 8x7b is used for the RAG chain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c24eed-5ce8-4dd3-9907-2c61c8d942de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pypdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082117c-583f-4870-a3b6-d2addcb313d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "url_filename_map = {\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf\": \"Amazon-com-Inc-2023-Shareholder-Letter.pdf\"\n",
    "}\n",
    "\n",
    "# Download files with progress bar\n",
    "for url, filename in tqdm(url_filename_map.items(), unit=\"file\"):\n",
    "    urlretrieve(url, os.path.join(workdir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1877b7-b73e-48d6-bd79-e0a49838ae7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(f\"{workdir}/Amazon-com-Inc-2023-Shareholder-Letter.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a6246-f61f-4996-8836-632342a58030",
   "metadata": {},
   "source": [
    "To set up the RAG chain, we need to use an embedding model to convert the text into embeddings, and store the embeddings in the vector database. [The Amazon Titan Text Embedding v2 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) is used here. It can intake up to 8,192 tokens and outputs a vector of 1,024 dimensions. The model also works in 100+ different languages. The model is optimized for text retrieval tasks, but can also perform additional tasks, such as semantic similarity and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f7d89-9f55-4e46-8195-04ed5f97c9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\", client=bedrock_runtime\n",
    ")\n",
    "\n",
    "# Use the recursive character splitter\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    pages,\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "vectorstore_faiss.save_local(f\"{workdir}/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795c238-9997-4843-8a11-e44bdfbedeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def get_question(input):\n",
    "    if not input:\n",
    "        return None\n",
    "    elif isinstance(input,str):\n",
    "        return input\n",
    "    elif isinstance(input,dict) and 'question' in input:\n",
    "        return input['question']\n",
    "    elif isinstance(input,BaseMessage):\n",
    "        return input.content\n",
    "    else:\n",
    "        raise Exception(\"string or dict with 'question' key expected as RAG chain input.\")\n",
    "\n",
    "\n",
    "context_template = \"\"\"\n",
    "<s>[INST]Use the given context to answer the question. \n",
    "If you don't know the answer, respond \"I don't know\".\n",
    "Keep your response as precise as possible and limit it to a few words. \n",
    "\n",
    "Here is the context:\n",
    "{context}\n",
    "\n",
    "Here is the question: \n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(context_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(get_question) | vectorstore_faiss.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | mistral_8x7b_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae82a3-0d5d-40aa-a7cc-d0cbadbdc006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_chain.invoke(\n",
    "    {\"question\": \"what's the biggest opportunity for Amazon?\"},\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658212fa-409d-479b-9683-a18f5a471cf0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Web Search Chain: Retrieving News\n",
    "\n",
    "In addition to querying structured data and analyzing unstructured documents, our multi-chain routing system incorporates a Web Search Chain to retrieve relevant news articles from the internet. This capability is essential in the financial services industry, where staying up-to-date with the latest news and developments can significantly impact investment decisions and financial analysis.\n",
    "\n",
    "Langchain supports many [search tools](https://python.langchain.com/v0.1/docs/integrations/tools/search_tools/), including DuckDuckGo, Google Search, Bing Search etc. Here as an example, we will use DuckDuckGo.\n",
    "\n",
    "\n",
    "Mistral 7B is used as an example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b96fce-6b4d-4b6e-93c5-09fb271669db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search_prompt_template = '''\n",
    "<s>[INST]Use the given search result to answer the question. \n",
    "If you don't know the answer, respond \"I don't know\".\n",
    "Keep your response as precise as possible and limit it to a few words. \n",
    "\n",
    "Here is search result:\n",
    "{search_result}\n",
    "\n",
    "Here is the question: \n",
    "{question}\n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "search = DuckDuckGoSearchResults(backend=\"news\")\n",
    "search_prompt = PromptTemplate.from_template(search_prompt_template)\n",
    "\n",
    "search_chain = (\n",
    "    {\n",
    "        \"search_result\": RunnableLambda(get_question) | search.run,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | search_prompt\n",
    "    | mistral_7b_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdd351-f53a-4b5d-9133-462e048e9947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_chain.invoke(\n",
    "    {\"question\":\"what's new for stock market today?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898dbbae-ac13-432a-a772-78da78813fbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dynamic multi-chain routing\n",
    "\n",
    "\n",
    "Now we wrap up the three chains into a single pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc58f31-e508-42a2-8d2d-de752431851b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template='''\n",
    "<s>[INST]Given the user question below, classify it as either being about `my-investment`, `company-financial-reports`, or `news`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "[/INST]\n",
    "'''\n",
    "chain_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = (\n",
    "    chain_prompt\n",
    "    | mistral_large_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8907b-eacc-416a-896b-b1fe17442813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"my-investment\" in info[\"topic\"].lower():\n",
    "        return sql_chain\n",
    "    elif \"company-financial-reports\" in info[\"topic\"].lower():\n",
    "        return rag_chain\n",
    "    else:\n",
    "        return search_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596193d5-c0b7-4e08-9c7e-37968070f645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_chain = {\n",
    "    \"topic\": chain, \"question\": lambda x: x[\"question\"]\n",
    "} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788546ec-7d0a-4ac2-9645-ead8ce937dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"Which company do I have the most shares?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa2b5d-6bd7-4f72-8bd3-81943e66e652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"What's amazon's biggest opportunity in its shareholder letter?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac786598-66ca-46eb-a8aa-4d9e458dc2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"What's new about stock market today?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753264f-44ea-4553-bbc6-8eb527d0a4de",
   "metadata": {},
   "source": [
    "## Concurrent Execution of Multiple Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c115941b-06a1-4c7b-ba6d-9f4916cfb06c",
   "metadata": {},
   "source": [
    "In our Financial Services Industry use case, we often need to gather information from multiple sources simultaneously. RunnableParallel is a powerful feature in LangChain that allows us to execute multiple chains concurrently, improving the efficiency of our multi-chain routing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea583b6-fe40-420d-889c-1c8f8c45e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348f9a6-4fed-4d94-88e7-ace5f0559580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all prompt templates\n",
    "SQL_QUERY_TEMPLATE = \"\"\"<s>[INST]Write a SQL query to check all the information related to {company_name} from the stocks table:\n",
    "Table schema: (ticker TEXT, name TEXT, sector TEXT, shares INTEGER, exchange TEXT, currency TEXT)\n",
    "Table name: stocks\n",
    "Just generate the SQL query without explanations.\n",
    "[/INST]\"\"\"\n",
    "\n",
    "INVESTMENT_TEMPLATE = \"\"\"<s>[INST]Check if there are any investments in {company_name} in the portfolio:\n",
    "{sql_result}\n",
    "\n",
    "Table schema: (ticker TEXT, name TEXT, sector TEXT, shares INTEGER, exchange TEXT, currency TEXT)\n",
    "Provide a concise summary of the investment position, including the number of shares.\n",
    "[/INST]\"\"\"\n",
    "\n",
    "FINANCIAL_TEMPLATE = \"\"\"<s>[INST]Analyze the following financial report excerpt for {company_name}:\n",
    "{report_content}\n",
    "\n",
    "Provide key financial metrics and trends. Only use two sentances.\n",
    "[/INST]\"\"\"\n",
    "\n",
    "NEWS_TEMPLATE = \"\"\"<s>[INST]Summarize the recent news about {company_name}:\n",
    "{news_articles}\n",
    "\n",
    "Highlight major developments and market sentiment. Only use two sentances.\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80938cf-0aad-440f-bc4b-9b03b2abda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt templates\n",
    "sql_prompt = PromptTemplate.from_template(SQL_QUERY_TEMPLATE)\n",
    "investment_prompt = PromptTemplate.from_template(INVESTMENT_TEMPLATE)\n",
    "financial_report_prompt = PromptTemplate.from_template(FINANCIAL_TEMPLATE)\n",
    "news_prompt = PromptTemplate.from_template(NEWS_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b8527-1e6e-43cd-a383-2195f93725fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base chains for data retrieval\n",
    "sql_query_chain = (\n",
    "    {\"company_name\": RunnablePassthrough()} |\n",
    "    sql_prompt | \n",
    "    mistral_7b_llm | \n",
    "    StrOutputParser() | \n",
    "    RunnableLambda(execute_sql)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64226356-63f8-4a24-9ca7-6218b6c77adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis chains\n",
    "investment_chain = (\n",
    "    {\n",
    "        \"company_name\": RunnablePassthrough(),\n",
    "        \"sql_result\": sql_query_chain\n",
    "    } |\n",
    "    investment_prompt |\n",
    "    mistral_7b_llm |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "financial_report_chain = (\n",
    "    {\n",
    "        \"company_name\": RunnablePassthrough(),\n",
    "        \"report_content\": lambda x: rag_chain.invoke(x[\"company_name\"])\n",
    "    } |\n",
    "    financial_report_prompt |\n",
    "    mistral_large_llm |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "news_chain = (\n",
    "    {\n",
    "        \"company_name\": RunnablePassthrough(),\n",
    "        \"news_articles\": lambda x: search_chain.invoke(x[\"company_name\"])\n",
    "    } |\n",
    "    news_prompt |\n",
    "    mistral_8x7b_llm |\n",
    "    StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003cc96-47ac-440a-85f2-6eeb20675b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine chains using RunnableParallel\n",
    "stock_analysis_parallel = RunnableParallel(\n",
    "    {\n",
    "        \"investment_status\": investment_chain,\n",
    "        \"financial_analysis\": financial_report_chain,\n",
    "        \"news_summary\": news_chain\n",
    "    }\n",
    ")\n",
    "\n",
    "def analyze_stock(company_name: str):\n",
    "    #Analyze a stock by parallel processing of investment data, financial reports, and news articles.\n",
    "    results = stock_analysis_parallel.invoke({\"company_name\": company_name})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f01ac-4335-4bba-9e02-696a2e69f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_stock(\"Amazon.com, Inc.\")\n",
    "    \n",
    "print(\"\\nInvestment Status:\")\n",
    "print(results[\"investment_status\"])\n",
    "print(\"\\nFinancial Analysis:\")\n",
    "print(results[\"financial_analysis\"])\n",
    "print(\"\\nNews Summary:\")\n",
    "print(results[\"news_summary\"])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

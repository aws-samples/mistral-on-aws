{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3298b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_PROFILE=3p-model\n"
     ]
    }
   ],
   "source": [
    "%env AWS_PROFILE=3p-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29080424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import httpx\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from typing import Dict, Any\n",
    "from sagemaker import ModelPackage\n",
    "from IPython.display import Markdown\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import time\n",
    "import concurrent.futures\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09cf07",
   "metadata": {},
   "source": [
    "# Mistral OCR SageMaker Deployment\n",
    "\n",
    "This notebook demonstrates how to deploy the Mistral OCR model to an Amazon SageMaker endpoint for real-time inference.\n",
    "\n",
    "## Supported Instance Types\n",
    "\n",
    "The Mistral OCR model requires GPU instances. The following instance types are supported:\n",
    "\n",
    "### Quota increase via auto approval \n",
    "- `ml.g6.2xlarge`\n",
    "- `ml.g6.4xlarge`\n",
    "- `ml.g6.8xlarge`\n",
    "- `ml.g6.16xlarge`\n",
    "\n",
    "### Quota increase via support ticket (May take a few days)\n",
    "- `ml.g6e.xlarge`\n",
    "- `ml.g6e.2xlarge`\n",
    "- `ml.g6e.4xlarge`\n",
    "- `ml.g6e.8xlarge`\n",
    "- `ml.g6e.16xlarge`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44319f4",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Before running this notebook, you need to configure the following parameters:\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| `MISTRAL_OCR_MODEL_PACKAGE_ARN` | The Amazon Resource Name (ARN) of the Mistral OCR model package from AWS Marketplace (Product ARN) | `arn:aws:sagemaker:us-west-2:123456789012:model-package/...` |\n",
    "| `MISTRAL_OCR_MODEL_CONFIG_INSTANCE_TYPE` | The EC2 instance type to host the model (see supported types above) | `ml.g6.4xlarge` |\n",
    "| `SAGEMAKER_EXECUTION_ROLE_ARN` | IAM role ARN with permissions to create and invoke SageMaker endpoints | `arn:aws:iam::123456789012:role/SageMakerExecutionRole` |\n",
    "| `MISTRAL_OCR_ENDPOINT_NAME` | A unique name for your SageMaker endpoint | `mistral-ocr-endpoint` |\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<div style=\"background-color: #fff3cd; border-left: 6px solid #ffeb3b; padding: 10px;\">\n",
    "<strong>Note:</strong> Please contact your AWS Account Manager to get model access. For AWS employees, please contact the AWS 3P team.\n",
    "Once the access is provided, you can search \"Mistral OCR\" on AWS Marketplace to subscribe to this model. \n",
    "<br><br>\n",
    "You can find the product ARN from the AWS Marketplace product detail page. Select the region you want to deploy the model first, then you will have the correct product ARN in that region.\n",
    "</div>\n",
    "\n",
    "### Required IAM Permissions\n",
    "\n",
    "The IAM role specified in `SAGEMAKER_EXECUTION_ROLE_ARN` should have the following policies attached:\n",
    "- `AmazonSageMakerFullAccess`\n",
    "- S3 read/write access to model artifacts\n",
    "- CloudWatch Logs access for endpoint logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76034f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_OCR_MODEL_PACKAGE_ARN = \"<MISTRAL_OCR_MODEL_PACKAGE_ARN>\" \n",
    "MISTRAL_OCR_MODEL_CONFIG_INSTANCE_TYPE = \"ml.g6.16xlarge\" \n",
    "SAGEMAKER_EXECUTION_ROLE_ARN = \"<SAGEMAKER_EXECUTION_ROLE_ARN>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c03e29",
   "metadata": {},
   "source": [
    "## Real-time Inference Endpoint Deployment\n",
    "\n",
    "This section demonstrates how to deploy Mistral OCR as a real-time inference endpoint on SageMaker. \n",
    "\n",
    "### Deployment Steps:\n",
    "1. Create a ModelPackage object from the Marketplace ARN\n",
    "2. Deploy the model to a SageMaker endpoint \n",
    "3. Configure auto-scaling to optimize cost and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "MISTRAL_OCR_ENDPOINT_NAME = \"mistral-ocr-real-time-endpoint-1\" # provide an unique endpoint name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = SAGEMAKER_EXECUTION_ROLE_ARN\n",
    "model_package = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=MISTRAL_OCR_MODEL_PACKAGE_ARN,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "model = model_package.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=MISTRAL_OCR_MODEL_CONFIG_INSTANCE_TYPE,\n",
    "    endpoint_name=MISTRAL_OCR_ENDPOINT_NAME,\n",
    "    model_data_download_timeout=3600,\n",
    "    container_startup_health_check_timeout=3600\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-Scaling Configuration\n",
    "\n",
    "# Configure auto-scaling to handle variable traffic patterns\n",
    "# Note: Below endpoints cannot scale to zero instances. Inference component doesn't support model package from MarketPlace\n",
    "client = session.boto_session.client('application-autoscaling')\n",
    "\n",
    "# For traditional endpoints.  \n",
    "resource_id = f'endpoint/{MISTRAL_OCR_ENDPOINT_NAME}/variant/AllTraffic'\n",
    "scalable_dimension = 'sagemaker:variant:DesiredInstanceCount'\n",
    "\n",
    "# Register the endpoint as a scalable target\n",
    "client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=1,  # Minimum 1 instance required \n",
    "    MaxCapacity=5   # Maximum 5 instances - adjust based on your expected traffic\n",
    ")\n",
    "\n",
    "# Configure scaling policy based on invocation count\n",
    "client.put_scaling_policy(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    PolicyName=f'{MISTRAL_OCR_ENDPOINT_NAME}-scaling-policy',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 5.0,  # Target 5 invocations per instance\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance', \n",
    "        },\n",
    "        'ScaleInCooldown': 300,  # Wait 5 minutes before scaling in \n",
    "        'ScaleOutCooldown': 60,   # Wait 1 minute before scaling out \n",
    "        'DisableScaleIn': False    # Enable scale-in to minimum capacity\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "425cf4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "def run_inference(client, endpoint_name: str, payload: dict[str,Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke the SageMaker endpoint for OCR inference.\n",
    "    \n",
    "    Args:\n",
    "        client: SageMaker runtime client\n",
    "        endpoint_name: Name of the deployed endpoint\n",
    "        payload: JSON payload containing the image data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing parsed OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inference_out = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        inference_resp_str = inference_out[\"Body\"].read().decode(\"utf-8\")\n",
    "        return json.loads(inference_resp_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        raise\n",
    "\n",
    "def download_and_encode_file(url: str, file_type: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Download a file from a URL and encode it as base64.\n",
    "    \n",
    "    Args:\n",
    "        url: URL of the file to download\n",
    "        file_type: Type of file ('pdf' or 'image'). If None, will be auto-detected\n",
    "                  from the URL extension\n",
    "        \n",
    "    Returns:\n",
    "        Base64-encoded file string\n",
    "    \"\"\"\n",
    "    if file_type is None:\n",
    "        # Auto-detect file type from URL\n",
    "        lower_url = url.lower()\n",
    "        if lower_url.endswith(('.pdf')):\n",
    "            file_type = 'pdf'\n",
    "        elif lower_url.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp')):\n",
    "            file_type = 'image'\n",
    "        else:\n",
    "            raise ValueError(f\"Could not detect file type from URL: {url}. Please specify file_type parameter.\")\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(url, timeout=10)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Encode the content to base64\n",
    "        file_data = response.content\n",
    "        base64_encoded_data = base64.b64encode(file_data).decode('utf-8')\n",
    "        return base64_encoded_data\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {exc.request.url}\")\n",
    "        raise\n",
    "    except httpx.RequestException as e:\n",
    "        print(f\"Error downloading {file_type}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efeda6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Single image Inference Test\n",
    "\n",
    "# Download sample receipt image and encode as base64\n",
    "receipt_image_url = \"https://cms.mistral.ai/assets/1d7df1b8-5caa-47b9-b6a1-666b05d38019\"\n",
    "receipt_image_b64 = download_and_encode_file(url=receipt_image_url, file_type='image')\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "payload = {\n",
    "    \"model\": \"mistral-ocr-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{receipt_image_b64}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a client and invoke the endpoint\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "receipt_parsed = run_inference(client=sagemaker_client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=payload)\n",
    "\n",
    "# Display the OCR results in markdown format\n",
    "Markdown(receipt_parsed[\"pages\"][0][\"markdown\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Single PDF file Inference Test\n",
    "\n",
    "# Download sample pdf file and encode as base64. This sample pdf file has 24 pages \n",
    "pdf_url = \"https://arxiv.org/pdf/2410.07073\"\n",
    "pdf_b64 = download_and_encode_file(url=pdf_url, file_type='pdf')\n",
    "\n",
    "# Prepare the payload for Mistral OCR model\n",
    "payload ={\n",
    "    \"model\":\"mistral-ocr-2505\",\n",
    "    \"document\":{\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{pdf_b64}\" \n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a client and invoke the endpoint\n",
    "pixtral_report_parsed = run_inference(client=sagemaker_client, endpoint_name=MISTRAL_OCR_ENDPOINT_NAME, payload=payload)\n",
    "\n",
    "# Display the OCR results in markdown format\n",
    "Markdown(pixtral_report_parsed[\"pages\"][1][\"markdown\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stress testing \n",
    "\n",
    "def send_request(args):\n",
    "    \"\"\"\n",
    "    Send a single request to the endpoint and measure latency.\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple containing (client, endpoint_name, payload, request_id)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with success/failure status and latency information\n",
    "    \"\"\"\n",
    "    client, endpoint_name, payload, request_id = args\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        _ = run_inference(client, endpoint_name, payload)\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        return {\"success\": True, \"latency\": latency}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def run_stress_test(file_url, endpoint_name, num_requests=100, max_workers=10):\n",
    "    \"\"\"\n",
    "    Run a stress test against the endpoint with configurable concurrency.\n",
    "    \n",
    "    Args:\n",
    "        file_url: URL of the image or pdf file to use for testing\n",
    "        endpoint_name: Name of the SageMaker endpoint\n",
    "        num_requests: Total number of requests to send\n",
    "        max_workers: Maximum number of concurrent requests\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing test statistics\n",
    "    \"\"\"\n",
    "    # Setup client and prepare payload\n",
    "    sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "    ##--- Test for one image, comment this part if you want to run PDF file testing. ---##\n",
    " \n",
    "    print(f\"Downloading and encoding image from {file_url}...\")\n",
    "    receipt_image_b64 = download_and_encode_file(url=file_url, file_type = \"image\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"mistral-ocr-2505\",\n",
    "        \"document\": {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{receipt_image_b64}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    ##--- End of image stress testing ---##\n",
    "\n",
    "    ##--- Test for a sample pdf file with 24 pages, comment this part if you want to run image testing ---##\n",
    "    # print(f\"Downloading and encoding pdf file from {file_url}...\")\n",
    "    # pdf_b64 = download_and_encode_file(url=file_url, file_type='pdf')\n",
    "\n",
    "    # # Prepare the payload for Mistral OCR model\n",
    "    # payload ={\n",
    "    # \"model\":\"mistral-ocr-2505\",\n",
    "    # \"document\":{\n",
    "    #     \"type\": \"document_url\",\n",
    "    #     \"document_url\": f\"data:application/pdf;base64,{pdf_b64}\" \n",
    "    # }\n",
    "    # }\n",
    "    ##-- End of pdf file stress testing ---##\n",
    "\n",
    "    # Create argument list for parallel requests\n",
    "    args_list = [(sagemaker_client, endpoint_name, payload, i) for i in range(num_requests)]\n",
    "\n",
    "    # Track metrics\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "\n",
    "    print(f\"Starting stress test - sending {num_requests} requests to {endpoint_name}...\")\n",
    "\n",
    "    # Run requests in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(send_request, arg) for arg in args_list]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=num_requests):\n",
    "            results.append(future.result())\n",
    "\n",
    "    # Calculate statistics\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = num_requests - successful\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nStress Test Results:\")\n",
    "    print(f\"Total requests: {num_requests}\")\n",
    "    print(f\"Successful: {successful} ({successful/num_requests*100:.1f}%)\")\n",
    "    print(f\"Failed: {failed} ({failed/num_requests*100:.1f}%)\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Throughput: {successful/total_time:.2f} requests/second\")\n",
    "\n",
    "\n",
    "    if failed > 0:\n",
    "        error_counts = {}\n",
    "        for r in results:\n",
    "            if not r[\"success\"]:\n",
    "                error_type = r[\"error\"].split(':')[0]\n",
    "                error_counts[error_type] = error_counts.get(error_type, 0) + 1\n",
    "\n",
    "        print(\"\\nError breakdown:\")\n",
    "        for error, count in error_counts.items():\n",
    "            print(f\"  {error}: {count} ({count/failed*100:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        \"total_requests\": num_requests,\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"total_time\": total_time,\n",
    "        \"throughput\": successful/total_time,\n",
    "    }\n",
    "\n",
    "# Run the stress test with increasing concurrency\n",
    "\n",
    "file_url = \"https://cms.mistral.ai/assets/1d7df1b8-5caa-47b9-b6a1-666b05d38019\" ## image file \n",
    "# file_url = \"https://arxiv.org/pdf/2410.07073\" ## PDF file \n",
    "\n",
    "for concurrency in [5, 10, 20]:\n",
    "    print(f\"\\n=== Testing with {concurrency} concurrent requests ===\")\n",
    "    stats = run_stress_test(\n",
    "        file_url=file_url,\n",
    "        endpoint_name=MISTRAL_OCR_ENDPOINT_NAME,\n",
    "        num_requests=100,  # Adjust based on your needs\n",
    "        max_workers=concurrency\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95e399",
   "metadata": {},
   "source": [
    "## Conclusion                                                                                                    \n",
    "This notebook demonstrates deploying Mistral OCR on Amazon SageMaker for document understanding. Key achievements: \n",
    "\n",
    "- Configured auto-scaling for efficient resource utilization                                                                                                               \n",
    "- Processed both images and multi-page PDFs with high accuracy                                                                                                        \n",
    "- Tested endpoint performance under various workloads                                                                                                                \n",
    "\n",
    "Mistral OCR on SageMaker provides a production-ready solution with managed infrastructure, scalability, and enterprise-grade security. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3p-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c70d750-8e48-49e6-a44b-37ef1f44dc79",
   "metadata": {},
   "source": [
    "# Preparing Mistral NeMo for Amazon Bedrock Custom Model Import (CMI)\n",
    "\n",
    "This notebook demonstrates how to prepare and import Mistral NeMo into Amazon Bedrock using Custom Model Import (CMI).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **HuggingFace Access**\n",
    "   - Active HuggingFace account\n",
    "   - Valid access token\n",
    "   - CLI authentication with HuggingFace (required for file transfers)\n",
    "\n",
    "2. **File Transfer Method**\n",
    "   - This notebook uses HF Transfer for efficient direct transfers from HuggingFace\n",
    "   - Alternative: Manual download and S3 upload\n",
    "\n",
    "3. **Model Configuration Requirements**\n",
    "   - Must set `max_position_embeddings` to 8192 or less to comply with Bedrock limits\n",
    "   - Defines the maximum sequence length\n",
    "\n",
    "4. **File Format Requirements**\n",
    "   - All model files must be in HuggingFace format\n",
    "   - Required files include:\n",
    "     - Model weights (.safetensors)\n",
    "     - Configuration files (config.json, generation_config.json)\n",
    "     - Tokenizer files (tokenizer.json, tokenizer_config.json)\n",
    "     - Supporting files (vocab.json, merges.txt, special_tokens_map.json)\n",
    "\n",
    "## Important Note on Model Precision\n",
    "Bedrock CMI has specific requirements for model precision:\n",
    "- Supported: FP32, FP16, and BF16 precision\n",
    "- Not supported: Quantized models (including 4-bit quantization)\n",
    "- Note: FP32 models will be automatically converted to BF16 precision internally by Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374d2dd-5f28-4b8e-a6e3-c6f70650c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers hf_transfer huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f24354-6bc6-49f8-8f35-19d736608558",
   "metadata": {},
   "source": [
    "### Download and Upload Hugging Face Model Files to S3 using HF Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ea144-0cac-4c84-867a-ef4d03b0e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# Enable the faster transfers\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Repository and files\n",
    "repo_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "files_to_download = [\n",
    "    \"tokenizer_config.json\",\n",
    "    \"generation_config.json\",\n",
    "    \"merges.txt\",\n",
    "    \"model-00001-of-00005.safetensors\",\n",
    "    \"model-00002-of-00005.safetensors\",\n",
    "    \"model-00003-of-00005.safetensors\",\n",
    "    \"model-00004-of-00005.safetensors\",\n",
    "    \"model-00005-of-00005.safetensors\",\n",
    "    \"model.safetensors.index.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"vocab.json\",\n",
    "    \"config.json\"\n",
    "]\n",
    "\n",
    "# S3 configuration\n",
    "bucket_name = \"BUCKET_NAME\"\n",
    "prefix = \"PREFIX\"\n",
    "\n",
    "# Download location\n",
    "temp_dir = \"./temp_model_files\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Process each file\n",
    "for file in files_to_download:\n",
    "    try:\n",
    "        print(f\"Downloading {file} using accelerated transfer...\")\n",
    "        # Step 1: Download to temporary directory (will use hf_transfer under the hood)\n",
    "        local_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=file,\n",
    "            local_dir=temp_dir\n",
    "        )\n",
    "        \n",
    "        # Step 2: Upload to S3\n",
    "        s3_key = f\"{prefix}/{os.path.basename(local_path)}\"\n",
    "        print(f\"Uploading to s3://{bucket_name}/{s3_key}...\")\n",
    "        s3_client.upload_file(\n",
    "            Filename=local_path,\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key\n",
    "        )\n",
    "        print(f\"Successfully transferred {file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af116cab-5239-4434-b099-4a9a2f72c7ee",
   "metadata": {},
   "source": [
    "### Update max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738718dd-23df-440f-bc35-292d5c504c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 details\n",
    "bucket_name = 'BUCKET_NAME'\n",
    "config_key = ''  # full path to config.json\n",
    "\n",
    "# Download the current config\n",
    "response = s3.get_object(Bucket=bucket_name, Key=config_key)\n",
    "config = json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Modify the config\n",
    "config['max_position_embeddings'] = 8192  # Using Bedrock's recommended value\n",
    "\n",
    "# Upload modified config back to S3\n",
    "s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=config_key,\n",
    "    Body=json.dumps(config, indent=2),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print(\"Config updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b1461-0f76-410d-98df-ac90d79c797f",
   "metadata": {},
   "source": [
    "### Start CMI Import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb4d54-008f-434d-b652-0558c6bb67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock = boto3.client('bedrock')\n",
    "\n",
    "def start_model_import(\n",
    "    bucket_name=\"BUCKET_NAME\", \n",
    "    prefix=\"PREFIX\",\n",
    "    model_name=\"mistral-nemo\",\n",
    "    role_arn=\"\"\n",
    "):\n",
    "    \n",
    "    # Construct S3 URI\n",
    "    s3_uri = f\"s3://{bucket_name}/{prefix}\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock.create_model_import_job(\n",
    "            importedModelName=model_name,\n",
    "            jobName=f\"{model_name}-import-{int(time.time())}\",\n",
    "            modelDataSource={\n",
    "                \"s3DataSource\": {\n",
    "                    \"s3Uri\": s3_uri\n",
    "                }\n",
    "            },\n",
    "            roleArn=role_arn\n",
    "        )\n",
    "        \n",
    "        print(f\"Model import job created successfully. Job ARN: {response['jobArn']}\")\n",
    "        return response['jobArn']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating model import job: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def check_import_status(job_arn):\n",
    "    try:\n",
    "        response = bedrock.get_model_import_job(\n",
    "            jobIdentifier=job_arn\n",
    "        )\n",
    "        # Print full response for debugging\n",
    "        print(\"Full response:\")\n",
    "        print(json.dumps(response, indent=2, default=str))\n",
    "        \n",
    "        # Try to get status from response\n",
    "        if 'status' in response:\n",
    "            return response['status']\n",
    "        elif 'modelImportJob' in response and 'status' in response['modelImportJob']:\n",
    "            return response['modelImportJob']['status']\n",
    "        else:\n",
    "            print(\"Status not found in response structure\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking job status: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Start the import\n",
    "job_arn = start_model_import()\n",
    "\n",
    "# Monitor the import status\n",
    "while True:\n",
    "    try:\n",
    "        status = check_import_status(job_arn)\n",
    "        if status:\n",
    "            print(f\"Import status: {status}\")\n",
    "            if status in ['Completed', 'Failed']:\n",
    "                break\n",
    "        else:\n",
    "            print(\"Could not determine status\")\n",
    "            break\n",
    "        time.sleep(60)  # Check every minute\n",
    "    except Exception as e:\n",
    "        print(f\"Error in monitoring loop: {str(e)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577e008-c236-4764-b93a-c9bbb0dd2a89",
   "metadata": {},
   "source": [
    "### Test Inference with Imported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277a5b7-ae0f-49f5-9066-c89272580e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock Runtime client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"What does Mistral AI do?\"\n",
    "\n",
    "# Prepare the request body\n",
    "request_body = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Invoke the model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId='Model_ARN',\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    # Parse and print the response\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    print(\"Response:\")\n",
    "    print(response_body)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

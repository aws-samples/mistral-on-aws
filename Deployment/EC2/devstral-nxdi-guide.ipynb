{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a972332",
   "metadata": {},
   "source": [
    "# Guide to deploy and benchmark Devstral Small 2505 with NxDI and vLLM on Inf2\n",
    "\n",
    "## Devstral Small 2505 \n",
    "Official model card: <https://huggingface.co/mistralai/Devstral-Small-2505>\n",
    "\n",
    "## NeuronX Distributed Inference (NxDI)  \n",
    "[NxDI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/index.html) is an open-source PyTorch-based inference library that simplifies deep learning model deployment on AWS Inferentia and Inferentia2 instances. Introduced with Neuron SDK 2.21 release, it offers advanced inference capabilities, including features such as continuous batching and speculative decoding for high performance inference.\n",
    "\n",
    "## Overview\n",
    "1. **Install dependencies** – NxDI, the Neuron vLLM fork, and supporting libraries.  \n",
    "2. **(Optional)** Install benchmarking / evaluation utilities (`llmperf`, `lm_eval`).  \n",
    "3. **Download** the Devstral Small 2505 base model weights.  \n",
    "4. **Compile and save** the model with `inference_demo` and verify generation.  \n",
    "5. **Deploy** the model behind a vLLM server.  \n",
    "6. **Benchmark** latency and throughput with `llmperf`.  \n",
    "7. **Evaluate accuracy** with `lm_eval`.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- **Amazon EC2 inf2.24xlarge instance** with `ubuntu 22.04 neuron` DLAMI\n",
    "- **NXDI virtual environment** (e.g., `aws_neuronx_venv_pytorch_2_5_nxd_inference`) is required.\n",
    "\n",
    "- To request a quota increase for `inf2.24xlarge` on EC2, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon EC2.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `inf2.24xlarge` for ec2 on-demand use\n",
    "4. If needed, request a quota increase for these resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb874f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9a878",
   "metadata": {},
   "source": [
    "### Create Your EC2 instance and ssh into it\n",
    "\n",
    "Follow the steps here for a detailed set up of your EC2 instance setup:\n",
    "\n",
    "#### Steps:\n",
    "- Navigate to the EC2 dashboard from the AWS mgmt console and launch your instance.\n",
    "- Search for the Ubuntu 22.04 Neuron DLAMI.\n",
    "- Choose the instance size as inf2.24xlarge or any other Neuron based instance you're able to fit the model.\n",
    "- Set the inbound rule for ssh to your local machine's ip address or anywhere (note that it is not in accordance to set this to allow trafic from any ipv4, please ensure you secure these ports once done testing.\n",
    "- Create and specify your ssh key in the instance configuration step. You will need your .pem file\n",
    "- Create your instance.\n",
    "- Once you have launched your instance, navigate to either your terminal or VSCODE and follow the steps below:\n",
    "\n",
    "#### ssh for powershell:\n",
    "\n",
    "`$PUBLIC_DNS=\"paste your public ipv4 dns here\" # public ipv4 DNS, e.g. ec2-3-80-.... from ec2 console`\n",
    "`$KEY_PATH=\"paste ssh key path here\" # local path to key, e.g. ssh/inf.pem`\n",
    "\n",
    "`ssh -i $KEY_PATH -L 8888:127.0.0.1:8888 -L 8000:127.0.0.1:8000 -L 8086:127.0.0.1:8086 -L 3001:127.0.0.1:3001 ec2-user@$PUBLIC_DNS`\n",
    "\n",
    "#### ssh for linux/macOS:\n",
    "\n",
    "`export PUBLIC_DNS=\"paste your public ipv4 dns here\" # public ipv4 DNS, e.g. ec2-3-80-.... from ec2 console`\n",
    "`export KEY_PATH=\"paste ssh key path here\" # local path to key, e.g. ssh/inf.pem`\n",
    "\n",
    "`ssh -i $KEY_PATH -L 8888:127.0.0.1:8888 -L 8000:127.0.0.1:8000 -L 8086:127.0.0.1:8086 -L 3001:127.0.0.1:3001 ec2-user@$PUBLIC_DNS`\n",
    "\n",
    "You should have sshed into your EC2 instance. \n",
    "\n",
    "- Activate your NXDI venv:\n",
    "\n",
    "`source /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/bin/activate`\n",
    "\n",
    "- Activate jupyter server:\n",
    "\n",
    "`jupyter lab —no-browser —port 8888 —ip 0.0.0.0`\n",
    "\n",
    "You should see a familiar jupyter output with a URL to the notebook.\n",
    "\n",
    "`http://localhost:8888/....`\n",
    "\n",
    "We can click on it, and a jupyter environment opens in our local browser. Upload this notebook to your jupyter environment and run the steps in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd41f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64bd980",
   "metadata": {},
   "source": [
    "## Install and Set up Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea016e9",
   "metadata": {},
   "source": [
    "### 1. Validate / Activate Python Environment\n",
    "\n",
    "Inside a Jupyter notebook, using `source /opt/aws_neuronx_venv_pytorch_2_5_nxd_inference/bin/activate` directly will not persist the environment in subsequent cells, because source runs in a subshell. Please run the command to activate the venv in the terminal or activate prior to spinning up the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# (Optional) Uncomment or modify the following line to activate a custom environment.\n",
    "#source /opt/aws_neuronx_venv_pytorch_2_5_nxd_inference/bin/activate\n",
    "\n",
    "echo 'Python environment check:'\n",
    "which python\n",
    "python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a264e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers==4.45.2\n",
    "huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b809d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list | grep neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3af8db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b03151",
   "metadata": {},
   "source": [
    "### 2. Install Neuron vLLM Fork\n",
    "\n",
    "If you would like to serve your model via [vLLM](https://vllm.readthedocs.io/en/latest/) specialized for Neuron-based inference, you can install AWS Neuron's vLLM fork. NxD Inference integrates into vLLM by extending the model execution components responsible for loading and invoking models used in vLLM's LLMEngine (see [link](https://docs.vllm.ai/en/latest/design/arch_overview.html#llm-engine) for more details on vLLM architecture). This means input processing, scheduling and output processing follow the default vLLM behavior.\n",
    "\n",
    "You enable the Neuron integration in vLLM by setting the device type used by `vLLM` to `neuron`.\n",
    "\n",
    "Currently, we support continuous batching and streaming generation in the NxD Inference vLLM integration. We are working with the vLLM community to enable support for other vLLM features like PagedAttention and Chunked Prefill on Neuron instances through NxD Inference in upcoming releases.\n",
    "\n",
    "\n",
    "Skip this step if you do not need the vLLM server. Cloning and installing vLLM takes 8-10 minutes to complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euxo pipefail\n",
    "\n",
    "if [ -d \"/home/ubuntu/upstreaming-to-vllm\" ]; then\n",
    "    echo \"Neuron vLLM fork already cloned. Skipping.\"\n",
    "else\n",
    "    echo \"Cloning and installing AWS Neuron vLLM fork...\"\n",
    "    cd /home/ubuntu/\n",
    "    git clone -b neuron-2.24-vllm-v0.7.2 https://github.com/aws-neuron/upstreaming-to-vllm.git #neuron 2.24 vllm version\n",
    "    cd upstreaming-to-vllm\n",
    "    pip install -r requirements-neuron.txt --quiet\n",
    "\n",
    "    # Install in editable mode with device set to neuron\n",
    "    VLLM_TARGET_DEVICE=\"neuron\" pip install -e . --quiet\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfd248",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4be831",
   "metadata": {},
   "source": [
    "### 3. (Optional) Install accuracy and perf benchmarking tools\n",
    "\n",
    "#### 3.1 Install llmperf\n",
    "\n",
    "If you'd like to run benchmarks or load tests, you can install [llmperf](https://github.com/ray-project/llmperf). Skip if not needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59438c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if pip show llmperf > /dev/null 2>&1; then\n",
    "    echo \"llmperf is already installed. Skipping.\"\n",
    "else\n",
    "    echo \"Installing llmperf...\"\n",
    "    cd /home/ubuntu/\n",
    "    git clone https://github.com/ray-project/llmperf.git > /dev/null 2>&1 --quiet\n",
    "    cd llmperf\n",
    "    pip install -e . --quiet\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073794da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list| grep neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aca9be",
   "metadata": {},
   "source": [
    "#### 3.2 Accuracy-benchmarking with lm_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d43825",
   "metadata": {},
   "source": [
    "Clone the `aws-neuron-samples` repo to your instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/aws-neuron/aws-neuron-samples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b940944",
   "metadata": {},
   "source": [
    "Copy the [inference-benchmarking](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/) directory to some location on your instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372968d",
   "metadata": {},
   "source": [
    "Change directory to the your copy of inference-benchmarking. Install other required dependencies in the same python env (e.g aws_neuron_venv_pytorch if you followed manual install NxD Inference ) by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c681e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/ubuntu/aws-neuron-samples/inference-benchmarking/\n",
    "pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6e2e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b37d8",
   "metadata": {},
   "source": [
    "## 4. Download or Provide Your Model\n",
    "\n",
    "Below is a template for downloading the Devstral Small 2505 model. You can skip or adjust if you already have a local model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfd0c3",
   "metadata": {},
   "source": [
    "You will need to log in to huggingface from the commandline. You will need your token from https://huggingface.co/settings/tokens Paste it to replace the MY_HUGGINGFACE_TOKEN_HERE text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following code in the terminal to install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd56f5",
   "metadata": {},
   "source": [
    "`sudo apt-get update`\n",
    "\n",
    "`sudo apt-get install git-lfs`\n",
    "\n",
    "`git lfs install`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that git lfs is installed on path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cab300",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start a tmux session and run the following code in the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa78771",
   "metadata": {},
   "source": [
    "`sudo apt-get update`\n",
    "\n",
    "`sudo apt-get install tmux`\n",
    "\n",
    "`tmux new -s mysession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following code to download the model in a tmux session since this may take a while - run in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fbca6",
   "metadata": {},
   "source": [
    "`git clone https://huggingface.co/mistralai/Devstral-Small-2505`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh /home/ubuntu/Devstral-Small-2505/ #check if the full model was copied in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752996a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9365d",
   "metadata": {},
   "source": [
    "## 5. Compile and save the model\n",
    "\n",
    "Use the `inference_demo` command that ships with **NeuronX Distributed Inference** to compile the model for Inferentia2 and generate a quick sample response. Compiled artifacts (NEFF files) are stored under the `--compiled-model-path` you provide and can be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a global parameter which is required for distrbiuted inferencing/training , since we are defining TP=4 we need to define it\n",
    "import os\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Devstral is fine tuned from Mistral Small 3.1-https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503 we would need to make sure we download the mistrall small tokenizer files\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "def download_tokenizer_files(\n",
    "    repo_id=\"mistralai/Mistral-Small-3.1-24B-Base-2503\",\n",
    "    output_dir=\"/home/ubuntu/Devstral-Small-2505\"\n",
    "):\n",
    "    # Verify output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Error: Directory {output_dir} does not exist!\")\n",
    "        return [], [\"Output directory not found\"]\n",
    "    \n",
    "    # Only the files that exist\n",
    "    tokenizer_files = [\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\"\n",
    "    ]\n",
    "    \n",
    "    downloaded_files = []\n",
    "    errors = []\n",
    "    \n",
    "    print(f\"Downloading tokenizer files from {repo_id}\")\n",
    "    print(f\"To existing directory: {output_dir}\")\n",
    "    \n",
    "    for file in tokenizer_files:\n",
    "        try:\n",
    "            print(f\"\\nDownloading {file}...\")\n",
    "            path = hf_hub_download(\n",
    "                repo_id=repo_id,\n",
    "                filename=file,\n",
    "                local_dir=output_dir\n",
    "            )\n",
    "            downloaded_files.append(path)\n",
    "            print(f\"✓ Successfully downloaded {file}\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error downloading {file}: {str(e)}\")\n",
    "    \n",
    "    # Verify downloads\n",
    "    print(\"\\nVerifying downloads:\")\n",
    "    for file in tokenizer_files:\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.exists(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"✓ {file} ({size:,} bytes)\")\n",
    "            print(f\"  Location: {file_path}\")\n",
    "        else:\n",
    "            print(f\"✗ Missing: {file}\")\n",
    "    \n",
    "    return downloaded_files, errors\n",
    "\n",
    "# Run the function\n",
    "files, errors = download_tokenizer_files()\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"\\nAll files downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace this with the path where you downloaded and saved the model files.\n",
    "# These should be the same paths used when compiling the model.\n",
    "MODEL_PATH=\"/home/ubuntu/Devstral-Small-2505/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Devstral-small-2505/\"\n",
    "TP_DEGREE=4\n",
    "\n",
    "inference_demo \\\n",
    "    --model-type llama \\\n",
    "    --task-type causal-lm \\\n",
    "        run \\\n",
    "        --model-path $MODEL_PATH \\\n",
    "        --compiled-model-path $COMPILED_MODEL_PATH \\\n",
    "        --torch-dtype bfloat16 \\\n",
    "        --start_rank_id 0 \\\n",
    "        --tp-degree $TP_DEGREE \\\n",
    "        --batch-size 4 \\\n",
    "        --max-context-length 4096 \\\n",
    "        --seq-len 4096 \\\n",
    "        --on-device-sampling \\\n",
    "        --top-k 1 \\\n",
    "        --do-sample \\\n",
    "        --fused-qkv \\\n",
    "        --sequence-parallel-enabled \\\n",
    "        --pad-token-id 2 \\\n",
    "        --enable-bucketing \\\n",
    "        --context-encoding-buckets 1024 2048 4096  \\\n",
    "            --token-generation-buckets 1024 2048 4096 \\\n",
    "        --prompt \"Write a Python function that generates a Fibonacci sequence.?\" 2>&1 | tee log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c9be3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c85374",
   "metadata": {},
   "source": [
    "## 6. Deploy the model using vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff33d1",
   "metadata": {},
   "source": [
    "#### 6.1 Run Devstral Small 2505 on Inferentia2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb548c",
   "metadata": {},
   "source": [
    "The Neuron‑aware vLLM fork can load the **pre‑compiled** artifacts produced in step 5.\n",
    "\n",
    "If pre-compiled artifacts are provided, then configurations passed through the vllm API will not be used.\n",
    "\n",
    "If they are absent, vLLM automatically triggers a one‑time compilation on first launch.  \n",
    "See the [vLLM user guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#loading-pre-compiled-models-serialization-support) for details.\n",
    "\n",
    "Key CLI flags:\n",
    "\n",
    "* `--max-num-seqs` – maximum batch size in NxDI.  \n",
    "* `--max-model-len` – maximum context length (tokens) per sequence.  \n",
    "* `--tensor-parallel-size` – number of NeuronCores across which the model is sharded.  \n",
    "* `--override-neuron-config` – accepts a dictionary that can be provided to change the default configurations in NxDI while compiling the model for deployment.\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model /home/ubuntu/Devstral-small-2505 \\\n",
    "  --max-num-seqs 16 \\\n",
    "  --max-model-len 8192 \\\n",
    "  --tensor-parallel-size 4 \\\n",
    "  --compiled-model-path /home/ubuntu/traced_model/Devstral-small-2505 \\\n",
    "  --override-neuron-config /home/ubuntu/traced_model/Devstral-small-2505/neuron_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334444bc",
   "metadata": {},
   "source": [
    "In the below steps, we use the precompiled model artifacts we had saved from the previous run with `inference_demo` and we set `VLLM_NEURON_FRAMEWORK=\"neuronx-distributed-inference\"` to override the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1da073",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0893889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE FOLLOWING CELL IN A TERMINAL - spin up the vllm server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92919e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be the same paths used when compiling the model. - command for terminal\n",
    "MODEL_PATH=\"/home/ubuntu/Devstral-Small-2505/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Devstral-small-2505/\"\n",
    "\n",
    "export LOCAL_WORLD_SIZE=4\n",
    "export VLLM_NEURON_FRAMEWORK=\"neuronx-distributed-inference\"\n",
    "export NEURON_COMPILED_ARTIFACTS=$COMPILED_MODEL_PATH\n",
    "\n",
    "VLLM_RPC_TIMEOUT=100000 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --max-num-seqs 4 \\\n",
    "    --max-model-len 2048 \\\n",
    "    --tensor-parallel-size 4 \\\n",
    "    --device neuron \\\n",
    "    --use-v2-block-manager \\\n",
    "    --port 8000 \\\n",
    "    --chat-template \"{% for message in messages %}{% if message['role'] == 'user' %}[INST] {{ message['content'] }} [/INST]{% elif message['role'] == 'assistant' %}{{ message['content'] }}</s>{% endif %}{% endfor %}\" &\n",
    "\n",
    "PID=$!\n",
    "echo \"vLLM server started with PID $PID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec02de",
   "metadata": {},
   "source": [
    "Let's send a quick request with a python client to the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Client Setup\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model_name = models.data[0].id\n",
    "\n",
    "# Sampling Parameters\n",
    "max_tokens = 1024\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = 50\n",
    "stream = False\n",
    "\n",
    "# Chat Completion Request\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "       {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "       {\"role\": \"user\", \"content\": \"Write a Python function to implement binary search?\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "generated_text = \"\"\n",
    "generated_text = response.choices[0].message.content\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403205f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!neuron-ls # show running processes - vllm server is still running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042cfae",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff448c7",
   "metadata": {},
   "source": [
    "#### 6.2 Benchmarking with llmperf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b13ccd",
   "metadata": {},
   "source": [
    "Follow the [LLMPerf on Trainium guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/llm-inference-benchmarking-guide.html) to install and configure the tool.\n",
    "\n",
    "Below is a sample shell script that targets the vLLM server started in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/ubuntu/llmperf/\n",
    "\n",
    "MODEL_PATH=\"/home/ubuntu/Devstral-Small-2505/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Devstral-small-2505/\"\n",
    "OUTPUT_PATH=llmperf-results-sonnets\n",
    "\n",
    "export OPENAI_API_BASE=\"http://localhost:8000/v1\"\n",
    "export OPENAI_API_KEY=\"mock_key\"\n",
    "\n",
    "python token_benchmark_ray.py \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --mean-input-tokens 1000 \\\n",
    "    --stddev-input-tokens 0 \\\n",
    "    --mean-output-tokens 500 \\\n",
    "    --stddev-output-tokens 0 \\\n",
    "    --num-concurrent-requests 4\\\n",
    "    --timeout 3600 \\\n",
    "    --max-num-completed-requests 10 \\\n",
    "    --additional-sampling-params '{}' \\\n",
    "    --results-dir $OUTPUT_PATH \\\n",
    "    --llm-api \"openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo kill $(pgrep -f \"vllm.entrypoints.openai.api_server\")  # Stop the vLLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ad9e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66466df5",
   "metadata": {},
   "source": [
    "#### 6.3 Accuracy evaluation with lm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183370e",
   "metadata": {},
   "source": [
    "This approach expands on the accuracy evaluation using logits and enables you to evaluate accuracy using open source datasets like MMLU and GSM8K for tasks such as instruction following and mathematical reasoning.\n",
    "\n",
    "Under the hood, this accuracy suite uses vLLM server to serve the model and can use benchmarking clients such as [lm-eval](https://github.com/EleutherAI/lm-evaluation-harness) to evaluate on their supported datasets. Refer to the [Accuracy eval](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/accuracy-eval-with-datasets.html) guide in the neuron docs for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656218f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile devstral_config.yaml\n",
    "\n",
    "server:\n",
    "  name: \"Devstral-Small-2505\"\n",
    "  model_path: \"/home/ubuntu/Devstral-Small-2505/\"\n",
    "  model_s3_path: null\n",
    "  compiled_model_path: \"/home/ubuntu/traced_model/Devstral-small-2505/\"\n",
    "  max_seq_len: 4096\n",
    "  context_encoding_len: 4096\n",
    "  tp_degree: 4\n",
    "  n_vllm_threads: 32\n",
    "  server_port: 8888\n",
    "  continuous_batch_size: 1\n",
    "\n",
    "test:\n",
    "  accuracy:\n",
    "    mytest:\n",
    "      client: \"lm_eval\"\n",
    "      datasets: [\"gsm8k_cot\"]\n",
    "      max_concurrent_requests: 1\n",
    "      timeout: 3600\n",
    "      client_params:\n",
    "        limit: 200\n",
    "        use_chat: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if test -f \"/home/ubuntu/aws-neuron-samples/inference-benchmarking/devstral_config.yaml\"; then\n",
    "   echo \"config file exists.\"\n",
    "else \n",
    "   echo \"Copying config file.\"\n",
    "   mv /home/ubuntu/devstral_config.yaml /home/ubuntu/aws-neuron-samples/inference-benchmarking/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export LOCAL_WORLD_SIZE=4\n",
    "cd /home/ubuntu/aws-neuron-samples/inference-benchmarking/\n",
    "python accuracy.py --config devstral_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1172d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd647c98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "* Compiled **Devstral Small 2505** for Inferentia2 with `inference_demo`.\n",
    "* Served the model through the Neuron‑enabled **vLLM** server.\n",
    "* Measured latency and throughput using **llmperf**.\n",
    "* Verified accuracy with **lm_eval**.\n",
    "\n",
    "You can now adapt these steps for your own prompts and workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e236f",
   "metadata": {},
   "source": [
    "In this notebook, we successfully walked through deploying and benchmarking Devstral Small 2505 on Inf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e0a2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a12ae9",
   "metadata": {},
   "source": [
    "#### Distributors\n",
    "- AWS\n",
    "- Mistral"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_7_nxd_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a972332",
   "metadata": {},
   "source": [
    "# Guide to deploy, benchmark, and profile Mistral Small 2501 with NXDI and VLLM on Trn1\n",
    "\n",
    "This notebook provides a step-by-step guide for serving, profiling, and running benchmarks on Mistral Small 24B model on a **Trn1** instance. \n",
    "\n",
    "## Mistral Small 2501\n",
    "\n",
    "[Mistral Small 3.0](https://mistral.ai/news/mistral-small-3) is a 24B-parameter language model from Mistral AI optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment. The model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware.\n",
    "\n",
    "## Neuronx-Distributed-Inference (NxDI)\n",
    "\n",
    "[NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-overview.html#nxdi-overview) (where NxD stands for NeuronX Distributed) is an open-source PyTorch-based inference library that simplifies deep learning model deployment on AWS Inferentia and Trainium instances. Introduced with Neuron SDK 2.21 release, it offers advanced inference capabilities, including features such as continuous batching and speculative decoding for high performance inference. Additionally, it supports inference engine for vLLM for seamless integration with the majority of customers’ production deployment systems. ML developers can use NxD Inference library at different levels of abstraction that fits their inference use case.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Check/Install Dependencies** for AWS Neuron (NXDI, vLLM fork, etc.).\n",
    "2. **Optional**: Install additional utilities (`inference-benchmarking` (lm_eval), InfluxDB, `llmperf` for performance benchmarking, etc.).\n",
    "3. **Download** Mistral Small 24B base model.\n",
    "4. **Spin Up** a VLLM server, benchmark and pull a profile.\n",
    "   \n",
    "### Prerequisites\n",
    "\n",
    "- **Amazon EC2 Trn1.32xlarge instance** with AWS Neuron drivers and recommended PyTorch environment.\n",
    "- **NXDI virtual environment** (e.g., `aws_neuronx_venv_pytorch_2_5_nxd_inference`) is required.\n",
    "\n",
    "- To request a quota increase for `trn1.32xlarge` on EC2, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon EC2.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `trn1.32xlarge` for ec2 on-demand use\n",
    "4. If needed, request a quota increase for these resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c31c9-b15d-4879-84d4-92d62fd82d4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934c846-b3f5-458a-b11b-77b813904e0c",
   "metadata": {},
   "source": [
    "### Create Your EC2 instance\n",
    "\n",
    "Follow the steps here for a detailed set up of your EC2 instance setup:\n",
    "\n",
    "#### Steps:\n",
    "- Navigate to the EC2 dashboard from the AWS mgmt console and launch your instance.\n",
    "- Search for the Ubuntu 22.04 Neuron DLAMI.\n",
    "- Choose the instance size as Trn1.32xlarge or any other Neuron based instance you're able to fit the model.\n",
    "- Set the inbound rule for ssh to your local machine's ip address or anywhere (note that it is not in accordance to set this to allow trafic from any ipv4, please ensure you secure these ports once done testing.\n",
    "- Create and specify your ssh key in the instance configuration step. You will need your .pem file\n",
    "- Create your instance.\n",
    "- Once you have launched your instance, navigate to either your terminal or VSCODE and follow the steps below:\n",
    "\n",
    "#### ssh for powershell:\n",
    "\n",
    "`$PUBLIC_DNS=\"paste your public ipv4 dns here\" # public ipv4 DNS, e.g. ec2-3-80-.... from ec2 console`\n",
    "`$KEY_PATH=\"paste ssh key path here\" # local path to key, e.g. ssh/trn.pem`\n",
    "\n",
    "`ssh -i $KEY_PATH -L 8888:127.0.0.1:8888 -L 8000:127.0.0.1:8000 -L 8086:127.0.0.1:8086 -L 3001:127.0.0.1:3001 ec2-user@$PUBLIC_DNS`\n",
    "\n",
    "#### ssh for linux/macOS:\n",
    "\n",
    "`export PUBLIC_DNS=\"paste your public ipv4 dns here\" # public ipv4 DNS, e.g. ec2-3-80-.... from ec2 console`\n",
    "`export KEY_PATH=\"paste ssh key path here\" # local path to key, e.g. ssh/trn.pem`\n",
    "\n",
    "`ssh -i $KEY_PATH -L 8888:127.0.0.1:8888 -L 8000:127.0.0.1:8000 -L 8086:127.0.0.1:8086 -L 3001:127.0.0.1:3001 ec2-user@$PUBLIC_DNS`\n",
    "\n",
    "You should have sshed into your EC2 instance. \n",
    "\n",
    "- Activate your NXDI venv:\n",
    "\n",
    "`source /opt/aws_neuronx_venv_pytorch_2_5_nxd_inference/bin/activate`\n",
    "\n",
    "- Activate jupyter server:\n",
    "\n",
    "`jupyter lab —no-browser —port 8888 —ip 0.0.0.0`\n",
    "\n",
    "You should see a familiar jupyter output with a URL to the notebook.\n",
    "\n",
    "`http://localhost:8888/....`\n",
    "\n",
    "We can click on it, and a jupyter environment opens in our local browser. Upload this notebook to your jupyter environment and run the steps in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d12827-cdd6-49ad-a5b9-fae27f1e7cda",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f6786-8ed7-4036-9070-3d6669fcb92b",
   "metadata": {},
   "source": [
    "## Install and Set up Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652fc5a",
   "metadata": {},
   "source": [
    "### 1. Validate / Activate Python Environment\n",
    "\n",
    "Inside a Jupyter notebook, using `source myenv/bin/activate` directly will not persist the environment in subsequent cells, because source runs in a subshell. Please run the command to actuvate the venv in the terminal or activate prior to spinning up the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa75aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# (Optional) Uncomment or modify the following line to activate a custom environment.\n",
    "#source /opt/aws_neuronx_venv_pytorch_2_5_nxd_inference/bin/activate\n",
    "\n",
    "echo 'Python environment check:'\n",
    "which python\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657dec1-61fc-4cf7-bec8-48ecd1213c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "torch==2.5.1\n",
    "transformers==4.45.2\n",
    "huggingface_hub\n",
    "git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4844ef-f16c-4bda-bb05-62ca83a7d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc63be-90d4-4145-b42b-ef3fcf4c1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list | grep neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9200f12-356c-43f9-afad-68cb1b486e4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea6ed2",
   "metadata": {},
   "source": [
    "### 2. Install Neuron vLLM Fork\n",
    "\n",
    "If you would like to serve your model via [vLLM](https://vllm.readthedocs.io/en/latest/) specialized for Neuron-based inference, you can install AWS Neuron's vLLM fork. NxD Inference integrates into vLLM by extending the model execution components responsible for loading and invoking models used in vLLM’s LLMEngine (see [link](https://docs.vllm.ai/en/latest/design/arch_overview.html#llm-engine) for more details on vLLM architecture). This means input processing, scheduling and output processing follow the default vLLM behavior.\n",
    "\n",
    "You enable the Neuron integration in vLLM by setting the device type used by `vLLM` to `neuron`.\n",
    "\n",
    "Currently, we support continuous batching and streaming generation in the NxD Inference vLLM integration. We are working with the vLLM community to enable support for other vLLM features like PagedAttention and Chunked Prefill on Neuron instances through NxD Inference in upcoming releases.\n",
    "\n",
    "\n",
    "Skip this step if you do not need the vLLM server. Cloning and installing vLLM takes 8-10 minutes to complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euxo pipefail\n",
    "\n",
    "if [ -d \"/home/ubuntu/upstreaming-to-vllm\" ]; then\n",
    "    echo \"Neuron vLLM fork already cloned. Skipping.\"\n",
    "else\n",
    "    echo \"Cloning and installing AWS Neuron vLLM fork...\"\n",
    "    cd /home/ubuntu/\n",
    "    git clone -b neuron-2.22-vllm-v0.7.2 https://github.com/aws-neuron/upstreaming-to-vllm.git #neuron 2.22 vllm version\n",
    "    cd upstreaming-to-vllm\n",
    "    pip install -r requirements-neuron.txt --quiet\n",
    "\n",
    "    # Install in editable mode with device set to neuron\n",
    "    VLLM_TARGET_DEVICE=\"neuron\" pip install -e . --quiet\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c245e5-21ee-4002-b85e-01127d3e4340",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb91034",
   "metadata": {},
   "source": [
    "### 3. (Optional) Install benchmarking and profiling tools\n",
    "\n",
    "#### 3.1 Install llmperf\n",
    "\n",
    "If you'd like to run benchmarks or load tests, you can install [llmperf](https://github.com/ray-project/llmperf). Skip if not needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba57d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if pip show llmperf > /dev/null 2>&1; then\n",
    "    echo \"llmperf is already installed. Skipping.\"\n",
    "else\n",
    "    echo \"Installing llmperf...\"\n",
    "    cd /home/ubuntu/\n",
    "    git clone https://github.com/ray-project/llmperf.git > /dev/null 2>&1 --quiet\n",
    "    cd llmperf\n",
    "    pip install -e . --quiet\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b95852-a69f-4c6a-bdcd-915bdaf6d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list| grep neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8c80e-20dd-4d40-8d38-9973f03e08eb",
   "metadata": {},
   "source": [
    "#### 3.2 Install AWS Neuron Tools (If Needed)\n",
    "\n",
    "This cell installs the Neuron packages for profiling and other tooling. If already installed, the script checks and skips. For more information, see [Installing Neuron Tools](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/index.html).\n",
    "\n",
    "> **Note**: If you have your apt sources already configured and have installed the Neuron packages, you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855f8f0-7451-454d-a25f-2ac3574334de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euxo pipefail\n",
    "\n",
    "# Check if aws-neuronx-tools is installed\n",
    "if dpkg -s aws-neuronx-tools > /dev/null 2>&1; then\n",
    "    echo \"aws-neuronx-tools is already installed. Skipping.\"\n",
    "else\n",
    "    echo \"Installing aws-neuronx-tools...\"\n",
    "    . /etc/os-release\n",
    "\n",
    "    sudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF\n",
    "deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main\n",
    "EOF\n",
    "\n",
    "    wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -\n",
    "    sudo apt-get update -y\n",
    "    sudo apt-get install -y aws-neuronx-runtime-lib aws-neuronx-dkms aws-neuronx-tools\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322c6a7-89b9-4188-a0e1-270097e45639",
   "metadata": {},
   "source": [
    "#### 3.3 (Optional) Install InfluxDB 2.x\n",
    "\n",
    "Install InfluxDB if using the Neuron Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047c8ba-864f-4191-bf20-9e7239ecf5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if dpkg -s influxdb2 > /dev/null 2>&1; then\n",
    "    echo \"InfluxDB2 is already installed, skipping.\"\n",
    "    if systemctl is-active --quiet influxdb; then\n",
    "        echo \"InfluxDB is already running.\"\n",
    "    else\n",
    "        sudo systemctl start influxdb\n",
    "        echo \"Setting up InfluxDB ...\"\n",
    "        # influx setup\n",
    "    fi\n",
    "else\n",
    "    # Install InfluxDB\n",
    "    wget -q https://repos.influxdata.com/influxdata-archive_compat.key\n",
    "    echo '393e8779c89ac8d958f81f942f9ad7fb82a25e133faddaf92e15b16e6ac9ce4c influxdata-archive_compat.key' | sha256sum -c && \\\n",
    "      cat influxdata-archive_compat.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/influxdata-archive_compat.gpg > /dev/null\n",
    "    echo 'deb [signed-by=/etc/apt/trusted.gpg.d/influxdata-archive_compat.gpg] https://repos.influxdata.com/debian stable main' | sudo tee /etc/apt/sources.list.d/influxdata.list\n",
    "    \n",
    "    sudo apt-get update && sudo apt-get install influxdb2 influxdb2-cli -y\n",
    "    sudo systemctl start influxdb\n",
    "    \n",
    "    # Run non-interactive influx setup with all necessary flags\n",
    "    # replace the following flags below with the necessary credentials\n",
    "    influx setup \\\n",
    "      --username admin \\\n",
    "      --password testpassowrd \\\n",
    "      --org yourorg \\\n",
    "      --bucket yourbucket \\\n",
    "      --token yoursupersecrettoken \\\n",
    "      --force\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb98a5-dcc6-4d82-8283-949c6b3bf266",
   "metadata": {},
   "source": [
    "#### 3.4 Accuracy-benchmarking with lm_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614ddb2-5284-488c-b377-77598508e001",
   "metadata": {},
   "source": [
    "Copy the [inference-benchmarking](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/) directory to some location on your instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702b601-0a7e-4d87-8004-fa7bb7e0389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/aws-neuron/aws-neuron-samples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5e818-6488-4270-975a-81845b3d3cd4",
   "metadata": {},
   "source": [
    "Change directory to the your copy of inference-benchmarking. Install other required dependencies in the same python env (e.g aws_neuron_venv_pytorch if you followed manual install NxD Inference ) by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dddce8-101c-4d2d-b01c-fb93c6a07b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/ubuntu/aws-neuron-samples/inference-benchmarking/\n",
    "pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe3940-45bb-4bae-a12f-d7d6959a2927",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1970fc",
   "metadata": {},
   "source": [
    "## 4. Download or Provide Your Model\n",
    "\n",
    "Below is a template for downloading the model. You can skip or adjust if you already have a local model.\n",
    "\n",
    "For more information on model checkpoint usage, see the [NxDI inference with Hugging Face-based models](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/feature-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff01a8-94f7-4d10-bdf7-71229ec19cb9",
   "metadata": {},
   "source": [
    "You will need to log in to huggingface from the commandline.  You will need your token from https://huggingface.co/settings/tokens Paste it to replace the MY_HUGGINGFACE_TOKEN_HERE text below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7186278-332e-4e07-87b8-fdf1a30f3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b9863-fc7e-4752-ae55-8d4b89312d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following code in the terminal to install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500bcdc-6f0a-4484-b64d-dc23ef94c445",
   "metadata": {},
   "source": [
    "`sudo apt-get update`\n",
    "\n",
    "`sudo apt-get install git-lfs`\n",
    "\n",
    "`git lfs install`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2bdbf-1af1-4a20-a255-7642b8da704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that git lfs is installed on path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f91e7-89ee-4b90-99f4-91239197ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8774eaf-d975-49ab-a1dc-2f6062513aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start a tmux session and run the following code in the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117214b-99fd-4ecf-92fa-e084ee4d95cc",
   "metadata": {},
   "source": [
    "`sudo apt-get update`\n",
    "\n",
    "`sudo apt-get install tmux`\n",
    "\n",
    "`tmux new -s mysession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae2706-cd59-4e38-9353-fe9d3b2283ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following code to download the model in a tmux session since this may take a while - run in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb753745-5fd9-4e17-984f-e1647be82dda",
   "metadata": {},
   "source": [
    "`git clone https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae5abf-e4e8-4418-8747-d5bc05e7f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh /home/ubuntu/Mistral-Small-24B-Instruct-2501/ #check if the full model was copied in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f481bc0-8312-4d24-8dd3-ad834518d1dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083589fe-ee9b-4908-bbe0-b66fff1703a9",
   "metadata": {},
   "source": [
    "## 5. Compile and save model and run generation with HuggingFaceGenerationAdapter- `inference_demo.py`\n",
    "\n",
    "NxD Inference supports running inference with the HuggingFace generate inference. To use HuggingFace-style generation, create a HuggingFaceGenerationAdapter that wraps a Neuron application model. Then, you can call generate on the adapted model. In the below cell, we use the `inference_demo` script that NXDI provides to compile, save, and run some prompts with our Mistral Small 24B model, for more information on the flags we set, refer to the [nxdi api reference guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/api-guides/api-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a78e0-4ed2-4057-b702-d472d24f515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace this with the path where you downloaded and saved the model files.\n",
    "# These should be the same paths used when compiling the model.\n",
    "MODEL_PATH=\"/home/ubuntu/Mistral-Small-24B-Instruct-2501/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Mistral-Small-24B-Instruct-2501/\"\n",
    "TP_DEGREE=32\n",
    "\n",
    "inference_demo \\\n",
    "    --model-type llama \\\n",
    "    --task-type causal-lm \\\n",
    "        run \\\n",
    "        --model-path $MODEL_PATH \\\n",
    "        --compiled-model-path $COMPILED_MODEL_PATH \\\n",
    "        --torch-dtype bfloat16 \\\n",
    "        --start_rank_id 0 \\\n",
    "        --tp-degree $TP_DEGREE \\\n",
    "        --batch-size 1 \\\n",
    "        --max-context-length 12288 \\\n",
    "        --seq-len 12800 \\\n",
    "        --on-device-sampling \\\n",
    "        --top-k 1 \\\n",
    "        --do-sample \\\n",
    "        --fused-qkv \\\n",
    "        --sequence-parallel-enabled \\\n",
    "        --pad-token-id 2 \\\n",
    "        --enable-bucketing \\\n",
    "        --context-encoding-buckets 2048 4096 8192 12288 \\\n",
    "            --token-generation-buckets 2048 4096 8192 12800 \\\n",
    "        --prompt \"What is annapurna labs?\" 2>&1 | tee log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df76059-45ba-4726-9bcc-7a545a6d1575",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ba793-3901-4620-b4b0-5150325a005a",
   "metadata": {},
   "source": [
    "## 6. vLLM demo and perf benchmarking - standalone model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9425cb-9354-4f0a-8375-900961ff54af",
   "metadata": {},
   "source": [
    "#### 6.1 Run Mistral Small 2501 on Trainium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58b2a2-8475-4599-8750-24253c3c9c29",
   "metadata": {},
   "source": [
    "Here is an example for running online inference with Mistral Small 2501 and let's get some perf results. We will first compile and run generation on a sample prompt using a command installed by neuronx-distributed-inference. The script compiles the model and runs generation on the given input prompt. Note the path we used to save the compiled model. This path should be used when launching vLLM server for inference so that the compiled model can be loaded without recompilation. Please refer to [NxD Inference API Reference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/api-guides/api-guide.html) and [VLLM user guide for NxDI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html)for more information on these `inference_demo` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c866cb-9ace-4fae-a940-8776c4e41ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd75bb0-6fb8-4f89-8203-73b8247fc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE FOLLOWING CELL IN A TERMINAL - spin up the vllm server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca565569-45f9-4c0c-b81d-e0c7eaa98249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be the same paths used when compiling the model. - command for terminal\n",
    "MODEL_PATH=\"/home/ubuntu/Mistral-Small-24B-Instruct-2501/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Mistral-Small-24B-Instruct-2501/\"\n",
    "\n",
    "export VLLM_NEURON_FRAMEWORK=\"neuronx-distributed-inference\"\n",
    "export NEURON_COMPILED_ARTIFACTS=$COMPILED_MODEL_PATH\n",
    "VLLM_RPC_TIMEOUT=100000 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --max-num-seqs 1 \\\n",
    "    --max-model-len 12800 \\\n",
    "    --tensor-parallel-size 32 \\\n",
    "    --device neuron \\\n",
    "    --use-v2-block-manager \\\n",
    "    --port 8000 &\n",
    "PID=$!\n",
    "echo \"vLLM server started with PID $PID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24058f-67a6-4f94-9a39-0ce4ab64d801",
   "metadata": {},
   "source": [
    "Let's send a quick request with a python client to the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124db78f-f910-4bcd-b631-4a2fb4166a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Client Setup\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model_name = models.data[0].id\n",
    "\n",
    "# Sampling Parameters\n",
    "max_tokens = 1024\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = 50\n",
    "stream = False\n",
    "\n",
    "# Chat Completion Request\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "       {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "       {\"role\": \"user\", \"content\": \"What is AWS Neuron?\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "generated_text = \"\"\n",
    "generated_text = response.choices[0].message.content\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c4bad-6cb2-43d6-86db-d2011f1bdaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!neuron-ls # show running processes - vllm server is still running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f6afb-6f64-4387-ad65-a7fe705d9fe9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e81d2-e2cf-4ef7-9a55-1fb495bc416f",
   "metadata": {},
   "source": [
    "#### 6.2 llmperf- let's run some quick benchmarks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45478cd-64a5-4a9a-8622-294848e2f852",
   "metadata": {},
   "source": [
    "After the above steps, the vllm server should be running. You can now measure the performance using LLMPerf. Before we can use the llmperf package, we need to make a few changes to its code. Follow benchmarking with LLMPerf guide to apply the code changes.\n",
    "\n",
    "Below is a sample shell script to run LLMPerf. To provide the model with 10000 tokens as input and generate 1500 tokens as output on average, we use the following parameters from LLMPerf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ab418-c069-40cb-8041-5c0574c3377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/ubuntu/llmperf/\n",
    "\n",
    "MODEL_PATH=\"/home/ubuntu/Mistral-Small-24B-Instruct-2501/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Mistral-Small-24B-Instruct-2501/\"\n",
    "OUTPUT_PATH=llmperf-results-sonnets\n",
    "\n",
    "export OPENAI_API_BASE=\"http://localhost:8000/v1\"\n",
    "export OPENAI_API_KEY=\"mock_key\"\n",
    "\n",
    "python token_benchmark_ray.py \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --mean-input-tokens 10000 \\\n",
    "    --stddev-input-tokens 0 \\\n",
    "    --mean-output-tokens 1500 \\\n",
    "    --stddev-output-tokens 0 \\\n",
    "    --num-concurrent-requests 1\\\n",
    "    --timeout 3600 \\\n",
    "    --max-num-completed-requests 50 \\\n",
    "    --additional-sampling-params '{}' \\\n",
    "    --results-dir $OUTPUT_PATH \\\n",
    "    --llm-api \"openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d5e8d-0c16-4061-b22f-c558ba1d5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo kill 55509 #stop the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00332e-d7e0-4607-b91e-e08503a2319a",
   "metadata": {},
   "source": [
    "Summarized results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943d616-6474-40a7-b9cd-61faac231c15",
   "metadata": {},
   "source": [
    "| Scenario                                                                  | TTFT (p50 ms) | TPOT (p50 ms) | Output-token Throughput (tokens/s, p50) |\n",
    "|---------------------------------------------------------------------------|---------------|---------------|-----------------------------------------|\n",
    "| Mistral-Small-24B-Instruct-2501 on Trainium (OpenAI-style API)            | 347           | 10.55         | 107.35                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99fb0e-5712-43f1-be25-2892342bbce4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec3558-b527-44a2-b107-72a6c89d4722",
   "metadata": {},
   "source": [
    "#### 6.3 Running Evaluations\n",
    "\n",
    "There are two methods that you can use the evaluation scirpts to run your evaluation. For more information, check out the [inference-demo](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/)directory and [tutorials](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/trn1-llama3.1-70b-instruct-accuracy-eval-tutorial.html) in NXDI.\n",
    "\n",
    "1. Using a `yaml` configuration file and `accuracy.py` script\n",
    "\n",
    "2. Writing your own python script that uses several components provided in `accuracy.py` and `server_config.py`\n",
    "\n",
    "In this notebook we only demonstrate running an eval with the `yaml` config file.\n",
    "\n",
    "In this method all you need is to create a yaml config file that specifies the server configuration and testing scenario you want to run. Create `config.yaml` with the following content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81c85a-6e50-47f5-9baa-f22e465f7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mistral_config.yaml\n",
    "\n",
    "server:\n",
    "  name: \"Mistral-Small-24B-Instruct\"\n",
    "  model_path: \"/home/ubuntu/Mistral-Small-24B-Instruct-2501/\"\n",
    "  model_s3_path: null\n",
    "  compiled_model_path: \"/home/ubuntu/traced_model/Mistral-Small-24B-Instruct-2501/\"\n",
    "  max_seq_len: 12800\n",
    "  context_encoding_len: 12288\n",
    "  tp_degree: 32\n",
    "  n_vllm_threads: 32\n",
    "  server_port: 8888\n",
    "  continuous_batch_size: 1\n",
    "\n",
    "test:\n",
    "  accuracy:\n",
    "    mytest:\n",
    "      client: \"lm_eval\"\n",
    "      datasets: [\"gsm8k_cot\"]\n",
    "      max_concurrent_requests: 1\n",
    "      timeout: 3600\n",
    "      client_params:\n",
    "        limit: 200\n",
    "        use_chat: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c171e0-a6ef-4782-bb67-2927d393e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if test -f \"/home/ubuntu/aws-neuron-samples/inference-benchmarking/mistral_config.yaml\"; then\n",
    "   echo \"config file exists.\"\n",
    "else \n",
    "   echo \"Copying config file.\"\n",
    "   mv /home/ubuntu/mistral_config.yaml /home/ubuntu/aws-neuron-samples/inference-benchmarking/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efea9af-ed19-476e-9841-764ea6f1b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/ubuntu/aws-neuron-samples/inference-benchmarking/\n",
    "python accuracy.py --config mistral_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee55692-a5f9-41f5-a955-ce36c52f5b96",
   "metadata": {},
   "source": [
    "Results Summary:\n",
    "\n",
    "Accuracy_mytest_gsm8k_cot:\n",
    "    Saved at  results/accuracy/mytest/gsm8k_cot/__home__ubuntu__Mistral-Small-24B-Instruct-2501__/results_2025-04-26T20-02-47.843052.json:\n",
    "    \n",
    "    Metrics: {'gsm8k_cot': {'AccuracyExactMatchStrictMatch': 39.5, 'AccuracyExactMatchStrictMatchStderr': 3.46537, 'AccuracyExactMatchFlexibleExtract': 78.5, 'AccuracyExactMatchFlexibleExtractStderr': 2.91224}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0f6af-8bb3-4e62-b98e-a6bb779f85a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7fa83-aa5c-44c1-986b-c472a1243ed4",
   "metadata": {},
   "source": [
    "#### 6.4 Profiling with `neuron-profile`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3376804-9c21-4e68-ba32-e299ed75fb9d",
   "metadata": {},
   "source": [
    "`neuron-profile` helps developers identify performance bottlenecks and optimize their workloads for NeuronDevices. `neuron-profile` provides insights into NeuronDevice activity including the instructions executed on each compute engine (ex. Tensor engine, Vector engine, etc.), DMA data movement activity, and performance metrics such as engine utilization, DMA throughput, memory usage, and more. NeuronDevice activity is collected by the `neuron-profile` capture command which runs the model with tracing enabled. Profiling typically has near zero overhead because NeuronDevices have dedicated on-chip hardware profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be3b7c-a7e0-4295-b306-e31bbbac8da9",
   "metadata": {},
   "source": [
    "Let's cd into `/tmp/nxd_model` for the compiler working dir with the `context_encoding` and `token_generation` directories that we set the context encoding and token generation buckets for, which hold the NEFFs for these. The neuron-profile tool can both capture and post-process profiling information. neuron-profile takes a compiled model (a NEFF), executes it, and saves the profile results to a NTFF (profile.ntff by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ce0b5-f029-41a3-9971-a087341584fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd /tmp/nxd_model/\n",
    "ls #list directories\n",
    "cd context_encoding_model\n",
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c44a0-7c75-46f8-adb6-e413344e51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_tp0_bk0  _tp0_bk1  _tp0_bk2  _tp0_bk3 - are the context encoding buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9398648-ae6c-4069-baee-338e1205a1e0",
   "metadata": {},
   "source": [
    "##### Capturing profiles for multi-worker jobs\n",
    "`neuron-profile` can capture profiles for collectives-enabled NEFFs running across multiple NeuronCores, NeuronDevices, or even nodes. This is useful for understanding performance and communication overheads when deploying larger distributed models.\n",
    "\n",
    "The following example, performs a distributed run across all NeuronDevices and NeuronCores on our trn1.32xlarge instance, capturing profiles for all 32 workers (one for each NeuronCore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746ce34-fc4a-48bb-9558-c1babbcd7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 1. Make sure the directory exists and is writable\n",
    "mkdir -p /tmp/output/          \n",
    "\n",
    "cd /tmp/nxd_model/context_encoding_model/_tp0_bk1/\n",
    "# 2. Run the capture, pointing -s at that directory\n",
    "neuron-profile capture \\\n",
    "  -n graph.neff \\\n",
    "  --collectives-workers-per-node 32 \\\n",
    "  -s /tmp/output/profile.ntff        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5250d4-f3a7-494b-9c42-d14fbbb07a26",
   "metadata": {},
   "source": [
    "Now if we check our output dir- A profile is saved for each worker in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40fe3a-f01f-4f62-96f5-2df5db0d05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd /tmp/output/\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fbaba-ecfb-4f5e-8af8-31bd6df05162",
   "metadata": {},
   "source": [
    "##### Viewing profiles for multi-worker jobs\n",
    "Profiles from multi-worker jobs (i.e. more than one NeuronCore) can either be viewed individually or in a combined collectives view. Since profile data is often similar between workers and processing profile data for all workers can be time-consuming, it is recommended to first explore the profile for a single worker or small subset of workers. Viewing the profile for a specific worker is the same as for single-worker profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b39974-8a7a-456b-b035-c841adecb8d1",
   "metadata": {},
   "source": [
    "In the beginning, we forwarded port 3001 and 806. This is because `neuron-profile` view is running on a remote instance, we need to use port forwarding to access the profiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c5311-fc27-4dc3-b9e1-ec3da5654089",
   "metadata": {},
   "source": [
    "Viewing the profile for a specific worker is as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f6ec8-ee60-49b2-912f-b2a9be8c1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tmp/nxd_model/context_encoding_model/_tp0_bk1/\n",
    "neuron-profile view -n graph.neff -s /tmp/output/profile_rank_2.ntff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3308394-08a3-413d-a38b-d3266ae2c58f",
   "metadata": {},
   "source": [
    "You will see an output like- View profile at http://localhost:3001/profile/n_a1143c514431fb4c23b7aae9208fd1a89cad42f6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756c277-bc7b-462a-91ed-eaaf3fa6dd8d",
   "metadata": {},
   "source": [
    "![image-profile](imgs/img-neff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e323e2f-3980-4ac7-8c32-ed395c937f0e",
   "metadata": {},
   "source": [
    "To view the profile for multiple workers, pass the directory containing all worker profiles to neuron-profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b082bd6-ed2c-4d70-b092-1e97fe9df1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tmp/nxd_model/context_encoding_model/_tp0_bk1/\n",
    "neuron-profile view -n graph.neff -d /tmp/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb861db0-99e5-42ce-b9c9-bb7cc185af6a",
   "metadata": {},
   "source": [
    "For more on profiling with neuron and understanding profiles,check out the [link](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html) to `neuron-profile` user guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aca927-7701-4f4e-9320-cc2faf9d05bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071be6d-3a5c-4fa0-b02f-4cf3802ff7c2",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70149be-ba89-4320-a17c-082ba6a72b6d",
   "metadata": {},
   "source": [
    "In this notebook, we successfully walked through deploying, benchmarking, and generating profiles for NEFFs on TRN1 using Mistral Small 2501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281311e-0c58-41c1-9f61-93735c768f1f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b2877-bb28-4762-8ef2-9165ef505046",
   "metadata": {},
   "source": [
    "#### Distributors\n",
    "- AWS\n",
    "- Mistral"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
